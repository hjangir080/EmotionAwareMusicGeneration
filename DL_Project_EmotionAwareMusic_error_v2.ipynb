{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+/Qy8gp6OTovV1HBWBVuF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "782083788e3e46e28634090150abc160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd80b17142f44067805c16cd8c0485d9",
              "IPY_MODEL_c98ee5f421ea465e92fa2f56f0b3d76a",
              "IPY_MODEL_ef3f0843864a407393c816a3e969f48a"
            ],
            "layout": "IPY_MODEL_9094318b92414f20a005dd7865839557"
          }
        },
        "cd80b17142f44067805c16cd8c0485d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdd76e6c72684211812ee840618130ef",
            "placeholder": "​",
            "style": "IPY_MODEL_d52ef34e687540d4bfc5ad7f0ccd1476",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c98ee5f421ea465e92fa2f56f0b3d76a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4db7b506259b4ad3b299b7b5a420b876",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d3c8fd60a4e4947b31494a6a7890b95",
            "value": 48
          }
        },
        "ef3f0843864a407393c816a3e969f48a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1fcc3efebd144dcb34837e4a8451c1b",
            "placeholder": "​",
            "style": "IPY_MODEL_033291e50b5c4a8d950543c2dca8435d",
            "value": " 48.0/48.0 [00:00&lt;00:00, 5.42kB/s]"
          }
        },
        "9094318b92414f20a005dd7865839557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd76e6c72684211812ee840618130ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d52ef34e687540d4bfc5ad7f0ccd1476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4db7b506259b4ad3b299b7b5a420b876": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d3c8fd60a4e4947b31494a6a7890b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1fcc3efebd144dcb34837e4a8451c1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "033291e50b5c4a8d950543c2dca8435d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "914ffddea0c44cceb74f3e8cd1e48959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2181ad258d354daa9643625ef377a410",
              "IPY_MODEL_99b09c6694c84f6185f661c9f7c93f20",
              "IPY_MODEL_98328ed4b8924369b5bbbfb8081811f3"
            ],
            "layout": "IPY_MODEL_c7fba3449213432a84f4d67ea7fece20"
          }
        },
        "2181ad258d354daa9643625ef377a410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c53c2bde2e4fe89937f16c1b93d6b2",
            "placeholder": "​",
            "style": "IPY_MODEL_054901e20828436ea16ead085b753c26",
            "value": "config.json: 100%"
          }
        },
        "99b09c6694c84f6185f661c9f7c93f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c5a437ef9ac49e6ae3df04d90cdb533",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5f294e059e3459499328dca51c62f00",
            "value": 570
          }
        },
        "98328ed4b8924369b5bbbfb8081811f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd17fed08b9448c3aac4a2d672b84db8",
            "placeholder": "​",
            "style": "IPY_MODEL_e6112a3dddff44ba90904784d06cf29a",
            "value": " 570/570 [00:00&lt;00:00, 48.6kB/s]"
          }
        },
        "c7fba3449213432a84f4d67ea7fece20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c53c2bde2e4fe89937f16c1b93d6b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "054901e20828436ea16ead085b753c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c5a437ef9ac49e6ae3df04d90cdb533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f294e059e3459499328dca51c62f00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd17fed08b9448c3aac4a2d672b84db8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6112a3dddff44ba90904784d06cf29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e5894d2bf194e60a613753d7616cd04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f95457e068740d08cfd638080764bda",
              "IPY_MODEL_89a3c298cc44493a84b484581d855ad7",
              "IPY_MODEL_6b2f569d0efc461686d73f04c119b4d4"
            ],
            "layout": "IPY_MODEL_05a3cea4c2f04711987406e85600994c"
          }
        },
        "2f95457e068740d08cfd638080764bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67bbcca04b7e4142bed78badb045f6f5",
            "placeholder": "​",
            "style": "IPY_MODEL_fa133a2b1d054dbeb93b23fff1b55764",
            "value": "vocab.txt: 100%"
          }
        },
        "89a3c298cc44493a84b484581d855ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73240ff07b4f4b1ea97d768de4eba257",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7cf5ea451d644be9db5ea174e45f808",
            "value": 231508
          }
        },
        "6b2f569d0efc461686d73f04c119b4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94da7fe55c404c9f81e5731b67429170",
            "placeholder": "​",
            "style": "IPY_MODEL_e49a26f8c11d44bfbcf5975da223e140",
            "value": " 232k/232k [00:00&lt;00:00, 8.20MB/s]"
          }
        },
        "05a3cea4c2f04711987406e85600994c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67bbcca04b7e4142bed78badb045f6f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa133a2b1d054dbeb93b23fff1b55764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73240ff07b4f4b1ea97d768de4eba257": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7cf5ea451d644be9db5ea174e45f808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94da7fe55c404c9f81e5731b67429170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e49a26f8c11d44bfbcf5975da223e140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e115e3f334d74388b0a940ee05bf2bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_082f8a6f47034958801c197aff69c14e",
              "IPY_MODEL_89b797a9ff294ac98235515a0e07fda0",
              "IPY_MODEL_b7901698232c40f78379ba1b5cd9bd35"
            ],
            "layout": "IPY_MODEL_5f8dbd878d1c48adb019c01274fee69a"
          }
        },
        "082f8a6f47034958801c197aff69c14e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a87e35ea35434246996c28376741eca6",
            "placeholder": "​",
            "style": "IPY_MODEL_a925e71c86214980b33b5b6daf88d93b",
            "value": "tokenizer.json: 100%"
          }
        },
        "89b797a9ff294ac98235515a0e07fda0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a9e88fa6cfb4fc2b43ec625ec0318d1",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_900fe6ac95e1412692cc2d05af86b4d4",
            "value": 466062
          }
        },
        "b7901698232c40f78379ba1b5cd9bd35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_245a2e5df53246da8669f997dcca2241",
            "placeholder": "​",
            "style": "IPY_MODEL_9ab1d058d3af438586c972b42873315d",
            "value": " 466k/466k [00:00&lt;00:00, 1.31MB/s]"
          }
        },
        "5f8dbd878d1c48adb019c01274fee69a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a87e35ea35434246996c28376741eca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a925e71c86214980b33b5b6daf88d93b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a9e88fa6cfb4fc2b43ec625ec0318d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "900fe6ac95e1412692cc2d05af86b4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "245a2e5df53246da8669f997dcca2241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ab1d058d3af438586c972b42873315d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd8de1cfed7148fe9a890204c9ee9735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9429334ec874445d83bed6cd791ec982",
              "IPY_MODEL_826c57dbb4e248d39de1aa7b9bc66a12",
              "IPY_MODEL_72f527cf53f64eb6ae5b0a94eb92c0a9"
            ],
            "layout": "IPY_MODEL_80aeb8a927de4cbca3d9ad9bea2d4230"
          }
        },
        "9429334ec874445d83bed6cd791ec982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8da5906185dd4df885144a8daeaf6f85",
            "placeholder": "​",
            "style": "IPY_MODEL_227d573321a2407ca2e0a82ff13c7e2f",
            "value": "config.json: 100%"
          }
        },
        "826c57dbb4e248d39de1aa7b9bc66a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11dc79f439344eedb6b352a4d9267d6e",
            "max": 1005,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c615f6c815d147b6bddbe90e5992982e",
            "value": 1005
          }
        },
        "72f527cf53f64eb6ae5b0a94eb92c0a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd9274741f824c95bea0764343acf6dd",
            "placeholder": "​",
            "style": "IPY_MODEL_027faca9a90345e9ad9f319f823b3229",
            "value": " 1.00k/1.00k [00:00&lt;00:00, 114kB/s]"
          }
        },
        "80aeb8a927de4cbca3d9ad9bea2d4230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8da5906185dd4df885144a8daeaf6f85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "227d573321a2407ca2e0a82ff13c7e2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11dc79f439344eedb6b352a4d9267d6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c615f6c815d147b6bddbe90e5992982e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd9274741f824c95bea0764343acf6dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "027faca9a90345e9ad9f319f823b3229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a749b020ef6468b9c347473541b6f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86afe405052e48eeb71392430217748b",
              "IPY_MODEL_f280b05fc336438e9efaa440baf7286d",
              "IPY_MODEL_7202f1f2e57f46d48819d89509774c3a"
            ],
            "layout": "IPY_MODEL_eb01855b777646e4b4c8c5f080be4556"
          }
        },
        "86afe405052e48eeb71392430217748b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08ec4b84402142318782385bf5aebcdd",
            "placeholder": "​",
            "style": "IPY_MODEL_4ce6def95aae473e87c1cfd943c18309",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "f280b05fc336438e9efaa440baf7286d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef16a37805094a28976e7b8a5c054369",
            "max": 328544361,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a22dde4938d94fad83b8f1a91b21715f",
            "value": 328544361
          }
        },
        "7202f1f2e57f46d48819d89509774c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaa8ca9b309f41f4b0163c2efd3d4a26",
            "placeholder": "​",
            "style": "IPY_MODEL_f5f41c6c4c4e4b1ea32f5d9d8b921e91",
            "value": " 329M/329M [00:06&lt;00:00, 15.2MB/s]"
          }
        },
        "eb01855b777646e4b4c8c5f080be4556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08ec4b84402142318782385bf5aebcdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ce6def95aae473e87c1cfd943c18309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef16a37805094a28976e7b8a5c054369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a22dde4938d94fad83b8f1a91b21715f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaa8ca9b309f41f4b0163c2efd3d4a26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f41c6c4c4e4b1ea32f5d9d8b921e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff9b1506d76a4560a73c16d548ac5592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff47bb799f6445db850d560fcc15c552",
              "IPY_MODEL_19e090e766a84fdf8425098b6fbce3fa",
              "IPY_MODEL_14b55d255181444c96b7a62fe86ec79a"
            ],
            "layout": "IPY_MODEL_db4779223b494858942ed196bf54ab1b"
          }
        },
        "ff47bb799f6445db850d560fcc15c552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26a93f04fea347b8857df86b8d887dc3",
            "placeholder": "​",
            "style": "IPY_MODEL_7c4140ee719140eeb92ee88a628ceeef",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "19e090e766a84fdf8425098b6fbce3fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_800dc3bf4d38459d85efab750aad80e6",
            "max": 294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fc18312608145faac293e3521b457a5",
            "value": 294
          }
        },
        "14b55d255181444c96b7a62fe86ec79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c60ea209c774977a9244edb0c6e3972",
            "placeholder": "​",
            "style": "IPY_MODEL_437774ec25254810b104237b08c1e25e",
            "value": " 294/294 [00:00&lt;00:00, 34.3kB/s]"
          }
        },
        "db4779223b494858942ed196bf54ab1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26a93f04fea347b8857df86b8d887dc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c4140ee719140eeb92ee88a628ceeef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "800dc3bf4d38459d85efab750aad80e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fc18312608145faac293e3521b457a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c60ea209c774977a9244edb0c6e3972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "437774ec25254810b104237b08c1e25e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "492c48f195d84feab71b6f8602913979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03474b8b9d484b079e3ebc4f56f4cae0",
              "IPY_MODEL_38fd46f32ad64ca1a20e608663cc85a0",
              "IPY_MODEL_afbfaa9d43ad4096adf0e5026f57560d"
            ],
            "layout": "IPY_MODEL_ea402f2b272c4c7aaa8be4c4db7b4874"
          }
        },
        "03474b8b9d484b079e3ebc4f56f4cae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edd13942d57742b9a21d188b1ec03d6b",
            "placeholder": "​",
            "style": "IPY_MODEL_19a9bc62ce4f4be79426c5b45cf221bb",
            "value": "vocab.json: 100%"
          }
        },
        "38fd46f32ad64ca1a20e608663cc85a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1766566a7e454942925578fbacca8b1b",
            "max": 798293,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed658f262a514ad083749458f2dcc408",
            "value": 798293
          }
        },
        "afbfaa9d43ad4096adf0e5026f57560d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6e7c948dbcf44309ad916828cd15fa1",
            "placeholder": "​",
            "style": "IPY_MODEL_c582f3c66b2046f994f0aca735dd5dc7",
            "value": " 798k/798k [00:00&lt;00:00, 1.55MB/s]"
          }
        },
        "ea402f2b272c4c7aaa8be4c4db7b4874": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edd13942d57742b9a21d188b1ec03d6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19a9bc62ce4f4be79426c5b45cf221bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1766566a7e454942925578fbacca8b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed658f262a514ad083749458f2dcc408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6e7c948dbcf44309ad916828cd15fa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c582f3c66b2046f994f0aca735dd5dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f5566aa2e7e494ca87f23758979b292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e01d93f7e71844c5ac44f9042b5d2a4f",
              "IPY_MODEL_8571ff32b1e84cca83dbc780fde792af",
              "IPY_MODEL_38b42c4c3fae4ee7a069c12daea1c6ca"
            ],
            "layout": "IPY_MODEL_694590d2504446b486a80743ba03aa37"
          }
        },
        "e01d93f7e71844c5ac44f9042b5d2a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a4b9280369a4e7fab40cd174e3de76f",
            "placeholder": "​",
            "style": "IPY_MODEL_70a9048ec38b4b5c8c2ef3e72dfb08e7",
            "value": "model.safetensors: 100%"
          }
        },
        "8571ff32b1e84cca83dbc780fde792af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b02b7a0272f482fa96227d5d28646f7",
            "max": 328511860,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24b420ec8956499cb114abe4ea18bf24",
            "value": 328511860
          }
        },
        "38b42c4c3fae4ee7a069c12daea1c6ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56d94ac19312469b8b2a21669054c9d7",
            "placeholder": "​",
            "style": "IPY_MODEL_5c63f393c28b4f74bf253e07425e4711",
            "value": " 329M/329M [00:02&lt;00:00, 156MB/s]"
          }
        },
        "694590d2504446b486a80743ba03aa37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a4b9280369a4e7fab40cd174e3de76f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70a9048ec38b4b5c8c2ef3e72dfb08e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b02b7a0272f482fa96227d5d28646f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b420ec8956499cb114abe4ea18bf24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56d94ac19312469b8b2a21669054c9d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c63f393c28b4f74bf253e07425e4711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "069bb0ad30d641969a96315a9e279303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1f2d9778f15419b9ec93bed86cc0e0f",
              "IPY_MODEL_f4219935e5804ac992c488c06f98b28a",
              "IPY_MODEL_b5b245fa528b4fb09d455376064f9f02"
            ],
            "layout": "IPY_MODEL_0bd64190681a4e6ab34b14b527d69790"
          }
        },
        "c1f2d9778f15419b9ec93bed86cc0e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4b522a10c1741ba973a44fcda643dcc",
            "placeholder": "​",
            "style": "IPY_MODEL_25dc68291fbd43babf54c2c6ca18dc18",
            "value": "merges.txt: 100%"
          }
        },
        "f4219935e5804ac992c488c06f98b28a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b0d72ac577a4c849485016399e2c507",
            "max": 456356,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_059c94cdcf1043dd93540879d98b5dd7",
            "value": 456356
          }
        },
        "b5b245fa528b4fb09d455376064f9f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04d66892c9b44798be7a020b4eb72617",
            "placeholder": "​",
            "style": "IPY_MODEL_ce86bf21bd734af6bde6c756890539bb",
            "value": " 456k/456k [00:00&lt;00:00, 1.30MB/s]"
          }
        },
        "0bd64190681a4e6ab34b14b527d69790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b522a10c1741ba973a44fcda643dcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25dc68291fbd43babf54c2c6ca18dc18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b0d72ac577a4c849485016399e2c507": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "059c94cdcf1043dd93540879d98b5dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04d66892c9b44798be7a020b4eb72617": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce86bf21bd734af6bde6c756890539bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5735b4c35a884874817799d2733cf107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3361886dff64932b9616c97f98f4cd4",
              "IPY_MODEL_e1236af13bee438ea5b247d9c0906558",
              "IPY_MODEL_8300c415118e435ba7ad8e0148f0e19b"
            ],
            "layout": "IPY_MODEL_fcb4cb1488a6427f9198eaa76149acdf"
          }
        },
        "d3361886dff64932b9616c97f98f4cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c016136ec5004817a6b3fc01d558c4ee",
            "placeholder": "​",
            "style": "IPY_MODEL_3ca6c759b77941f687654f058e802e73",
            "value": "tokenizer.json: 100%"
          }
        },
        "e1236af13bee438ea5b247d9c0906558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0fc1ec3795b497cb715281ffe5e5a06",
            "max": 1356047,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0764a7700efc4c08acc55c69c84ba17f",
            "value": 1356047
          }
        },
        "8300c415118e435ba7ad8e0148f0e19b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13b3e12765ea4b94909170c1b21a35ca",
            "placeholder": "​",
            "style": "IPY_MODEL_c8f08e7112b04478a6e4f6a07a34639f",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 1.91MB/s]"
          }
        },
        "fcb4cb1488a6427f9198eaa76149acdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c016136ec5004817a6b3fc01d558c4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ca6c759b77941f687654f058e802e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0fc1ec3795b497cb715281ffe5e5a06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0764a7700efc4c08acc55c69c84ba17f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13b3e12765ea4b94909170c1b21a35ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8f08e7112b04478a6e4f6a07a34639f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a7f30ee55604a1c954a3114a634d0f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0cadd2f8a1a34947b078f471c900eb02",
              "IPY_MODEL_469eb36ba135442c8f67c09502ada96c",
              "IPY_MODEL_1a1f415c1ca640f0baa79abfd6db25a5"
            ],
            "layout": "IPY_MODEL_42fa54593a7e406e902b79a90b7348dd"
          }
        },
        "0cadd2f8a1a34947b078f471c900eb02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_495994f06e3646feaa20d243257d4d0f",
            "placeholder": "​",
            "style": "IPY_MODEL_23af157bc3c140b39ac404b99bf59b08",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "469eb36ba135442c8f67c09502ada96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_909df247fb80457ebeb20d629acd35d6",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a058f5e44e2d4065831e7a1d32a33dac",
            "value": 239
          }
        },
        "1a1f415c1ca640f0baa79abfd6db25a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02e6fb78050a4091991b664cb501cdf7",
            "placeholder": "​",
            "style": "IPY_MODEL_1e0ab156d6354d01a710601af34fd7a0",
            "value": " 239/239 [00:00&lt;00:00, 28.6kB/s]"
          }
        },
        "42fa54593a7e406e902b79a90b7348dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "495994f06e3646feaa20d243257d4d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23af157bc3c140b39ac404b99bf59b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "909df247fb80457ebeb20d629acd35d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a058f5e44e2d4065831e7a1d32a33dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02e6fb78050a4091991b664cb501cdf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e0ab156d6354d01a710601af34fd7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c656350e7c84905ba4d841fa3e5648e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1408ab89f3d4bb289fd8745abb31ec8",
              "IPY_MODEL_875658072f9640c4b689939c9f533ef6",
              "IPY_MODEL_20a01cb93a9440e1aa6637d75068e7b8"
            ],
            "layout": "IPY_MODEL_d1c3cea2a9fa4a749c59e8f0c200fd4e"
          }
        },
        "d1408ab89f3d4bb289fd8745abb31ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87f085e9e07c4b4f845f5c55e231dc9d",
            "placeholder": "​",
            "style": "IPY_MODEL_9693dd4cce4e4b80b3907453c88b1d6e",
            "value": "  0%"
          }
        },
        "875658072f9640c4b689939c9f533ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4af94ef844c14aefabedca57cfe00758",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_474174ad24e94e8c8e0a6e202c791b1d",
            "value": 0
          }
        },
        "20a01cb93a9440e1aa6637d75068e7b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8223c875558649a68ab7d15521cd7006",
            "placeholder": "​",
            "style": "IPY_MODEL_fba45276b966462da1ef88acf3e79f82",
            "value": " 0/50 [00:00&lt;?, ?it/s]"
          }
        },
        "d1c3cea2a9fa4a749c59e8f0c200fd4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87f085e9e07c4b4f845f5c55e231dc9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9693dd4cce4e4b80b3907453c88b1d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4af94ef844c14aefabedca57cfe00758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "474174ad24e94e8c8e0a6e202c791b1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8223c875558649a68ab7d15521cd7006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fba45276b966462da1ef88acf3e79f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hjangir080/EmotionAwareMusicGeneration/blob/main/DL_Project_EmotionAwareMusic_error_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dCTrxNK4ocQ"
      },
      "outputs": [],
      "source": [
        "class MuLanEmbedding(nn.Module):\n",
        "    def __init__(self, text_embedding_dim=768, audio_embedding_dim=512, joint_embedding_dim=256):\n",
        "        super(MuLanEmbedding, self).__init__()\n",
        "\n",
        "        # Text encoder (BERT-based)\n",
        "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.text_projection = nn.Linear(768, joint_embedding_dim)\n",
        "\n",
        "        # Audio encoder\n",
        "        self.audio_encoder = nn.Sequential(\n",
        "            nn.Conv1d(80, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.audio_projection = nn.Linear(512, joint_embedding_dim)\n",
        "\n",
        "    def encode_text(self, input_ids, attention_mask):\n",
        "        outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
        "        text_embedding = self.text_projection(cls_embedding)\n",
        "        return F.normalize(text_embedding, p=2, dim=1)\n",
        "\n",
        "    def encode_audio(self, mel_spectrogram):\n",
        "        # mel_spectrogram shape: [batch_size, freq_bins, time_frames]\n",
        "        audio_features = self.audio_encoder(mel_spectrogram).squeeze(-1)\n",
        "        audio_embedding = self.audio_projection(audio_features)\n",
        "        return F.normalize(audio_embedding, p=2, dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, mel_spectrogram=None):\n",
        "        text_embedding = self.encode_text(input_ids, attention_mask)\n",
        "\n",
        "        if mel_spectrogram is not None:\n",
        "            audio_embedding = self.encode_audio(mel_spectrogram)\n",
        "            return text_embedding, audio_embedding\n",
        "\n",
        "        return text_embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SoundStreamDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=256, output_channels=1):\n",
        "        super(SoundStreamDecoder, self).__init__()\n",
        "\n",
        "        self.initial_layer = nn.Linear(latent_dim, 16 * 256)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(32, output_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, latent_vector):\n",
        "        # latent_vector shape: [batch_size, latent_dim]\n",
        "        x = self.initial_layer(latent_vector)\n",
        "        x = x.view(-1, 256, 16)  # Reshape to [batch_size, channels, length]\n",
        "        waveform = self.decoder(x)\n",
        "        return waveform"
      ],
      "metadata": {
        "id": "q6P7dBoc4tY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMusicLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomMusicLM, self).__init__()\n",
        "\n",
        "        # Text-to-music embedding model\n",
        "        self.mulan_model = MuLanEmbedding()\n",
        "\n",
        "        # Latent vector generation\n",
        "        self.latent_generator = nn.Sequential(\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Audio decoder\n",
        "        self.soundstream_decoder = SoundStreamDecoder()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get text embedding\n",
        "        text_embedding = self.mulan_model.encode_text(input_ids, attention_mask)\n",
        "\n",
        "        # Generate latent vector\n",
        "        latent_vector = self.latent_generator(text_embedding)\n",
        "\n",
        "        # Decode to audio\n",
        "        waveform = self.soundstream_decoder(latent_vector)\n",
        "\n",
        "        return waveform"
      ],
      "metadata": {
        "id": "3NSULfem4xKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_custom_musiclm(model, train_dataloader, num_epochs=100):\n",
        "    # Define loss functions\n",
        "    reconstruction_loss = nn.MSELoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            audio_targets = batch['audio_waveform']\n",
        "\n",
        "            # Forward pass\n",
        "            audio_output = model(input_ids, attention_mask)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = reconstruction_loss(audio_output, audio_targets)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "Te1FCjCb40dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def use_custom_musiclm(literary_music_generator, text_prompts):\n",
        "    # Initialize custom MusicLM model\n",
        "    custom_musiclm = CustomMusicLM()\n",
        "\n",
        "    # Load trained weights\n",
        "    custom_musiclm.load_state_dict(torch.load('custom_musiclm.pth'))\n",
        "    custom_musiclm.eval()\n",
        "\n",
        "    # Tokenize the text prompts\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    audio_outputs = []\n",
        "\n",
        "    for prompt in text_prompts:\n",
        "        # Tokenize\n",
        "        encoded_input = tokenizer(\n",
        "            prompt,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Generate audio\n",
        "        with torch.no_grad():\n",
        "            audio_output = custom_musiclm(\n",
        "                encoded_input['input_ids'],\n",
        "                encoded_input['attention_mask']\n",
        "            )\n",
        "\n",
        "        audio_outputs.append(audio_output.squeeze().cpu().numpy())\n",
        "\n",
        "    return audio_outputs"
      ],
      "metadata": {
        "id": "W2KxM2GH42dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class MuLANTextEncoder(nn.Module):\n",
        "    \"\"\"Text encoder based on BERT with MuLAN-style additions for music understanding\"\"\"\n",
        "    def __init__(self, bert_model=\"bert-base-uncased\", embedding_dim=512):\n",
        "        super(MuLANTextEncoder, self).__init__()\n",
        "        # Load pre-trained BERT model\n",
        "        self.bert = BertModel.from_pretrained(bert_model)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
        "\n",
        "        # Project BERT outputs to our embedding space\n",
        "        self.projection = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
        "\n",
        "        # Music-specific token embeddings (words related to music semantics)\n",
        "        self.music_token_embedding = nn.Embedding(1000, embedding_dim)\n",
        "        self.music_vocab = self._create_music_vocab()\n",
        "\n",
        "    def _create_music_vocab(self):\n",
        "        \"\"\"Create vocabulary mapping for music-specific terms\"\"\"\n",
        "        music_terms = [\n",
        "            \"tempo\", \"rhythm\", \"melody\", \"harmony\", \"bass\", \"treble\",\n",
        "            \"major\", \"minor\", \"piano\", \"guitar\", \"drums\", \"strings\",\n",
        "            \"loud\", \"soft\", \"fast\", \"slow\", \"staccato\", \"legato\",\n",
        "            # Emotions\n",
        "            \"happy\", \"sad\", \"angry\", \"fearful\", \"tender\", \"excited\"\n",
        "            # Add more music-specific terms\n",
        "        ]\n",
        "        return {term: i for i, term in enumerate(music_terms)}\n",
        "\n",
        "    def forward(self, text):\n",
        "        \"\"\"Encode text into music-aware embeddings\"\"\"\n",
        "        # Tokenize input\n",
        "        tokens = self.tokenizer(text, return_tensors=\"pt\",\n",
        "                               padding=True, truncation=True, max_length=512)\n",
        "\n",
        "        # Move to the same device as the model\n",
        "        tokens = {k: v.to(self.bert.device) for k, v in tokens.items()}\n",
        "\n",
        "        # Get BERT embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert(**tokens)\n",
        "\n",
        "        # Use the [CLS] token embedding as sequence representation\n",
        "        sequence_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Project to our embedding space\n",
        "        projected_embedding = self.projection(sequence_embedding)\n",
        "\n",
        "        # Extract and enhance music-specific terms\n",
        "        music_embedding = self._enhance_music_terms(text, projected_embedding)\n",
        "\n",
        "        return music_embedding\n",
        "\n",
        "    def _enhance_music_terms(self, text, embedding):\n",
        "        \"\"\"Enhance embeddings with music-specific token information\"\"\"\n",
        "        # Simple version - just detect music terms and add their embeddings\n",
        "        batch_enhanced = []\n",
        "\n",
        "        for i, t in enumerate(text):\n",
        "            t_lower = t.lower()\n",
        "\n",
        "            # Initialize music embedding contribution\n",
        "            music_contrib = torch.zeros_like(embedding[i])\n",
        "            count = 0\n",
        "\n",
        "            # Look for music terms\n",
        "            for term, idx in self.music_vocab.items():\n",
        "                if term in t_lower:\n",
        "                    music_contrib += self.music_token_embedding(torch.tensor([idx],\n",
        "                                    device=embedding.device)).squeeze(0)\n",
        "                    count += 1\n",
        "\n",
        "            # Add weighted music embedding if any terms found\n",
        "            if count > 0:\n",
        "                enhanced = embedding[i] + (music_contrib / count) * 0.3  # 30% contribution\n",
        "            else:\n",
        "                enhanced = embedding[i]\n",
        "\n",
        "            batch_enhanced.append(enhanced)\n",
        "\n",
        "        return torch.stack(batch_enhanced)\n",
        "\n",
        "\n",
        "class SoundStreamEncoder(nn.Module):\n",
        "    \"\"\"Audio encoder inspired by SoundStream architecture\"\"\"\n",
        "    def __init__(self, input_channels=1, embedding_dim=512):\n",
        "        super(SoundStreamEncoder, self).__init__()\n",
        "\n",
        "        # Convolutional encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            # Initial convolution\n",
        "            nn.Conv1d(input_channels, 32, kernel_size=7, stride=1, padding=3),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Downsampling convolutions\n",
        "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),  # Downsample 2x\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),  # Downsample 2x\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),  # Downsample 2x\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),  # Downsample 2x\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Additional convolutions\n",
        "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Projection to embedding space\n",
        "        self.projection = nn.Linear(512, embedding_dim)\n",
        "\n",
        "        # Quantizer (VQ layer)\n",
        "        self.codebook_size = 1024\n",
        "        self.codebook = nn.Embedding(self.codebook_size, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Encode audio into latent representations\n",
        "        x shape: [batch_size, 1, time]\n",
        "        \"\"\"\n",
        "        # Apply convolutional encoder\n",
        "        encoded = self.encoder(x)  # [batch_size, 512, time/16]\n",
        "\n",
        "        # Global pooling to get fixed-size representation\n",
        "        pooled = F.adaptive_avg_pool1d(encoded, 1).squeeze(-1)  # [batch_size, 512]\n",
        "\n",
        "        # Project to embedding space\n",
        "        embedding = self.projection(pooled)  # [batch_size, embedding_dim]\n",
        "\n",
        "        # Nearest-neighbor lookup in the codebook (simplified VQ)\n",
        "        # In practice, SoundStream uses a more complex residual VQ approach\n",
        "        distances = torch.sum(embedding.unsqueeze(1)**2, dim=2) + \\\n",
        "                   torch.sum(self.codebook.weight**2, dim=1) - \\\n",
        "                   2 * torch.matmul(embedding, self.codebook.weight.t())\n",
        "\n",
        "        encoding_indices = torch.argmin(distances, dim=1)\n",
        "        quantized = self.codebook(encoding_indices)\n",
        "\n",
        "        # Straight-through estimator\n",
        "        quantized_st = embedding + (quantized - embedding).detach()\n",
        "\n",
        "        return quantized_st, encoded\n",
        "\n",
        "\n",
        "class SoundStreamDecoder(nn.Module):\n",
        "    \"\"\"Audio decoder inspired by SoundStream architecture\"\"\"\n",
        "    def __init__(self, embedding_dim=512, output_channels=1):\n",
        "        super(SoundStreamDecoder, self).__init__()\n",
        "\n",
        "        # Project embedding to the right dimension for the decoder\n",
        "        self.pre_decoder = nn.Linear(embedding_dim, 512)\n",
        "\n",
        "        # Convolutional decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            # Initial convolution\n",
        "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Upsampling convolutions using transposed convolutions\n",
        "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),  # Upsample 2x\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),  # Upsample 2x\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),   # Upsample 2x\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),    # Upsample 2x\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final convolution to get to the right number of channels\n",
        "            nn.Conv1d(32, output_channels, kernel_size=7, stride=1, padding=3),\n",
        "            nn.Tanh()  # Output in [-1, 1] range for audio\n",
        "        )\n",
        "\n",
        "    def forward(self, z, encoded=None, length=16000):\n",
        "        \"\"\"\n",
        "        Decode latent representation to audio\n",
        "        z shape: [batch_size, embedding_dim]\n",
        "        encoded shape (optional): [batch_size, 512, time/16]\n",
        "        \"\"\"\n",
        "        # Project to the right dimension\n",
        "        z_proj = self.pre_decoder(z)  # [batch_size, 512]\n",
        "\n",
        "        if encoded is not None:\n",
        "            # Use the temporal information from the encoder\n",
        "            z_temporal = z_proj.unsqueeze(-1) * F.adaptive_avg_pool1d(encoded, encoded.size(-1))\n",
        "        else:\n",
        "            # Create a temporal dimension\n",
        "            time_steps = length // 16  # Depends on the encoder downsampling\n",
        "            z_temporal = z_proj.unsqueeze(-1).repeat(1, 1, time_steps)\n",
        "\n",
        "        # Apply convolutional decoder\n",
        "        decoded = self.decoder(z_temporal)  # [batch_size, output_channels, time]\n",
        "\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class MusicConditionedUNet(nn.Module):\n",
        "    \"\"\"UNet-style model for high-resolution audio generation conditioned on text\"\"\"\n",
        "    def __init__(self, input_channels=1, output_channels=1, base_channels=32, embedding_dim=512):\n",
        "        super(MusicConditionedUNet, self).__init__()\n",
        "\n",
        "        # Encoder (downsampling) blocks\n",
        "        self.enc1 = self._encoder_block(input_channels, base_channels)\n",
        "        self.enc2 = self._encoder_block(base_channels, base_channels*2)\n",
        "        self.enc3 = self._encoder_block(base_channels*2, base_channels*4)\n",
        "        self.enc4 = self._encoder_block(base_channels*4, base_channels*8)\n",
        "\n",
        "        # Bottleneck with text condition integration\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv1d(base_channels*8, base_channels*16, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(num_groups=8, num_channels=base_channels*16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(base_channels*16, base_channels*16, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(num_groups=8, num_channels=base_channels*16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.text_projection = nn.Linear(embedding_dim, base_channels*16)\n",
        "\n",
        "        # Decoder (upsampling) blocks\n",
        "        self.dec4 = self._decoder_block(base_channels*16, base_channels*8)\n",
        "        self.dec3 = self._decoder_block(base_channels*16, base_channels*4)  # Double because of skip connection\n",
        "        self.dec2 = self._decoder_block(base_channels*8, base_channels*2)\n",
        "        self.dec1 = self._decoder_block(base_channels*4, base_channels)\n",
        "\n",
        "        # Final convolution\n",
        "        self.final_conv = nn.Conv1d(base_channels*2, output_channels, kernel_size=1)\n",
        "\n",
        "    def _encoder_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(num_groups=4, num_channels=out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, stride=2),  # Downsample\n",
        "            nn.GroupNorm(num_groups=4, num_channels=out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def _decoder_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),  # Upsample\n",
        "            nn.GroupNorm(num_groups=4, num_channels=out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(num_groups=4, num_channels=out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, text_embedding):\n",
        "        \"\"\"\n",
        "        Forward pass with text conditioning\n",
        "        x: [batch_size, 1, time] - Initial audio or noise\n",
        "        text_embedding: [batch_size, embedding_dim] - Text embeddings\n",
        "        \"\"\"\n",
        "        # Encoder\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(enc1)\n",
        "        enc3 = self.enc3(enc2)\n",
        "        enc4 = self.enc4(enc3)\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(enc4)\n",
        "\n",
        "        # Apply text conditioning\n",
        "        text_proj = self.text_projection(text_embedding).unsqueeze(-1)\n",
        "        text_proj = text_proj.repeat(1, 1, bottleneck.size(-1))\n",
        "        bottleneck = bottleneck + 0.1 * text_proj  # Add text features with scaling\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        dec4 = self.dec4(bottleneck)\n",
        "        dec3 = self.dec3(torch.cat([dec4, enc4], dim=1))\n",
        "        dec2 = self.dec2(torch.cat([dec3, enc3], dim=1))\n",
        "        dec1 = self.dec1(torch.cat([dec2, enc2], dim=1))\n",
        "\n",
        "        # Final layer with skip connection to first encoder output\n",
        "        output = self.final_conv(torch.cat([dec1, enc1], dim=1))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class MusicLMModel(nn.Module):\n",
        "    \"\"\"Complete MusicLM-inspired model combining all components\"\"\"\n",
        "    def __init__(self, embedding_dim=512, sample_rate=16000):\n",
        "        super(MusicLMModel, self).__init__()\n",
        "\n",
        "        # Text encoder (MuLAN-inspired)\n",
        "        self.text_encoder = MuLANTextEncoder(embedding_dim=embedding_dim)\n",
        "\n",
        "        # Audio encoder (SoundStream-inspired)\n",
        "        self.audio_encoder = SoundStreamEncoder(embedding_dim=embedding_dim)\n",
        "\n",
        "        # Audio decoder (SoundStream-inspired)\n",
        "        self.audio_decoder = SoundStreamDecoder(embedding_dim=embedding_dim)\n",
        "\n",
        "        # High-resolution audio refinement\n",
        "        self.audio_unet = MusicConditionedUNet(embedding_dim=embedding_dim)\n",
        "\n",
        "        # Sampling parameters\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def encode_text(self, text_prompts):\n",
        "        \"\"\"Encode text prompts to the joint embedding space\"\"\"\n",
        "        return self.text_encoder(text_prompts)\n",
        "\n",
        "    def encode_audio(self, audio):\n",
        "        \"\"\"Encode audio to the joint embedding space\"\"\"\n",
        "        return self.audio_encoder(audio)\n",
        "\n",
        "    def decode_audio(self, embedding, length=16000):\n",
        "        \"\"\"Decode embedding to audio\"\"\"\n",
        "        return self.audio_decoder(embedding, length=length)\n",
        "\n",
        "    def refine_audio(self, audio, text_embedding):\n",
        "        \"\"\"Apply high-resolution refinement to audio based on text embedding\"\"\"\n",
        "        return self.audio_unet(audio, text_embedding)\n",
        "\n",
        "    def generate_from_text(self, text_prompt, length_seconds=10,\n",
        "                          refinement_steps=50, noise_level=0.5):\n",
        "        \"\"\"Generate audio from text prompt\"\"\"\n",
        "        # Encode text prompt\n",
        "        text_embedding = self.encode_text([text_prompt])\n",
        "\n",
        "        # Calculate audio length in samples\n",
        "        audio_length = int(length_seconds * self.sample_rate)\n",
        "\n",
        "        # Generate initial audio from embedding\n",
        "        z_audio, _ = self.audio_encoder(torch.randn(1, 1, audio_length).to(text_embedding.device))\n",
        "        rough_audio = self.audio_decoder(z_audio, length=audio_length)\n",
        "\n",
        "        # Iterative refinement using the UNet\n",
        "        current_audio = rough_audio\n",
        "\n",
        "        for step in tqdm(range(refinement_steps), desc=\"Refining audio\"):\n",
        "            # Add noise proportional to the remaining steps\n",
        "            remaining_factor = (refinement_steps - step) / refinement_steps\n",
        "            noise_scale = noise_level * remaining_factor\n",
        "            noised_audio = current_audio + torch.randn_like(current_audio) * noise_scale\n",
        "\n",
        "            # Refine using UNet with text conditioning\n",
        "            refined = self.audio_unet(noised_audio, text_embedding)\n",
        "\n",
        "            # Weighted average of previous and refined audio to ensure stability\n",
        "            alpha = min(0.1 + step / (refinement_steps * 0.8), 0.8)\n",
        "            current_audio = (1 - alpha) * current_audio + alpha * refined\n",
        "\n",
        "        return current_audio.squeeze(0)"
      ],
      "metadata": {
        "id": "KteXZy1v8SHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "class MusicTextPairDataset(Dataset):\n",
        "    \"\"\"Dataset of paired music and text descriptions\"\"\"\n",
        "    def __init__(self, data_dir, metadata_file, max_audio_length=160000, sample_rate=16000):\n",
        "        self.data_dir = data_dir\n",
        "        self.sample_rate = sample_rate\n",
        "        self.max_audio_length = max_audio_length\n",
        "\n",
        "        # Load metadata\n",
        "        self.metadata = pd.read_csv(os.path.join(data_dir, metadata_file))\n",
        "\n",
        "        # Filter out entries with missing audio or text\n",
        "        self.metadata = self.metadata.dropna(subset=['audio_file', 'text_description'])\n",
        "\n",
        "        print(f\"Loaded dataset with {len(self.metadata)} examples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get metadata for this item\n",
        "        item = self.metadata.iloc[idx]\n",
        "\n",
        "        # Load audio file\n",
        "        audio_path = os.path.join(self.data_dir, item['audio_file'])\n",
        "        waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "        # Convert to mono if needed\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Resample if needed\n",
        "        if sr != self.sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Pad or trim to max length\n",
        "        if waveform.shape[1] < self.max_audio_length:\n",
        "            # Pad\n",
        "            padding = torch.zeros(1, self.max_audio_length - waveform.shape[1])\n",
        "            waveform = torch.cat([waveform, padding], dim=1)\n",
        "        else:\n",
        "            # Trim\n",
        "            waveform = waveform[:, :self.max_audio_length]\n",
        "\n",
        "        # Get text description\n",
        "        text = item['text_description']\n",
        "\n",
        "        # Get any additional metadata for fine-grained conditioning\n",
        "        metadata = {}\n",
        "        for col in self.metadata.columns:\n",
        "            if col not in ['audio_file', 'text_description']:\n",
        "                metadata[col] = item[col]\n",
        "\n",
        "        return {\n",
        "            'audio': waveform,\n",
        "            'text': text,\n",
        "            'metadata': metadata\n",
        "        }\n",
        "\n",
        "def prepare_musiccaps_dataset():\n",
        "    \"\"\"\n",
        "    Prepare dataset based on MusicCaps or similar dataset structure\n",
        "\n",
        "    Note: This is a placeholder implementation. You would need to:\n",
        "    1. Download the MusicCaps dataset or similar\n",
        "    2. Create a metadata CSV with audio_file and text_description columns\n",
        "    \"\"\"\n",
        "    # This is a placeholder - you would implement actual data download and preparation\n",
        "    # For a real implementation, you might use the AudioSet-based MusicCaps dataset\n",
        "\n",
        "    # Create a sample metadata file structure\n",
        "    sample_data = []\n",
        "    for i in range(100):\n",
        "        sample_data.append({\n",
        "            'audio_file': f'audio/sample_{i}.wav',\n",
        "            'text_description': f'A {[\"happy\", \"sad\", \"energetic\", \"calm\"][i%4]} music piece with {[\"piano\", \"guitar\", \"drums\", \"violin\"][i%4]}',\n",
        "            'genre': ['classical', 'rock', 'jazz', 'electronic'][i%4],\n",
        "            'tempo': np.random.randint(60, 180)\n",
        "        })\n",
        "\n",
        "    metadata = pd.DataFrame(sample_data)\n",
        "    metadata.to_csv('sample_metadata.csv', index=False)\n",
        "\n",
        "    print(\"Created sample metadata file for demonstration\")\n",
        "    print(\"In a real implementation, you'd need to download the actual audio files\")\n",
        "\n",
        "def create_audio_synthesizer_dataset(text_processor, emotion_extractor, music_generator,\n",
        "                                    output_dir, num_samples=1000):\n",
        "    \"\"\"\n",
        "    Create a dataset of MIDI files and corresponding text descriptions\n",
        "    for training the audio synthesis model\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, 'midi'), exist_ok=True)\n",
        "\n",
        "    metadata = []\n",
        "\n",
        "    # Generate example texts (for demonstration)\n",
        "    example_texts = [\n",
        "        \"A bright sunny day with birds chirping in the trees\",\n",
        "        \"The storm raged through the night, thunder shaking the windows\",\n",
        "        \"She felt a deep sadness as she read the letter from her old friend\",\n",
        "        \"The excitement of the race filled the stadium with energy\",\n",
        "        \"A quiet moment of reflection by the peaceful lake\"\n",
        "    ]\n",
        "\n",
        "    # Extend examples by repeating and modifying\n",
        "    expanded_texts = []\n",
        "    for _ in range(num_samples // len(example_texts) + 1):\n",
        "        for text in example_texts:\n",
        "            # Add some variation\n",
        "            words = text.split()\n",
        "            if len(words) > 10:\n",
        "                # Randomly remove or duplicate some words\n",
        "                if np.random.random() > 0.5:\n",
        "                    remove_idx = np.random.randint(0, len(words))\n",
        "                    words.pop(remove_idx)\n",
        "                else:\n",
        "                    dup_idx = np.random.randint(0, len(words))\n",
        "                    words.insert(dup_idx, words[dup_idx])\n",
        "\n",
        "            expanded_texts.append(\" \".join(words))\n",
        "\n",
        "    expanded_texts = expanded_texts[:num_samples]\n",
        "\n",
        "    # Process each text\n",
        "    for i, text in enumerate(tqdm(expanded_texts, desc=\"Generating dataset\")):\n",
        "        # Process text to extract emotions\n",
        "        _, cleaned_segments = text_processor.process_text(text)\n",
        "        emotion_maps = emotion_extractor.extract_emotions(cleaned_segments)\n",
        "\n",
        "        # For simplicity, use only the first segment\n",
        "        if emotion_maps:\n",
        "            emotion_map = emotion_maps[0]\n",
        "\n",
        "            # Convert to emotion vector\n",
        "            emotion_vector = torch.tensor([\n",
        "                emotion_map['joy'], emotion_map['sadness'], emotion_map['anger'],\n",
        "                emotion_map['fear'], emotion_map['surprise'], emotion_map['disgust'],\n",
        "                emotion_map['neutral']\n",
        "            ], dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            # Map to musical features\n",
        "            with torch.no_grad():\n",
        "                features = music_generator.emotion_to_music_mapper(emotion_vector)\n",
        "\n",
        "            # Convert to actual values\n",
        "            actual_values = music_generator.emotion_to_music_mapper.map_to_actual_values(features.squeeze(0))\n",
        "\n",
        "            # Generate MusicLM prompt\n",
        "            prompt = music_generator.emotion_to_music_mapper.generate_musiclm_prompt(\n",
        "                actual_values, emotion_scores=emotion_map\n",
        "            )\n",
        "\n",
        "            # Generate MIDI\n",
        "            midi_buffer = music_generator.music_generator.create_midi_from_features(actual_values)\n",
        "\n",
        "            # Save MIDI\n",
        "            midi_filename = f'sample_{i:04d}.mid'\n",
        "            midi_path = os.path.join(output_dir, 'midi', midi_filename)\n",
        "\n",
        "            with open(midi_path, 'wb') as f:\n",
        "                f.write(midi_buffer.getvalue())\n",
        "\n",
        "            # Add to metadata\n",
        "            metadata.append({\n",
        "                'midi_file': f'midi/{midi_filename}',\n",
        "                'text_description': prompt,\n",
        "                'original_text': text,\n",
        "                'joy': float(emotion_map['joy']),\n",
        "                'sadness': float(emotion_map['sadness']),\n",
        "                'anger': float(emotion_map['anger']),\n",
        "                'fear': float(emotion_map['fear']),\n",
        "                'surprise': float(emotion_map['surprise']),\n",
        "                'disgust': float(emotion_map['disgust']),\n",
        "                'neutral': float(emotion_map['neutral']),\n",
        "                'tempo': float(actual_values['tempo']),\n",
        "                'key': int(actual_values['key']),\n",
        "                'mode': float(actual_values['mode']),\n",
        "                'intensity': float(actual_values['intensity'])\n",
        "            })\n",
        "\n",
        "    # Save metadata\n",
        "    metadata_df = pd.DataFrame(metadata)\n",
        "    metadata_path = os.path.join(output_dir, 'metadata.csv')\n",
        "    metadata_df.to_csv(metadata_path, index=False)\n",
        "\n",
        "    print(f\"Created dataset with {len(metadata)} examples\")\n",
        "    print(f\"Metadata saved to {metadata_path}\")\n",
        "\n",
        "    return metadata_path"
      ],
      "metadata": {
        "id": "ImEVkmRH8Sqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2FgrLcl2-IiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5pwrVp3ZLfcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bSrwRNz0LfZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClE7DnJFLfVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📍 SETUP & DEPENDENCIES\n",
        "!pip install transformers torchaudio librosa datasets\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import librosa\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 📌 PART 1: MuLanEmbedding\n",
        "class MuLanEmbedding(nn.Module):\n",
        "    def __init__(self, input_dim=256):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# 📌 PART 2: SoundStreamDecoder\n",
        "class SoundStreamDecoder(nn.Module):\n",
        "    def __init__(self, input_dim=256):\n",
        "        super().__init__()\n",
        "        self.decoder = nn.Linear(input_dim, 16000)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "# 📌 PART 3: CustomMusicLM\n",
        "class CustomMusicLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = MuLanEmbedding()\n",
        "        self.decoder = SoundStreamDecoder()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        embedded = self.embedding(torch.randn(input_ids.size(0), 256))\n",
        "        audio = self.decoder(embedded)\n",
        "        return audio\n",
        "\n",
        "# 📌 PART 5: EmotionExtractor\n",
        "class EmotionExtractor:\n",
        "    def __init__(self):\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
        "        self.model.eval()\n",
        "\n",
        "    def extract_emotions(self, text_segments):\n",
        "        emotions = []\n",
        "        for segment in text_segments:\n",
        "            inputs = self.tokenizer(segment, return_tensors=\"pt\", truncation=True)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "            probs = F.softmax(outputs.logits, dim=-1)\n",
        "            label = torch.argmax(probs, dim=1).item()\n",
        "            emotions.append(label)\n",
        "        return emotions\n",
        "\n",
        "# 📌 PART 6: TextProcessor\n",
        "class TextProcessor:\n",
        "    def split_text(self, text, segment_length=50):\n",
        "        words = text.split()\n",
        "        return [\" \".join(words[i:i+segment_length]) for i in range(0, len(words), segment_length)]\n",
        "\n",
        "# 📌 PART 7: EmotionToPromptMapper\n",
        "class EmotionToPromptMapper:\n",
        "    def map_emotions_to_prompts(self, emotions):\n",
        "        emotion_to_prompt = {\n",
        "            0: \"sad and slow\",\n",
        "            1: \"joyful and bright\",\n",
        "            2: \"angry and intense\",\n",
        "            3: \"calm and peaceful\",\n",
        "            4: \"scared and tense\",\n",
        "            5: \"surprised and whimsical\"\n",
        "        }\n",
        "        return [emotion_to_prompt.get(e, \"neutral\") for e in emotions]\n",
        "\n",
        "# 📌 PART 10: DummyMusicDataset\n",
        "class DummyMusicDataset(Dataset):\n",
        "    def __init__(self, tokenizer, prompts, waveforms):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.prompts = prompts\n",
        "        self.waveforms = waveforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokenizer(\n",
        "            self.prompts[idx], padding='max_length', truncation=True, max_length=128, return_tensors='pt'\n",
        "        )\n",
        "        return tokens['input_ids'].squeeze(), tokens['attention_mask'].squeeze(), self.waveforms[idx]\n",
        "\n",
        "# 📌 PART 9: Sample Data\n",
        "prompts = [\"A happy forest adventure\", \"A tragic night alone\"]\n",
        "dummy_waveforms = [torch.randn(1, 256) for _ in range(len(prompts))]\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_dataset = DummyMusicDataset(tokenizer, prompts, dummy_waveforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2)\n",
        "\n",
        "# 📌 PART 8: Training Function\n",
        "def train_custom_musiclm(model, dataloader, num_epochs=3):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for input_ids, attention_mask, waveforms in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, waveforms.view(outputs.shape))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 📌 PART 4: Main Generator Class\n",
        "class LiteraryMusicGenerator:\n",
        "    def __init__(self):\n",
        "        self.text_processor = TextProcessor()\n",
        "        self.emotion_extractor = EmotionExtractor()\n",
        "        self.mapper = EmotionToPromptMapper()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.custom_musiclm = CustomMusicLM()\n",
        "\n",
        "    def generate_music(self, text):\n",
        "        segments = self.text_processor.split_text(text)\n",
        "        emotions = self.emotion_extractor.extract_emotions(segments)\n",
        "        text_prompts = self.mapper.map_emotions_to_prompts(emotions)\n",
        "        self.audio_waveforms = use_custom_musiclm(self, text_prompts)\n",
        "        return self.audio_waveforms\n",
        "\n",
        "# 📌 PART 11: Generation Wrapper\n",
        "def use_custom_musiclm(literary_music_generator, text_prompts):\n",
        "    custom_musiclm = literary_music_generator.custom_musiclm\n",
        "    tokenizer = literary_music_generator.tokenizer\n",
        "\n",
        "    audio_outputs = []\n",
        "    for prompt in text_prompts:\n",
        "        encoded_input = tokenizer(prompt, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "        with torch.no_grad():\n",
        "            audio_output = custom_musiclm(encoded_input['input_ids'], encoded_input['attention_mask'])\n",
        "        audio_outputs.append(audio_output.squeeze().cpu().numpy())\n",
        "    return audio_outputs\n",
        "\n",
        "# ✅ TRAIN AND GENERATE\n",
        "model = CustomMusicLM()\n",
        "train_custom_musiclm(model, train_loader, num_epochs=3)\n",
        "generator = LiteraryMusicGenerator()\n",
        "generator.custom_musiclm = model\n",
        "\n",
        "sample_text = \"The sun rose gently, casting golden hues over the quiet meadow. A feeling of joy filled the air.\"\n",
        "audio_waveforms = generator.generate_music(sample_text)\n",
        "\n",
        "print(\"Generated\", len(audio_waveforms), \"audio segments 🎵\")\n"
      ],
      "metadata": {
        "id": "IgiZ3ZrJL8yN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "782083788e3e46e28634090150abc160",
            "cd80b17142f44067805c16cd8c0485d9",
            "c98ee5f421ea465e92fa2f56f0b3d76a",
            "ef3f0843864a407393c816a3e969f48a",
            "9094318b92414f20a005dd7865839557",
            "cdd76e6c72684211812ee840618130ef",
            "d52ef34e687540d4bfc5ad7f0ccd1476",
            "4db7b506259b4ad3b299b7b5a420b876",
            "6d3c8fd60a4e4947b31494a6a7890b95",
            "d1fcc3efebd144dcb34837e4a8451c1b",
            "033291e50b5c4a8d950543c2dca8435d",
            "914ffddea0c44cceb74f3e8cd1e48959",
            "2181ad258d354daa9643625ef377a410",
            "99b09c6694c84f6185f661c9f7c93f20",
            "98328ed4b8924369b5bbbfb8081811f3",
            "c7fba3449213432a84f4d67ea7fece20",
            "a1c53c2bde2e4fe89937f16c1b93d6b2",
            "054901e20828436ea16ead085b753c26",
            "6c5a437ef9ac49e6ae3df04d90cdb533",
            "b5f294e059e3459499328dca51c62f00",
            "bd17fed08b9448c3aac4a2d672b84db8",
            "e6112a3dddff44ba90904784d06cf29a",
            "9e5894d2bf194e60a613753d7616cd04",
            "2f95457e068740d08cfd638080764bda",
            "89a3c298cc44493a84b484581d855ad7",
            "6b2f569d0efc461686d73f04c119b4d4",
            "05a3cea4c2f04711987406e85600994c",
            "67bbcca04b7e4142bed78badb045f6f5",
            "fa133a2b1d054dbeb93b23fff1b55764",
            "73240ff07b4f4b1ea97d768de4eba257",
            "a7cf5ea451d644be9db5ea174e45f808",
            "94da7fe55c404c9f81e5731b67429170",
            "e49a26f8c11d44bfbcf5975da223e140",
            "e115e3f334d74388b0a940ee05bf2bb5",
            "082f8a6f47034958801c197aff69c14e",
            "89b797a9ff294ac98235515a0e07fda0",
            "b7901698232c40f78379ba1b5cd9bd35",
            "5f8dbd878d1c48adb019c01274fee69a",
            "a87e35ea35434246996c28376741eca6",
            "a925e71c86214980b33b5b6daf88d93b",
            "1a9e88fa6cfb4fc2b43ec625ec0318d1",
            "900fe6ac95e1412692cc2d05af86b4d4",
            "245a2e5df53246da8669f997dcca2241",
            "9ab1d058d3af438586c972b42873315d"
          ]
        },
        "outputId": "dc9d69f5-4ace-499b-ddea-d8a222fb08ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchaudio)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch==2.6.0->torchaudio)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "782083788e3e46e28634090150abc160"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "914ffddea0c44cceb74f3e8cd1e48959"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e5894d2bf194e60a613753d7616cd04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e115e3f334d74388b0a940ee05bf2bb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[2, 16000]' is invalid for input of size 512",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5c671bc5a92c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m# ✅ TRAIN AND GENERATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomMusicLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mtrain_custom_musiclm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLiteraryMusicGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_musiclm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5c671bc5a92c>\u001b[0m in \u001b[0;36mtrain_custom_musiclm\u001b[0;34m(model, dataloader, num_epochs)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwaveforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 16000]' is invalid for input of size 512"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install midiutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ecqxL8e1lsM",
        "outputId": "245d6869-147c-47ba-edf9-c10df2ce5255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting midiutil\n",
            "  Downloading MIDIUtil-1.2.1.tar.gz (1.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: midiutil\n",
            "  Building wheel for midiutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for midiutil: filename=MIDIUtil-1.2.1-py3-none-any.whl size=54569 sha256=56f0e829aaef33b2ef2c28122f99f805b9f240a3533b0e03b1e6006c67e34497\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/42/75/fce10c67f06fe627fad8acd1fd3a004a24e07b0f077761fbbd\n",
            "Successfully built midiutil\n",
            "Installing collected packages: midiutil\n",
            "Successfully installed midiutil-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pretty_midi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0vRQ62B1a9x",
        "outputId": "af5feb44-5edd-4979-cc38-94f841ff8d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (2.0.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido>=1.1.16->pretty_midi) (24.2)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=2fe33c2bcbfa712e0f1c0852b43378bac76a2dc1d8ac9d30dfd44ce99095e665\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi\n",
            "Successfully installed mido-1.3.3 pretty_midi-0.2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkzjNpHf1tiX",
        "outputId": "1763be0d-28aa-4925-931f-a35328cc20d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import json\n",
        "import os\n",
        "from google.colab import files\n",
        "from midiutil import MIDIFile\n",
        "import pretty_midi\n",
        "import IPython.display as ipd\n",
        "import random\n",
        "import librosa\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "import io\n",
        "import base64\n",
        "import tempfile\n",
        "\n",
        "# Download NLTK resources properly\n",
        "# This ensures the data is downloaded correctly in Colab\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Make sure the above downloads complete before proceeding\n",
        "try:\n",
        "    word_tokenize(\"Testing NLTK\")\n",
        "    print(\"NLTK resources downloaded successfully!\")\n",
        "except LookupError:\n",
        "    print(\"Downloading additional NLTK resources...\")\n",
        "    # Alternative download method\n",
        "    !python -m nltk.downloader punkt\n",
        "    !python -m nltk.downloader stopwords\n",
        "    !python -m nltk.downloader wordnet\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        # Make sure stopwords are available\n",
        "        try:\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "        except LookupError:\n",
        "            nltk.download('stopwords')\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Make sure wordnet is available\n",
        "        try:\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "        except LookupError:\n",
        "            nltk.download('wordnet')\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Remove punctuation, lowercase, remove stopwords, and lemmatize\"\"\"\n",
        "        # Lowercase the text\n",
        "        text = text.lower()\n",
        "        # Remove punctuation\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        # Try to tokenize, with fallback method if it fails\n",
        "        try:\n",
        "            # Tokenize into words\n",
        "            words = word_tokenize(text)\n",
        "        except LookupError:\n",
        "            # Fallback: simple space-based tokenization\n",
        "            words = text.split()\n",
        "\n",
        "        # Remove stopwords and lemmatize\n",
        "        cleaned_words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]\n",
        "        return ' '.join(cleaned_words)\n",
        "\n",
        "    def segment_text(self, text, segment_size=500):\n",
        "        \"\"\"Split text into segments of roughly equal size\"\"\"\n",
        "        try:\n",
        "            sentences = sent_tokenize(text)\n",
        "        except LookupError:\n",
        "            # Fallback: simple period-based sentence splitting\n",
        "            sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]\n",
        "\n",
        "        segments = []\n",
        "        current_segment = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_length = len(sentence)\n",
        "            if current_length + sentence_length > segment_size and current_segment:\n",
        "                segments.append(' '.join(current_segment))\n",
        "                current_segment = [sentence]\n",
        "                current_length = sentence_length\n",
        "            else:\n",
        "                current_segment.append(sentence)\n",
        "                current_length += sentence_length\n",
        "\n",
        "        # Add the last segment if it exists\n",
        "        if current_segment:\n",
        "            segments.append(' '.join(current_segment))\n",
        "\n",
        "        return segments\n",
        "\n",
        "    def process_text(self, text, segment_size=500):\n",
        "        \"\"\"Process the full text: segment first, then clean each segment\"\"\"\n",
        "        segments = self.segment_text(text, segment_size)\n",
        "        cleaned_segments = [self.clean_text(segment) for segment in segments]\n",
        "        # Also keep the original segments for display purposes\n",
        "        return segments, cleaned_segments\n",
        "\n",
        "class EmotionExtractor:\n",
        "    def __init__(self):\n",
        "        # Load pre-trained emotion classification model\n",
        "        try:\n",
        "            self.emotion_classifier = pipeline(\n",
        "                \"text-classification\",\n",
        "                model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "                return_all_scores=True\n",
        "            )\n",
        "        except:\n",
        "            # Handle the warning about return_all_scores being deprecated\n",
        "            self.emotion_classifier = pipeline(\n",
        "                \"text-classification\",\n",
        "                model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "                top_k=None  # Using top_k=None instead of return_all_scores=True\n",
        "            )\n",
        "\n",
        "        # Define our emotion categories\n",
        "        self.emotion_categories = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'neutral']\n",
        "\n",
        "    def extract_emotions(self, text_segments):\n",
        "        \"\"\"Extract emotions from text segments\"\"\"\n",
        "        emotion_maps = []\n",
        "\n",
        "        for segment in text_segments:\n",
        "            # Get emotion scores for the segment\n",
        "            emotion_scores = self.emotion_classifier(segment)[0]\n",
        "\n",
        "            # Convert to dictionary with emotion as key and score as value\n",
        "            emotion_dict = {item['label']: item['score'] for item in emotion_scores}\n",
        "\n",
        "            # Map the model's emotions to our simplified set if needed\n",
        "            mapped_emotions = {\n",
        "                'joy': emotion_dict.get('joy', 0),\n",
        "                'sadness': emotion_dict.get('sadness', 0),\n",
        "                'anger': emotion_dict.get('anger', 0),\n",
        "                'fear': emotion_dict.get('fear', 0),\n",
        "                'surprise': emotion_dict.get('surprise', 0),\n",
        "                'disgust': emotion_dict.get('disgust', 0),\n",
        "                'neutral': emotion_dict.get('neutral', 0)\n",
        "            }\n",
        "\n",
        "            emotion_maps.append(mapped_emotions)\n",
        "\n",
        "        return emotion_maps\n",
        "\n",
        "    def create_emotional_progression(self, emotion_maps):\n",
        "        \"\"\"Create a time series of emotions for the entire text\"\"\"\n",
        "        progression = {emotion: [] for emotion in self.emotion_categories}\n",
        "\n",
        "        for emotion_map in emotion_maps:\n",
        "            for emotion in self.emotion_categories:\n",
        "                progression[emotion].append(emotion_map[emotion])\n",
        "\n",
        "        return progression\n",
        "\n",
        "    def get_dominant_emotions(self, emotion_maps, top_n=2):\n",
        "        \"\"\"Get the dominant emotions for each segment\"\"\"\n",
        "        dominant_emotions = []\n",
        "\n",
        "        for emotion_map in emotion_maps:\n",
        "            # Sort emotions by score\n",
        "            sorted_emotions = sorted(emotion_map.items(), key=lambda x: x[1], reverse=True)\n",
        "            # Take top n emotions\n",
        "            top_emotions = sorted_emotions[:top_n]\n",
        "            dominant_emotions.append(top_emotions)\n",
        "\n",
        "        return dominant_emotions\n",
        "class EmotionToMusicMapper(nn.Module):\n",
        "    def __init__(self, input_dim=7, hidden_dim=64, output_dim=10):\n",
        "        super(EmotionToMusicMapper, self).__init__()\n",
        "\n",
        "        # Neural network for mapping emotions to musical features\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Sigmoid()  # Normalized output\n",
        "        )\n",
        "\n",
        "        # Define music feature mappings\n",
        "        self.musical_features = {\n",
        "            'tempo': {'min': 60, 'max': 180},     # BPM\n",
        "            'key': {'min': 0, 'max': 11},         # C=0, C#=1, ..., B=11\n",
        "            'mode': {'min': 0, 'max': 1},         # Minor=0, Major=1\n",
        "            'intensity': {'min': 0, 'max': 1},    # Soft to loud\n",
        "            'instrumentation': {'min': 0, 'max': 5},  # Different instrument groups\n",
        "            'rhythm_complexity': {'min': 0, 'max': 1},\n",
        "            'harmonic_complexity': {'min': 0, 'max': 1},\n",
        "            'melodic_range': {'min': 0, 'max': 1},\n",
        "            'texture': {'min': 0, 'max': 1},      # Sparse to dense\n",
        "            'articulation': {'min': 0, 'max': 1}  # Staccato to legato\n",
        "        }\n",
        "\n",
        "        # Map features to index in output\n",
        "        self.feature_to_idx = {feature: i for i, feature in enumerate(self.musical_features.keys())}\n",
        "\n",
        "        # Define emotional correlations (expanded with more nuanced mappings)\n",
        "        self.emotion_correlations = {\n",
        "            'joy': {\n",
        "                'tempo': 'high',\n",
        "                'mode': 'major',\n",
        "                'intensity': 'moderate-high',\n",
        "                'rhythm_complexity': 'moderate',\n",
        "                'harmonic_complexity': 'moderate',\n",
        "                'instrumentation': ['piano', 'strings', 'synth'],\n",
        "                'description': ['uplifting', 'bright', 'cheerful', 'buoyant', 'exuberant', 'playful', 'optimistic']\n",
        "            },\n",
        "            'sadness': {\n",
        "                'tempo': 'low',\n",
        "                'mode': 'minor',\n",
        "                'intensity': 'low',\n",
        "                'rhythm_complexity': 'low',\n",
        "                'harmonic_complexity': 'moderate-high',\n",
        "                'instrumentation': ['piano', 'strings', 'guitar'],\n",
        "                'description': ['melancholic', 'somber', 'wistful', 'contemplative', 'haunting', 'bittersweet', 'mournful']\n",
        "            },\n",
        "            'anger': {\n",
        "                'tempo': 'high',\n",
        "                'mode': 'minor',\n",
        "                'intensity': 'high',\n",
        "                'rhythm_complexity': 'high',\n",
        "                'harmonic_complexity': 'high',\n",
        "                'instrumentation': ['synth', 'percussion', 'orchestral'],\n",
        "                'description': ['intense', 'aggressive', 'powerful', 'driving', 'dissonant', 'chaotic', 'turbulent']\n",
        "            },\n",
        "            'fear': {\n",
        "                'tempo': 'variable',\n",
        "                'mode': 'minor',\n",
        "                'intensity': 'variable',\n",
        "                'rhythm_complexity': 'low',\n",
        "                'harmonic_complexity': 'high',\n",
        "                'instrumentation': ['strings', 'synth', 'percussion'],\n",
        "                'description': ['tense', 'suspenseful', 'eerie', 'unsettling', 'mysterious', 'foreboding', 'chilling']\n",
        "            },\n",
        "            'surprise': {\n",
        "                'tempo': 'variable',\n",
        "                'mode': 'variable',\n",
        "                'intensity': 'variable',\n",
        "                'rhythm_complexity': 'high',\n",
        "                'harmonic_complexity': 'moderate',\n",
        "                'instrumentation': ['piano', 'synth', 'orchestral'],\n",
        "                'description': ['unexpected', 'quirky', 'sudden', 'playful', 'whimsical', 'unpredictable', 'startling']\n",
        "            },\n",
        "            'disgust': {\n",
        "                'tempo': 'low-moderate',\n",
        "                'mode': 'minor',\n",
        "                'intensity': 'moderate',\n",
        "                'rhythm_complexity': 'moderate',\n",
        "                'harmonic_complexity': 'high',\n",
        "                'instrumentation': ['synth', 'percussion', 'orchestral'],\n",
        "                'description': ['dissonant', 'unsettling', 'gritty', 'uncomfortable', 'jarring', 'off-kilter', 'distorted']\n",
        "            },\n",
        "            'neutral': {\n",
        "                'tempo': 'moderate',\n",
        "                'mode': 'variable',\n",
        "                'intensity': 'moderate',\n",
        "                'rhythm_complexity': 'moderate',\n",
        "                'harmonic_complexity': 'moderate',\n",
        "                'instrumentation': ['piano', 'strings', 'guitar'],\n",
        "                'description': ['balanced', 'ambient', 'atmospheric', 'calm', 'steady', 'peaceful', 'flowing']\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def forward(self, emotion_vector):\n",
        "        \"\"\"Map emotion vector to musical features\"\"\"\n",
        "        return self.network(emotion_vector)\n",
        "\n",
        "    def map_to_actual_values(self, normalized_features):\n",
        "        \"\"\"Convert normalized outputs to actual musical values\"\"\"\n",
        "        actual_values = {}\n",
        "\n",
        "        for feature, feature_range in self.musical_features.items():\n",
        "            idx = self.feature_to_idx[feature]\n",
        "            norm_value = normalized_features[idx].item()\n",
        "\n",
        "            # Scale to the actual range\n",
        "            min_val = feature_range['min']\n",
        "            max_val = feature_range['max']\n",
        "            actual_value = min_val + norm_value * (max_val - min_val)\n",
        "\n",
        "            # Round as needed\n",
        "            if feature in ['key', 'instrumentation']:\n",
        "                actual_value = round(actual_value)\n",
        "\n",
        "            actual_values[feature] = actual_value\n",
        "\n",
        "        return actual_values\n",
        "\n",
        "    def get_descriptors_for_emotion_blend(self, emotion_scores):\n",
        "        \"\"\"Get appropriate musical descriptors based on a blend of emotions\"\"\"\n",
        "        # Get top 2 emotions\n",
        "        sorted_emotions = sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_emotions = sorted_emotions[:2]\n",
        "\n",
        "        descriptors = []\n",
        "        instruments = set()\n",
        "\n",
        "        # Weighted selection of descriptors based on emotion intensity\n",
        "        total_weight = sum(score for _, score in top_emotions)\n",
        "\n",
        "        for emotion, score in top_emotions:\n",
        "            # Skip emotions with very low scores\n",
        "            if score < 0.1:\n",
        "                continue\n",
        "\n",
        "            # Weight by the emotion's intensity\n",
        "            weight = score / total_weight\n",
        "\n",
        "            # Get descriptors for this emotion\n",
        "            emotion_descriptors = self.emotion_correlations[emotion]['description']\n",
        "\n",
        "            # Add descriptors proportional to the emotion's weight\n",
        "            num_descriptors = max(1, int(weight * 3))  # At least 1, up to 3 descriptors\n",
        "            selected_descriptors = random.sample(emotion_descriptors, min(num_descriptors, len(emotion_descriptors)))\n",
        "            descriptors.extend(selected_descriptors)\n",
        "\n",
        "            # Add potential instruments\n",
        "            emotion_instruments = self.emotion_correlations[emotion]['instrumentation']\n",
        "            # Choose 1-2 instruments based on weight\n",
        "            num_instruments = max(1, int(weight * 2))\n",
        "            selected_instruments = random.sample(emotion_instruments, min(num_instruments, len(emotion_instruments)))\n",
        "            instruments.update(selected_instruments)\n",
        "\n",
        "        # Return unique descriptors and instruments\n",
        "        return list(set(descriptors)), list(instruments)\n",
        "\n",
        "    def generate_musiclm_prompt(self, musical_features, emotion_scores=None):\n",
        "        \"\"\"Convert musical features to a MusicLM text prompt\"\"\"\n",
        "        # Map key number to name\n",
        "        key_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
        "        key_name = key_names[round(musical_features['key'])]\n",
        "\n",
        "        # Map mode number to name\n",
        "        mode_name = \"major\" if musical_features['mode'] > 0.5 else \"minor\"\n",
        "\n",
        "        # Map instrumentation to instrument types\n",
        "        instrument_types = ['piano', 'strings', 'guitar', 'synth', 'orchestral', 'percussion']\n",
        "        instrument = instrument_types[round(musical_features['instrumentation'])]\n",
        "\n",
        "        # Determine tempo description\n",
        "        tempo = musical_features['tempo']\n",
        "        if tempo < 80:\n",
        "            tempo_desc = \"slow\"\n",
        "        elif tempo < 120:\n",
        "            tempo_desc = \"moderate\"\n",
        "        else:\n",
        "            tempo_desc = \"fast\"\n",
        "\n",
        "        # Determine intensity description\n",
        "        intensity = musical_features['intensity']\n",
        "        if intensity < 0.3:\n",
        "            intensity_desc = \"soft\"\n",
        "        elif intensity < 0.7:\n",
        "            intensity_desc = \"moderate\"\n",
        "        else:\n",
        "            intensity_desc = \"powerful\"\n",
        "\n",
        "        # Get emotional descriptors if emotion scores are provided\n",
        "        if emotion_scores:\n",
        "            emotional_descriptors, suggested_instruments = self.get_descriptors_for_emotion_blend(emotion_scores)\n",
        "\n",
        "            # Use suggested instruments if available, otherwise use the mapped one\n",
        "            if suggested_instruments:\n",
        "                # Prioritize the instrument from musical features but include others\n",
        "                all_instruments = [instrument] + [i for i in suggested_instruments if i != instrument]\n",
        "                # Take up to 2 instruments\n",
        "                instrument_phrase = \" and \".join(all_instruments[:2])\n",
        "            else:\n",
        "                instrument_phrase = instrument\n",
        "\n",
        "            # Build a more emotionally nuanced prompt\n",
        "            prompt = f\"A {' and '.join(emotional_descriptors[:2])} piece in {key_name} {mode_name}, \"\n",
        "            prompt += f\"{tempo_desc} tempo, {intensity_desc} in intensity, \"\n",
        "            prompt += f\"featuring {instrument_phrase}, \"\n",
        "\n",
        "        else:\n",
        "            # Use the original approach as fallback\n",
        "            # Build the prompt\n",
        "            prompt = f\"A {intensity_desc} {tempo_desc} melody in {key_name} {mode_name}, \"\n",
        "\n",
        "            # Add texture\n",
        "            texture = musical_features['texture']\n",
        "            if texture < 0.3:\n",
        "                prompt += \"with a sparse arrangement, \"\n",
        "            elif texture > 0.7:\n",
        "                prompt += \"with a dense, layered arrangement, \"\n",
        "\n",
        "            # Add instrumentation\n",
        "            prompt += f\"featuring {instrument}, \"\n",
        "\n",
        "        # Add complexity\n",
        "        rhythm_complexity = musical_features['rhythm_complexity']\n",
        "        harmonic_complexity = musical_features['harmonic_complexity']\n",
        "\n",
        "        if rhythm_complexity > 0.7 and harmonic_complexity > 0.7:\n",
        "            prompt += \"with complex rhythms and harmonies, \"\n",
        "        elif rhythm_complexity > 0.7:\n",
        "            prompt += \"with complex rhythms, \"\n",
        "        elif harmonic_complexity > 0.7:\n",
        "            prompt += \"with rich harmonies, \"\n",
        "        elif rhythm_complexity < 0.3 and harmonic_complexity < 0.3:\n",
        "            prompt += \"with simple, straightforward patterns, \"\n",
        "\n",
        "        # Add articulation\n",
        "        articulation = musical_features['articulation']\n",
        "        if articulation < 0.3:\n",
        "            prompt += \"played with staccato articulation.\"\n",
        "        elif articulation > 0.7:\n",
        "            prompt += \"played with smooth, legato phrasing.\"\n",
        "        else:\n",
        "            prompt += \"with balanced articulation.\"\n",
        "\n",
        "        # Add emotional context at the end for a more complete prompt\n",
        "        if emotion_scores:\n",
        "            top_emotion = max(emotion_scores.items(), key=lambda x: x[1])\n",
        "            if top_emotion[1] > 0.3:  # Only if the emotion is significant\n",
        "                prompt += f\" The music conveys a sense of {top_emotion[0]}.\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def generate_transition_prompt(self, prev_features, current_features, prev_emotions, current_emotions):\n",
        "        \"\"\"Generate a prompt specifically for transitions between emotional states\"\"\"\n",
        "        # Identify the most significant emotional shift\n",
        "        prev_top_emotion = max(prev_emotions.items(), key=lambda x: x[1])\n",
        "        current_top_emotion = max(current_emotions.items(), key=lambda x: x[1])\n",
        "\n",
        "        # Check if there's a significant emotional shift\n",
        "        if prev_top_emotion[0] != current_top_emotion[0] and prev_top_emotion[1] > 0.3 and current_top_emotion[1] > 0.3:\n",
        "            transition_type = f\"transition from {prev_top_emotion[0]} to {current_top_emotion[0]}\"\n",
        "        else:\n",
        "            # If no major emotional shift, focus on musical parameter changes\n",
        "            changes = []\n",
        "\n",
        "            # Check tempo change\n",
        "            prev_tempo = prev_features['tempo']\n",
        "            current_tempo = current_features['tempo']\n",
        "            if abs(prev_tempo - current_tempo) > 20:  # Significant tempo change\n",
        "                direction = \"accelerating\" if current_tempo > prev_tempo else \"decelerating\"\n",
        "                changes.append(direction)\n",
        "\n",
        "            # Check intensity change\n",
        "            prev_intensity = prev_features['intensity']\n",
        "            current_intensity = current_features['intensity']\n",
        "            if abs(prev_intensity - current_intensity) > 0.3:  # Significant intensity change\n",
        "                direction = \"building in intensity\" if current_intensity > prev_intensity else \"becoming more subdued\"\n",
        "                changes.append(direction)\n",
        "\n",
        "            # Check mode change\n",
        "            prev_mode = \"major\" if prev_features['mode'] > 0.5 else \"minor\"\n",
        "            current_mode = \"major\" if current_features['mode'] > 0.5 else \"minor\"\n",
        "            if prev_mode != current_mode:\n",
        "                changes.append(f\"shifting from {prev_mode} to {current_mode}\")\n",
        "\n",
        "            if changes:\n",
        "                transition_type = \", \".join(changes)\n",
        "            else:\n",
        "                transition_type = \"gradual transition\"\n",
        "\n",
        "        # Generate the transition prompt\n",
        "        prompt = f\"A {transition_type} that maintains musical coherence while \"\n",
        "\n",
        "        # Add emotional descriptors\n",
        "        prev_descriptors, _ = self.get_descriptors_for_emotion_blend(prev_emotions)\n",
        "        current_descriptors, _ = self.get_descriptors_for_emotion_blend(current_emotions)\n",
        "\n",
        "        if prev_descriptors and current_descriptors:\n",
        "            prompt += f\"evolving from {prev_descriptors[0]} to {current_descriptors[0]}. \"\n",
        "\n",
        "        # Add instrumentation continuity\n",
        "        instrument_types = ['piano', 'strings', 'guitar', 'synth', 'orchestral', 'percussion']\n",
        "        prev_instrument = instrument_types[round(prev_features['instrumentation'])]\n",
        "        current_instrument = instrument_types[round(current_features['instrumentation'])]\n",
        "\n",
        "        if prev_instrument == current_instrument:\n",
        "            prompt += f\"Featuring {prev_instrument} throughout. \"\n",
        "        else:\n",
        "            prompt += f\"Transitioning from {prev_instrument} to {current_instrument}. \"\n",
        "\n",
        "        return prompt\n",
        "class TemporalCoherenceModel(nn.Module):\n",
        "    def __init__(self, input_dim=10, hidden_dim=64, output_dim=10):\n",
        "        super(TemporalCoherenceModel, self).__init__()\n",
        "\n",
        "        # LSTM for sequence modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.3\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        \"\"\"Process a sequence of musical features to ensure temporal coherence\"\"\"\n",
        "        # Expected shape: [batch_size, sequence_length, features]\n",
        "        lstm_out, _ = self.lstm(sequence)\n",
        "\n",
        "        # Apply output layer to each time step\n",
        "        coherent_sequence = self.output_layer(lstm_out)\n",
        "\n",
        "        return coherent_sequence\n",
        "\n",
        "class MusicGenerator:\n",
        "    def __init__(self):\n",
        "        # MIDI settings\n",
        "        self.instruments = {\n",
        "            'piano': 0,       # Acoustic Grand Piano\n",
        "            'strings': 48,    # String Ensemble 1\n",
        "            'guitar': 24,     # Acoustic Guitar (nylon)\n",
        "            'synth': 80,      # Lead 1 (square)\n",
        "            'orchestral': 48, # String Ensemble 1\n",
        "            'percussion': 118 # Synth Drum\n",
        "        }\n",
        "\n",
        "        # Scales (for different keys and modes)\n",
        "        self.major_scale = [0, 2, 4, 5, 7, 9, 11]  # Whole, Whole, Half, Whole, Whole, Whole, Half\n",
        "        self.minor_scale = [0, 2, 3, 5, 7, 8, 10]  # Whole, Half, Whole, Whole, Half, Whole, Whole\n",
        "\n",
        "        # Common chord progressions\n",
        "        self.progressions = {\n",
        "            'major': [\n",
        "                [1, 4, 5, 1],       # I-IV-V-I\n",
        "                [1, 6, 4, 5],       # I-vi-IV-V\n",
        "                [1, 5, 6, 4],       # I-V-vi-IV\n",
        "                [2, 5, 1, 6]        # ii-V-I-vi\n",
        "            ],\n",
        "            'minor': [\n",
        "                [1, 4, 5, 1],       # i-iv-v-i\n",
        "                [1, 6, 3, 7],       # i-VI-III-VII\n",
        "                [1, 7, 6, 5],       # i-VII-VI-v\n",
        "                [1, 4, 7, 3]        # i-iv-VII-III\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def create_midi_from_features(self, musical_features, duration_seconds=15):\n",
        "        \"\"\"Generate MIDI file from musical features\"\"\"\n",
        "        # Extract features\n",
        "        tempo = musical_features['tempo']\n",
        "        key = int(musical_features['key'])\n",
        "        mode = 'major' if musical_features['mode'] > 0.5 else 'minor'\n",
        "        instrument_type = ['piano', 'strings', 'guitar', 'synth', 'orchestral', 'percussion'][int(musical_features['instrumentation'])]\n",
        "        instrument = self.instruments[instrument_type]\n",
        "\n",
        "        # Derived parameters\n",
        "        rhythm_complexity = musical_features['rhythm_complexity']\n",
        "        harmonic_complexity = musical_features['harmonic_complexity']\n",
        "        melodic_range = musical_features['melodic_range']\n",
        "        intensity = musical_features['intensity']\n",
        "        texture = musical_features['texture']\n",
        "        articulation = musical_features['articulation']\n",
        "\n",
        "        # Create MIDI file\n",
        "        midi = MIDIFile(2)  # 2 tracks - one for melody, one for accompaniment\n",
        "        track_melody = 0\n",
        "        track_accomp = 1\n",
        "\n",
        "        # Set tempo\n",
        "        midi.addTempo(track_melody, 0, tempo)\n",
        "        midi.addTempo(track_accomp, 0, tempo)\n",
        "\n",
        "        # Choose scale based on mode\n",
        "        scale = self.major_scale if mode == 'major' else self.minor_scale\n",
        "\n",
        "        # Calculate beats based on tempo and duration\n",
        "        total_beats = int((tempo / 60) * duration_seconds)\n",
        "\n",
        "        # Set program (instrument)\n",
        "        midi.addProgramChange(track_melody, 0, 0, instrument)\n",
        "        midi.addProgramChange(track_accomp, 0, 0, instrument)\n",
        "\n",
        "        # Choose chord progression based on mode and complexity\n",
        "        prog_idx = min(int(harmonic_complexity * len(self.progressions[mode])), len(self.progressions[mode]) - 1)\n",
        "        progression = self.progressions[mode][prog_idx]\n",
        "\n",
        "        # Chord duration in beats\n",
        "        chord_duration = max(4, total_beats // len(progression))\n",
        "        repetitions = max(1, total_beats // (chord_duration * len(progression)))\n",
        "\n",
        "        # Base octave for melody\n",
        "        base_octave = 5 if instrument_type != 'piano' else 4\n",
        "\n",
        "        # Calculate note range based on melodic range\n",
        "        low_note = base_octave * 12  # C in the base octave\n",
        "        high_note = low_note + int(12 + 12 * musical_features['melodic_range'])  # Up to an octave or two higher\n",
        "\n",
        "        # Velocity (volume) based on intensity\n",
        "        melody_velocity = 64 + int(intensity * 63)  # Between 64-127\n",
        "        chord_velocity = int(melody_velocity * 0.8)  # Slightly quieter\n",
        "\n",
        "        # Note duration modifier based on articulation (staccato to legato)\n",
        "        duration_modifier = 0.5 + (articulation * 0.5)  # 0.5 (staccato) to 1.0 (legato)\n",
        "\n",
        "        # Generate melody and chords\n",
        "        current_beat = 0\n",
        "        for rep in range(repetitions):\n",
        "            for chord_idx, chord_root in enumerate(progression):\n",
        "                # Map chord root to actual note in the key\n",
        "                chord_root_idx = chord_root - 1  # Adjust for 0-indexing\n",
        "                root_note = (key + scale[chord_root_idx]) % 12\n",
        "\n",
        "                # Determine chord type based on position in scale\n",
        "                is_major_chord = chord_root_idx in [0, 3, 4] if mode == 'major' else chord_root_idx in [2, 5]\n",
        "\n",
        "                # Create chord notes (root, third, fifth)\n",
        "                third_offset = 4 if is_major_chord else 3\n",
        "                chord_notes = [\n",
        "                    root_note + 60,  # Root note (C4 = 60, middle C)\n",
        "                    root_note + 60 + third_offset,  # Third\n",
        "                    root_note + 60 + 7   # Fifth\n",
        "                ]\n",
        "\n",
        "                # Add chord to accompaniment track with texture variation\n",
        "                if texture < 0.3:\n",
        "                    # Sparse - just root and fifth\n",
        "                    midi.addNote(track_accomp, 0, chord_notes[0], current_beat, chord_duration * 0.9, chord_velocity)\n",
        "                    midi.addNote(track_accomp, 0, chord_notes[2], current_beat, chord_duration * 0.9, chord_velocity)\n",
        "                elif texture < 0.7:\n",
        "                    # Medium - broken chord\n",
        "                    for i, note in enumerate(chord_notes):\n",
        "                        midi.addNote(track_accomp, 0, note, current_beat + i*0.5, chord_duration * 0.9 - i*0.5, chord_velocity)\n",
        "                else:\n",
        "                    # Dense - full chord plus extra notes\n",
        "                    for note in chord_notes:\n",
        "                        midi.addNote(track_accomp, 0, note, current_beat, chord_duration * 0.9, chord_velocity)\n",
        "                    # Add extra notes for texture\n",
        "                    midi.addNote(track_accomp, 0, chord_notes[0] + 12, current_beat + 1, chord_duration * 0.4, chord_velocity - 10)\n",
        "\n",
        "                # Generate melody for this chord\n",
        "                notes_per_beat = 1 + int(rhythm_complexity * 3)  # 1 to 4 notes per beat\n",
        "\n",
        "                for beat_offset in range(chord_duration):\n",
        "                    # Skip some beats randomly for variation\n",
        "                    if random.random() < 0.2:\n",
        "                        continue\n",
        "\n",
        "                    for note_idx in range(notes_per_beat):\n",
        "                        # Calculate precise timing\n",
        "                        note_start = current_beat + beat_offset + (note_idx / notes_per_beat)\n",
        "\n",
        "                        # Choose note from scale\n",
        "                        scale_idx = random.randint(0, len(scale) - 1)\n",
        "                        note = key + scale[scale_idx]\n",
        "\n",
        "                        # Map to the right octave range\n",
        "                        octave = random.randint(base_octave, base_octave + 1)\n",
        "                        note = (note % 12) + (octave * 12)\n",
        "\n",
        "                        # Ensure note is in our range\n",
        "                        note = max(low_note, min(note, high_note))\n",
        "\n",
        "                        # Calculate duration based on articulation and rhythm\n",
        "                        note_duration = (1.0 / notes_per_beat) * duration_modifier\n",
        "\n",
        "                        # Add note to melody track\n",
        "                        if random.random() < 0.8:  # 80% chance to add a note (for rests)\n",
        "                            midi.addNote(track_melody, 0, note, note_start, note_duration, melody_velocity)\n",
        "\n",
        "                current_beat += chord_duration\n",
        "\n",
        "        # Write MIDI file to bytes buffer\n",
        "        buffer = io.BytesIO()\n",
        "        midi.writeFile(buffer)\n",
        "        buffer.seek(0)\n",
        "\n",
        "        return buffer\n",
        "\n",
        "# Helper functions for audio playback in Colab\n",
        "def midi_to_audio(midi_buffer, sr=22050):\n",
        "    \"\"\"Convert MIDI to audio using pretty_midi\"\"\"\n",
        "    # Create a temporary file to save the MIDI\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".mid\", delete=False) as temp_file:\n",
        "        temp_file.write(midi_buffer.getvalue())\n",
        "        temp_file_path = temp_file.name\n",
        "\n",
        "    # Load the MIDI file\n",
        "    midi_data = pretty_midi.PrettyMIDI(temp_file_path)\n",
        "\n",
        "    # Synthesize audio\n",
        "    audio = midi_data.synthesize(fs=sr)\n",
        "\n",
        "    # Remove the temporary file\n",
        "    os.remove(temp_file_path)\n",
        "\n",
        "    return audio, sr\n",
        "\n",
        "def play_midi_in_colab(midi_buffer):\n",
        "    \"\"\"Play MIDI in Colab notebook\"\"\"\n",
        "    # Convert MIDI to audio\n",
        "    audio, sr = midi_to_audio(midi_buffer)\n",
        "\n",
        "    # Display audio player\n",
        "    return ipd.Audio(audio, rate=sr)\n",
        "\n",
        "def play_all_segments(midi_files):\n",
        "    \"\"\"Play all music segments sequentially\"\"\"\n",
        "    for i, midi_buffer in enumerate(midi_files):\n",
        "        print(f\"Playing music for segment {i+1}...\")\n",
        "        display(play_midi_in_colab(midi_buffer))\n",
        "\n",
        "# Example usage and demonstration\n",
        "def demo():\n",
        "    # Initialize the system\n",
        "    music_generator = LiteraryMusicGenerator()\n",
        "\n",
        "    # Example literary text (a short excerpt)\n",
        "    example_text = \"\"\"\n",
        "    The morning dawned bright and clear, with a crispness in the air that promised a beautiful day ahead. Sarah smiled as she stepped outside, breathing in the fresh scent of wildflowers that carpeted the meadow beyond her cottage. It had been too long since she felt this sense of peace.\n",
        "\n",
        "    Suddenly, dark clouds appeared on the horizon, rolling in with unexpected speed. The wind picked up, carrying with it the scent of rain and an undercurrent of foreboding. Sarah's smile faded as she watched the storm approach, her heart racing with a nameless anxiety.\n",
        "\n",
        "    Lightning flashed, followed by a deafening crack of thunder that seemed to shake the very earth. Sarah ran back inside, slamming the door just as the rain began to pound against the windows like an angry fist demanding entry. She pressed her back against the door, breathing heavily, fighting the irrational fear that had gripped her.\n",
        "\n",
        "    As quickly as it had arrived, the storm began to subside. The violent raindrops slowed to a gentle patter, and the thunder rumbled away into the distance. A ray of sunlight broke through the clouds, casting a golden glow over the rain-soaked landscape. Sarah felt her tension melting away, replaced by a profound sense of wonder at the resilience of nature and the beauty that can follow chaos.\n",
        "    \"\"\"\n",
        "\n",
        "    # Process the text\n",
        "    result = music_generator.process_literary_text(example_text)\n",
        "\n",
        "    # Display the results\n",
        "    print(\"Text Processing Complete!\")\n",
        "    print(f\"Text divided into {len(result['segments'])} segments\")\n",
        "\n",
        "    print(\"\\nEmotional Progression:\")\n",
        "    for emotion, values in result['emotion_progression'].items():\n",
        "        print(f\"  {emotion}: {[round(v, 2) for v in values]}\")\n",
        "\n",
        "    print(\"\\nOverall Emotional Flow Narrative:\")\n",
        "    print(result['emotional_narrative'])\n",
        "\n",
        "    print(\"\\nSegment-by-Segment Music Prompts:\")\n",
        "    for i, prompt_data in enumerate(result['musiclm_prompts']):\n",
        "        print(f\"\\nSegment {i+1}:\")\n",
        "        print(f\"  Context: {prompt_data['context']}\")\n",
        "        print(f\"  Dominant emotions: {prompt_data['dominant_emotions']}\")\n",
        "        print(f\"  Enhanced MusicLM prompt: {prompt_data['prompt']}\")\n",
        "        print(f\"  Playing music for this segment...\")\n",
        "        display(play_midi_in_colab(result['midi_files'][i]))\n",
        "\n",
        "    if 'transition_prompts' in result and result['transition_prompts']:\n",
        "        print(\"\\nTransition Prompts Between Segments:\")\n",
        "        for i, transition in enumerate(result['transition_prompts']):\n",
        "            print(f\"\\nTransition {i+1}: From Segment {transition['from_segment']+1} to {transition['to_segment']+1}\")\n",
        "            print(f\"  Prompt: {transition['prompt']}\")\n",
        "\n",
        "    # Visualize emotional progression\n",
        "    plot = music_generator.visualize_emotional_progression(result['emotion_progression'])\n",
        "    plot.show()\n",
        "\n",
        "    print(\"\\nAll music files generated successfully!\")\n",
        "\n",
        "# Mock training data generator\n",
        "def generate_mock_training_data(num_samples=100):\n",
        "    \"\"\"Generate synthetic data for training the models\"\"\"\n",
        "    emotions = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'neutral']\n",
        "    musical_features = ['tempo', 'key', 'mode', 'intensity', 'instrumentation',\n",
        "                         'rhythm_complexity', 'harmonic_complexity', 'melodic_range', 'texture', 'articulation']\n",
        "\n",
        "    # Generate random emotion vectors\n",
        "    emotion_vectors = np.random.rand(num_samples, len(emotions))\n",
        "    # Normalize to sum to 1\n",
        "    emotion_vectors = emotion_vectors / emotion_vectors.sum(axis=1, keepdims=True)\n",
        "\n",
        "    # Generate matching musical features\n",
        "    musical_feature_vectors = np.random.rand(num_samples, len(musical_features))\n",
        "\n",
        "    # Create training dataset\n",
        "    X_train = torch.tensor(emotion_vectors, dtype=torch.float32)\n",
        "    y_train = torch.tensor(musical_feature_vectors, dtype=torch.float32)\n",
        "\n",
        "    return X_train, y_train\n",
        "\n",
        "# Training function for the emotion-to-music mapper\n",
        "def train_emotion_to_music_mapper(model, num_epochs=100, batch_size=16):\n",
        "    \"\"\"Train the emotion-to-music mapping model\"\"\"\n",
        "    # Generate mock training data\n",
        "    X_train, y_train = generate_mock_training_data(500)\n",
        "\n",
        "    # Create DataLoader\n",
        "    dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets in dataloader:\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print statistics\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}')\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return model\n",
        "\n",
        "# Training function for the temporal coherence model\n",
        "def train_temporal_coherence_model(model, seq_length=5, num_epochs=100, batch_size=8):\n",
        "    \"\"\"Train the temporal coherence model\"\"\"\n",
        "    # For simplicity, we'll generate sequences of musical features\n",
        "    num_samples = 100\n",
        "    feature_dim = 10\n",
        "\n",
        "    # Generate sequences with some temporal patterns\n",
        "    sequences = []\n",
        "    for _ in range(num_samples):\n",
        "        # Start with random features\n",
        "        start_features = np.random.rand(feature_dim)\n",
        "        # Generate a sequence with gradual changes\n",
        "        sequence = [start_features]\n",
        "        for i in range(1, seq_length):\n",
        "            # Each step is a small random modification of the previous step\n",
        "            next_features = sequence[i-1] + np.random.normal(0, 0.1, feature_dim)\n",
        "            # Keep within bounds\n",
        "            next_features = np.clip(next_features, 0, 1)\n",
        "            sequence.append(next_features)\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    X_train = torch.tensor(np.array(sequences), dtype=torch.float32)\n",
        "    # Target is the same sequence (we want to learn the patterns)\n",
        "    y_train = X_train.clone()\n",
        "\n",
        "    # Create DataLoader\n",
        "    dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets in dataloader:\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print statistics\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}')\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return model\n",
        "\n",
        "# Main function to run everything\n",
        "def main():\n",
        "    # Initialize the system\n",
        "    music_generator = LiteraryMusicGenerator()\n",
        "\n",
        "    # Train models\n",
        "    print(\"Training emotion-to-music mapper...\")\n",
        "    train_emotion_to_music_mapper(music_generator.emotion_to_music_mapper, num_epochs=50)\n",
        "\n",
        "    print(\"\\nTraining temporal coherence model...\")\n",
        "    train_temporal_coherence_model(music_generator.temporal_coherence_model, num_epochs=50)\n",
        "\n",
        "    # Run demo\n",
        "    print(\"\\nRunning demonstration...\")\n",
        "    demo()\n",
        "\n",
        "    # Save models\n",
        "    torch.save(music_generator.emotion_to_music_mapper.state_dict(), 'emotion_to_music_mapper.pth')\n",
        "    torch.save(music_generator.temporal_coherence_model.state_dict(), 'temporal_coherence_model.pth')\n",
        "    print(\"\\nModels saved successfully!\")\n",
        "\n",
        "# Function to save MIDI files to disk\n",
        "def save_midi_files(midi_files, base_filename=\"segment\"):\n",
        "    \"\"\"Save MIDI files to disk and download them\"\"\"\n",
        "    saved_files = []\n",
        "\n",
        "    for i, midi_buffer in enumerate(midi_files):\n",
        "        filename = f\"{base_filename}_{i+1}.mid\"\n",
        "\n",
        "        # Write to file\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(midi_buffer.getvalue())\n",
        "\n",
        "        saved_files.append(filename)\n",
        "\n",
        "        # Initiate download in Colab\n",
        "        try:\n",
        "            files.download(filename)\n",
        "        except:\n",
        "            print(f\"Note: Download of {filename} may not be supported in this environment.\")\n",
        "\n",
        "    return saved_files\n",
        "\n",
        "# Function to combine all segments into a single music piece\n",
        "def combine_segments(midi_files, output_filename=\"combined_music.mid\"):\n",
        "    \"\"\"Combine all MIDI segments into a single continuous piece\"\"\"\n",
        "    # Create a new MIDI file with more tracks to prevent conflicts\n",
        "    combined_midi = MIDIFile(8)  # Increase number of tracks to avoid note overlaps\n",
        "\n",
        "    # Add tempo to all tracks\n",
        "    for track in range(8):\n",
        "        combined_midi.addTempo(track, 0, 120)\n",
        "\n",
        "    current_time = 0\n",
        "    track_offset = 0  # To distribute notes across different tracks\n",
        "\n",
        "    for midi_buffer in midi_files:\n",
        "        # Create a temporary file to save the MIDI\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".mid\", delete=False) as temp_file:\n",
        "            temp_file.write(midi_buffer.getvalue())\n",
        "            temp_file_path = temp_file.name\n",
        "\n",
        "        try:\n",
        "            # Load the MIDI file\n",
        "            midi_data = pretty_midi.PrettyMIDI(temp_file_path)\n",
        "\n",
        "            # Get segment duration\n",
        "            end_time = 0\n",
        "            if midi_data.instruments:\n",
        "                for instr in midi_data.instruments:\n",
        "                    if instr.notes:\n",
        "                        instr_end = max([note.end for note in instr.notes])\n",
        "                        end_time = max(end_time, instr_end)\n",
        "\n",
        "            if end_time == 0:\n",
        "                end_time = 15  # Default duration if no notes found\n",
        "\n",
        "            # Add each note from the segment to the combined file with offset\n",
        "            for instr_idx, instr in enumerate(midi_data.instruments):\n",
        "                # Use a different track for each instrument to avoid conflicts\n",
        "                track = (track_offset + instr_idx) % 8\n",
        "                program = instr.program\n",
        "\n",
        "                # Set instrument\n",
        "                combined_midi.addProgramChange(track, 0, current_time, program)\n",
        "\n",
        "                # Add notes with time offset\n",
        "                for note in instr.notes:\n",
        "                    start = current_time + note.start\n",
        "                    duration = note.end - note.start\n",
        "\n",
        "                    # Ensure notes have valid duration\n",
        "                    if duration <= 0:\n",
        "                        duration = 0.1  # Set a minimum duration\n",
        "\n",
        "                    pitch = note.pitch\n",
        "                    velocity = note.velocity\n",
        "\n",
        "                    combined_midi.addNote(track, 0, pitch, start, duration, velocity)\n",
        "\n",
        "            # Increment the track offset for the next segment to use different tracks\n",
        "            track_offset = (track_offset + len(midi_data.instruments)) % 8\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error processing segment: {e}\")\n",
        "        finally:\n",
        "            # Remove the temporary file\n",
        "            try:\n",
        "                os.remove(temp_file_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Update current time for next segment\n",
        "        current_time += end_time + 2  # Add 2 beat gap between segments\n",
        "\n",
        "    # Save the combined MIDI\n",
        "    try:\n",
        "        with open(output_filename, 'wb') as output_file:\n",
        "            combined_midi.writeFile(output_file)\n",
        "\n",
        "        print(f\"Successfully combined music saved to {output_filename}\")\n",
        "\n",
        "        # Initiate download in Colab\n",
        "        try:\n",
        "            files.download(output_filename)\n",
        "        except:\n",
        "            print(f\"Note: Download of {output_filename} may not be supported in this environment.\")\n",
        "\n",
        "        return output_filename\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving combined file: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to take user input and process it\n",
        "def process_user_text():\n",
        "    print(\"Please enter or paste your literary text (type 'END' on a new line when finished):\")\n",
        "    lines = []\n",
        "    while True:\n",
        "        line = input()\n",
        "        if line.strip() == 'END':\n",
        "            break\n",
        "        lines.append(line)\n",
        "\n",
        "    user_text = '\\n'.join(lines)\n",
        "\n",
        "    if not user_text.strip():\n",
        "        print(\"No text entered. Using example text instead.\")\n",
        "        user_text = \"\"\"\n",
        "        The morning dawned bright and clear, with a crispness in the air that promised a beautiful day ahead.\n",
        "        Sarah smiled as she stepped outside, breathing in the fresh scent of wildflowers that carpeted the meadow beyond her cottage.\n",
        "        As she walked along the path, her mind wandered to thoughts of the past, memories both sweet and bitter.\n",
        "        \"\"\"\n",
        "\n",
        "    # Process the text\n",
        "    music_generator = LiteraryMusicGenerator()\n",
        "    result = music_generator.process_literary_text(user_text)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\nProcessed {len(result['segments'])} text segments\")\n",
        "\n",
        "    # Visualize emotional progression\n",
        "    plot = music_generator.visualize_emotional_progression(result['emotion_progression'])\n",
        "    plot.show()\n",
        "\n",
        "    # Play music for each segment\n",
        "    print(\"\\nPlaying music for each segment:\")\n",
        "    for i, prompt_data in enumerate(result['musiclm_prompts']):\n",
        "        print(f\"\\nSegment {i+1}:\")\n",
        "        emotions_display = []\n",
        "        for e in prompt_data['dominant_emotions']:\n",
        "            emotions_display.append(f\"{e['emotion']} ({e['intensity']:.2f})\")\n",
        "        print(f\"  Dominant emotions: {emotions_display}\")\n",
        "        print(f\"  MusicLM prompt: {prompt_data['prompt']}\")\n",
        "\n",
        "        # Play the music\n",
        "        display(play_midi_in_colab(result['midi_files'][i]))\n",
        "\n",
        "    # Option to save MIDI files\n",
        "    save_choice = input(\"\\nWould you like to save the MIDI files? (y/n): \")\n",
        "    if save_choice.lower() == 'y':\n",
        "        saved_files = save_midi_files(result['midi_files'])\n",
        "        print(f\"Saved {len(saved_files)} MIDI files.\")\n",
        "\n",
        "        # Option to combine segments\n",
        "        combine_choice = input(\"Would you like to combine all segments into one continuous piece? (y/n): \")\n",
        "        if combine_choice.lower() == 'y':\n",
        "            combined_file = combine_segments(result['midi_files'])\n",
        "            print(f\"Combined music saved to {combined_file}\")\n",
        "\n",
        "            # Play the combined piece\n",
        "            print(\"\\nPlaying the combined music piece:\")\n",
        "            with open(combined_file, 'rb') as f:\n",
        "                combined_buffer = io.BytesIO(f.read())\n",
        "            display(play_midi_in_colab(combined_buffer))\n",
        "\n",
        "    # Save results to JSON\n",
        "    output_data = {\n",
        "        \"segments\": result['segments'],\n",
        "        \"emotion_progression\": {k: [float(v) for v in vals] for k, vals in result['emotion_progression'].items()},\n",
        "        \"musiclm_prompts\": result['musiclm_prompts']\n",
        "    }\n",
        "\n",
        "    with open('literary_music_results.json', 'w') as f:\n",
        "        json.dump(output_data, f, indent=2)\n",
        "\n",
        "    print(\"\\nResults saved to 'literary_music_results.json'\")\n",
        "    try:\n",
        "        files.download('literary_music_results.json')\n",
        "    except:\n",
        "        print(\"File download not supported in this environment.\")\n",
        "\n",
        "# Interface for Google Colab\n",
        "def colab_interface():\n",
        "    print(\"Literary Text to Music Converter\")\n",
        "    print(\"================================\")\n",
        "    print(\"1. Run demonstration with example text\")\n",
        "    print(\"2. Process your own literary text\")\n",
        "    print(\"3. Train models\")\n",
        "    print(\"4. Exit\")\n",
        "\n",
        "    choice = input(\"\\nEnter your choice (1-4): \")\n",
        "\n",
        "    if choice == '1':\n",
        "        demo()\n",
        "    elif choice == '2':\n",
        "        process_user_text()\n",
        "    elif choice == '3':\n",
        "        main()\n",
        "    elif choice == '4':\n",
        "        print(\"Exiting...\")\n",
        "    else:\n",
        "        print(\"Invalid choice. Please try again.\")\n",
        "\n",
        "# Run the Colab interface\n",
        "if __name__ == \"__main__\":\n",
        "    colab_interface()\n",
        "\n",
        "class MuLanEmbedding(nn.Module):\n",
        "    def __init__(self, text_embedding_dim=768, audio_embedding_dim=512, joint_embedding_dim=256):\n",
        "        super(MuLanEmbedding, self).__init__()\n",
        "\n",
        "        # Text encoder (BERT-based)\n",
        "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.text_projection = nn.Linear(768, joint_embedding_dim)\n",
        "\n",
        "        # Audio encoder\n",
        "        self.audio_encoder = nn.Sequential(\n",
        "            nn.Conv1d(80, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "        self.audio_projection = nn.Linear(512, joint_embedding_dim)\n",
        "\n",
        "    def encode_text(self, input_ids, attention_mask):\n",
        "        outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
        "        text_embedding = self.text_projection(cls_embedding)\n",
        "        return F.normalize(text_embedding, p=2, dim=1)\n",
        "\n",
        "    def encode_audio(self, mel_spectrogram):\n",
        "        # mel_spectrogram shape: [batch_size, freq_bins, time_frames]\n",
        "        audio_features = self.audio_encoder(mel_spectrogram).squeeze(-1)\n",
        "        audio_embedding = self.audio_projection(audio_features)\n",
        "        return F.normalize(audio_embedding, p=2, dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, mel_spectrogram=None):\n",
        "        text_embedding = self.encode_text(input_ids, attention_mask)\n",
        "\n",
        "        if mel_spectrogram is not None:\n",
        "            audio_embedding = self.encode_audio(mel_spectrogram)\n",
        "            return text_embedding, audio_embedding\n",
        "\n",
        "        return text_embedding\n",
        "\n",
        "def train_custom_musiclm(model, train_dataloader, num_epochs=100):\n",
        "    # Define loss functions\n",
        "    reconstruction_loss = nn.MSELoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            audio_targets = batch['audio_waveform']\n",
        "\n",
        "            # Forward pass\n",
        "            audio_output = model(input_ids, attention_mask)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = reconstruction_loss(audio_output, audio_targets)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "class CustomMusicLM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomMusicLM, self).__init__()\n",
        "\n",
        "        # Text-to-music embedding model\n",
        "        self.mulan_model = MuLanEmbedding()\n",
        "\n",
        "        # Latent vector generation\n",
        "        self.latent_generator = nn.Sequential(\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Audio decoder\n",
        "        self.soundstream_decoder = SoundStreamDecoder()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get text embedding\n",
        "        text_embedding = self.mulan_model.encode_text(input_ids, attention_mask)\n",
        "\n",
        "        # Generate latent vector\n",
        "        latent_vector = self.latent_generator(text_embedding)\n",
        "\n",
        "        # Decode to audio\n",
        "        waveform = self.soundstream_decoder(latent_vector)\n",
        "\n",
        "        return waveform\n",
        "\n",
        "class SoundStreamDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=256, output_channels=1):\n",
        "        super(SoundStreamDecoder, self).__init__()\n",
        "\n",
        "        self.initial_layer = nn.Linear(latent_dim, 16 * 256)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(32, output_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, latent_vector):\n",
        "        # latent_vector shape: [batch_size, latent_dim]\n",
        "        x = self.initial_layer(latent_vector)\n",
        "        x = x.view(-1, 256, 16)  # Reshape to [batch_size, channels, length]\n",
        "        waveform = self.decoder(x)\n",
        "        return waveform\n",
        "\n",
        "class LiteraryMusicGenerator:\n",
        "    def __init__(self):\n",
        "        self.text_processor = TextProcessor()\n",
        "        self.emotion_extractor = EmotionExtractor()\n",
        "        self.emotion_to_music_mapper = EmotionToMusicMapper()\n",
        "        self.temporal_coherence_model = TemporalCoherenceModel()\n",
        "        self.music_generator = MusicGenerator()\n",
        "\n",
        "        # Initialize with some pre-trained weights (in a real scenario)\n",
        "        # self.load_pretrained_models()\n",
        "\n",
        "    def load_pretrained_models(self):\n",
        "        \"\"\"Load pre-trained models if available\"\"\"\n",
        "        try:\n",
        "            self.emotion_to_music_mapper.load_state_dict(torch.load('emotion_to_music_mapper.pth'))\n",
        "            self.temporal_coherence_model.load_state_dict(torch.load('temporal_coherence_model.pth'))\n",
        "            print(\"Loaded pre-trained models successfully.\")\n",
        "        except:\n",
        "            print(\"Pre-trained models not found. Using initialized models.\")\n",
        "\n",
        "    def process_literary_text(self, text):\n",
        "        \"\"\"Process literary text end-to-end with enhanced emotional flow\"\"\"\n",
        "        # Step 1: Process text\n",
        "        original_segments, cleaned_segments = self.text_processor.process_text(text)\n",
        "\n",
        "        # Step 2: Extract emotions\n",
        "        emotion_maps = self.emotion_extractor.extract_emotions(original_segments)\n",
        "        emotional_progression = self.emotion_extractor.create_emotional_progression(emotion_maps)\n",
        "        dominant_emotions = self.emotion_extractor.get_dominant_emotions(emotion_maps)\n",
        "\n",
        "        # Step 3: Generate musical features for each segment\n",
        "        musical_features = []\n",
        "\n",
        "        for emotion_map in emotion_maps:\n",
        "            # Convert emotion map to tensor\n",
        "            emotion_vector = torch.tensor([\n",
        "                emotion_map['joy'], emotion_map['sadness'], emotion_map['anger'],\n",
        "                emotion_map['fear'], emotion_map['surprise'], emotion_map['disgust'],\n",
        "                emotion_map['neutral']\n",
        "            ], dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "            # Map emotions to musical features\n",
        "            with torch.no_grad():\n",
        "                features = self.emotion_to_music_mapper(emotion_vector)\n",
        "\n",
        "            musical_features.append(features.squeeze(0))\n",
        "\n",
        "        # Step 4: Ensure temporal coherence\n",
        "        if len(musical_features) > 1:\n",
        "            # Stack features into a sequence\n",
        "            feature_sequence = torch.stack(musical_features).unsqueeze(0)  # [1, seq_len, features]\n",
        "\n",
        "            # Apply temporal coherence model\n",
        "            with torch.no_grad():\n",
        "                coherent_features = self.temporal_coherence_model(feature_sequence)\n",
        "\n",
        "            # Convert back to list\n",
        "            musical_features = [feat for feat in coherent_features.squeeze(0)]\n",
        "\n",
        "        # Step 5: Generate MusicLM prompts and MIDI files with emotional flow awareness\n",
        "        musiclm_prompts = []\n",
        "        midi_files = []\n",
        "        transition_prompts = []\n",
        "\n",
        "        for i, features in enumerate(musical_features):\n",
        "            # Map normalized features to actual values\n",
        "            actual_values = self.emotion_to_music_mapper.map_to_actual_values(features)\n",
        "\n",
        "            # Get current emotion map\n",
        "            current_emotion_map = emotion_maps[i]\n",
        "\n",
        "            # Generate emotion-aware prompt\n",
        "            prompt = self.emotion_to_music_mapper.generate_musiclm_prompt(\n",
        "                actual_values,\n",
        "                emotion_scores=current_emotion_map\n",
        "            )\n",
        "\n",
        "            # Generate MIDI file\n",
        "            midi_buffer = self.music_generator.create_midi_from_features(actual_values)\n",
        "            midi_files.append(midi_buffer)\n",
        "\n",
        "            # Add context from the text segment\n",
        "            segment_context = f\"Music representing: '{original_segments[i][:100]}...'\"\n",
        "\n",
        "            # Generate transition prompt if not the first segment\n",
        "            if i > 0:\n",
        "                prev_features = self.emotion_to_music_mapper.map_to_actual_values(musical_features[i-1])\n",
        "                prev_emotion_map = emotion_maps[i-1]\n",
        "\n",
        "                transition_prompt = self.emotion_to_music_mapper.generate_transition_prompt(\n",
        "                    prev_features,\n",
        "                    actual_values,\n",
        "                    prev_emotion_map,\n",
        "                    current_emotion_map\n",
        "                )\n",
        "                transition_prompts.append({\n",
        "                    \"from_segment\": i-1,\n",
        "                    \"to_segment\": i,\n",
        "                    \"prompt\": transition_prompt\n",
        "                })\n",
        "\n",
        "            musiclm_prompts.append({\n",
        "                \"segment_idx\": i,\n",
        "                \"prompt\": prompt,\n",
        "                \"context\": segment_context,\n",
        "                \"dominant_emotions\": [{\"emotion\": e[0], \"intensity\": e[1]} for e in dominant_emotions[i]],\n",
        "                \"musical_features\": actual_values,\n",
        "                \"emotion_map\": {k: float(v) for k, v in current_emotion_map.items()}\n",
        "            })\n",
        "\n",
        "        # Generate a full emotional flow narrative for the entire piece\n",
        "        emotional_narrative = self.generate_emotional_flow_narrative(emotion_maps, original_segments)\n",
        "\n",
        "        return {\n",
        "            \"segments\": original_segments,\n",
        "            \"emotion_progression\": emotional_progression,\n",
        "            \"musiclm_prompts\": musiclm_prompts,\n",
        "            \"transition_prompts\": transition_prompts,\n",
        "            \"emotional_narrative\": emotional_narrative,\n",
        "            \"midi_files\": midi_files\n",
        "        }\n",
        "\n",
        "    def generate_emotional_flow_narrative(self, emotion_maps, text_segments):\n",
        "        \"\"\"Generate a cohesive narrative describing the emotional flow of the entire text\"\"\"\n",
        "        # Identify major emotional shifts\n",
        "        narrative = \"The emotional journey of this text flows through: \"\n",
        "\n",
        "        # Track dominant emotions through segments\n",
        "        dominant_per_segment = []\n",
        "        for emotion_map in emotion_maps:\n",
        "            top_emotion = max(emotion_map.items(), key=lambda x: x[1])\n",
        "            if top_emotion[1] > 0.2:  # Only if the emotion is significant\n",
        "                dominant_per_segment.append(top_emotion[0])\n",
        "            else:\n",
        "                dominant_per_segment.append(\"neutral\")\n",
        "\n",
        "        # Find emotional arcs (consecutive segments with the same dominant emotion)\n",
        "        emotional_arcs = []\n",
        "        current_arc = {\"emotion\": dominant_per_segment[0], \"start\": 0, \"end\": 0}\n",
        "\n",
        "        for i in range(1, len(dominant_per_segment)):\n",
        "            if dominant_per_segment[i] == current_arc[\"emotion\"]:\n",
        "                # Continue the current arc\n",
        "                current_arc[\"end\"] = i\n",
        "            else:\n",
        "                # End the current arc and start a new one\n",
        "                emotional_arcs.append(current_arc)\n",
        "                current_arc = {\"emotion\": dominant_per_segment[i], \"start\": i, \"end\": i}\n",
        "\n",
        "        # Add the last arc\n",
        "        emotional_arcs.append(current_arc)\n",
        "\n",
        "        # Generate narrative from arcs\n",
        "        if len(emotional_arcs) <= 3:\n",
        "            # Simple emotional progression\n",
        "            emotions_journey = \" → \".join([arc[\"emotion\"] for arc in emotional_arcs])\n",
        "            narrative += emotions_journey + \".\"\n",
        "        else:\n",
        "            # More complex emotional journey\n",
        "            narrative += \"an initial \" + emotional_arcs[0][\"emotion\"]\n",
        "\n",
        "            # Middle arcs with text context\n",
        "            for i in range(1, len(emotional_arcs) - 1):\n",
        "                arc = emotional_arcs[i]\n",
        "                # Get a snippet from the text segment for context\n",
        "                segment_snippet = text_segments[arc[\"start\"]][:50] + \"...\"\n",
        "                narrative += f\", then {arc['emotion']} during \\\"{segment_snippet}\\\"\"\n",
        "\n",
        "            # End with the final emotion\n",
        "            narrative += f\", and concludes with {emotional_arcs[-1]['emotion']}.\"\n",
        "\n",
        "        # Add musical suggestion based on the overall emotional flow\n",
        "        narrative += \"\\n\\nA MusicLM composition capturing this emotional flow should: \"\n",
        "\n",
        "        # Build specific suggestions\n",
        "        suggestions = []\n",
        "        for i, arc in enumerate(emotional_arcs):\n",
        "            emotion = arc[\"emotion\"]\n",
        "\n",
        "            # Get musical characteristics for this emotion\n",
        "            if hasattr(self.emotion_to_music_mapper, 'emotion_correlations'):\n",
        "                correlations = self.emotion_to_music_mapper.emotion_correlations.get(emotion, {})\n",
        "\n",
        "                if correlations:\n",
        "                    # Pick a random descriptor\n",
        "                    if 'description' in correlations and correlations['description']:\n",
        "                        descriptor = random.choice(correlations['description'])\n",
        "\n",
        "                        # Different suggestion format based on position in narrative\n",
        "                        if i == 0:\n",
        "                            suggestions.append(f\"begin with a {descriptor} section\")\n",
        "                        elif i == len(emotional_arcs) - 1:\n",
        "                            suggestions.append(f\"conclude with a {descriptor} finale\")\n",
        "                        else:\n",
        "                            suggestions.append(f\"transition to a {descriptor} middle section\")\n",
        "\n",
        "        narrative += \", \".join(suggestions) + \".\"\n",
        "\n",
        "        return narrative\n",
        "\n",
        "    def visualize_emotional_progression(self, emotional_progression):\n",
        "        \"\"\"Visualize the emotional progression throughout the text\"\"\"\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        for emotion, values in emotional_progression.items():\n",
        "            plt.plot(values, label=emotion)\n",
        "\n",
        "        plt.xlabel('Segment Index')\n",
        "        plt.ylabel('Emotion Intensity')\n",
        "        plt.title('Emotional Progression Throughout the Text')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        return plt\n",
        "\n",
        "    def train_models(self, training_data):\n",
        "        \"\"\"Train the emotion-to-music mapper and temporal coherence models\"\"\"\n",
        "        # In a real implementation, this would use actual training data\n",
        "        # This is a simplified placeholder\n",
        "        pass\n",
        "\n",
        "def use_custom_musiclm(literary_music_generator, text_prompts):\n",
        "    # Initialize custom MusicLM model\n",
        "    custom_musiclm = CustomMusicLM()\n",
        "\n",
        "    # Load trained weights\n",
        "    custom_musiclm.load_state_dict(torch.load('custom_musiclm.pth'))\n",
        "    custom_musiclm.eval()\n",
        "\n",
        "    # Tokenize the text prompts\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    audio_outputs = []\n",
        "\n",
        "    for prompt in text_prompts:\n",
        "        # Tokenize\n",
        "        encoded_input = tokenizer(\n",
        "            prompt,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Generate audio\n",
        "        with torch.no_grad():\n",
        "            audio_output = custom_musiclm(\n",
        "                encoded_input['input_ids'],\n",
        "                encoded_input['attention_mask']\n",
        "            )\n",
        "\n",
        "        audio_outputs.append(audio_output.squeeze().cpu().numpy())\n",
        "\n",
        "    return audio_outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bd8de1cfed7148fe9a890204c9ee9735",
            "9429334ec874445d83bed6cd791ec982",
            "826c57dbb4e248d39de1aa7b9bc66a12",
            "72f527cf53f64eb6ae5b0a94eb92c0a9",
            "80aeb8a927de4cbca3d9ad9bea2d4230",
            "8da5906185dd4df885144a8daeaf6f85",
            "227d573321a2407ca2e0a82ff13c7e2f",
            "11dc79f439344eedb6b352a4d9267d6e",
            "c615f6c815d147b6bddbe90e5992982e",
            "dd9274741f824c95bea0764343acf6dd",
            "027faca9a90345e9ad9f319f823b3229",
            "5a749b020ef6468b9c347473541b6f3c",
            "86afe405052e48eeb71392430217748b",
            "f280b05fc336438e9efaa440baf7286d",
            "7202f1f2e57f46d48819d89509774c3a",
            "eb01855b777646e4b4c8c5f080be4556",
            "08ec4b84402142318782385bf5aebcdd",
            "4ce6def95aae473e87c1cfd943c18309",
            "ef16a37805094a28976e7b8a5c054369",
            "a22dde4938d94fad83b8f1a91b21715f",
            "aaa8ca9b309f41f4b0163c2efd3d4a26",
            "f5f41c6c4c4e4b1ea32f5d9d8b921e91",
            "ff9b1506d76a4560a73c16d548ac5592",
            "ff47bb799f6445db850d560fcc15c552",
            "19e090e766a84fdf8425098b6fbce3fa",
            "14b55d255181444c96b7a62fe86ec79a",
            "db4779223b494858942ed196bf54ab1b",
            "26a93f04fea347b8857df86b8d887dc3",
            "7c4140ee719140eeb92ee88a628ceeef",
            "800dc3bf4d38459d85efab750aad80e6",
            "0fc18312608145faac293e3521b457a5",
            "3c60ea209c774977a9244edb0c6e3972",
            "437774ec25254810b104237b08c1e25e",
            "492c48f195d84feab71b6f8602913979",
            "03474b8b9d484b079e3ebc4f56f4cae0",
            "38fd46f32ad64ca1a20e608663cc85a0",
            "afbfaa9d43ad4096adf0e5026f57560d",
            "ea402f2b272c4c7aaa8be4c4db7b4874",
            "edd13942d57742b9a21d188b1ec03d6b",
            "19a9bc62ce4f4be79426c5b45cf221bb",
            "1766566a7e454942925578fbacca8b1b",
            "ed658f262a514ad083749458f2dcc408",
            "f6e7c948dbcf44309ad916828cd15fa1",
            "c582f3c66b2046f994f0aca735dd5dc7",
            "6f5566aa2e7e494ca87f23758979b292",
            "e01d93f7e71844c5ac44f9042b5d2a4f",
            "8571ff32b1e84cca83dbc780fde792af",
            "38b42c4c3fae4ee7a069c12daea1c6ca",
            "694590d2504446b486a80743ba03aa37",
            "8a4b9280369a4e7fab40cd174e3de76f",
            "70a9048ec38b4b5c8c2ef3e72dfb08e7",
            "8b02b7a0272f482fa96227d5d28646f7",
            "24b420ec8956499cb114abe4ea18bf24",
            "56d94ac19312469b8b2a21669054c9d7",
            "5c63f393c28b4f74bf253e07425e4711",
            "069bb0ad30d641969a96315a9e279303",
            "c1f2d9778f15419b9ec93bed86cc0e0f",
            "f4219935e5804ac992c488c06f98b28a",
            "b5b245fa528b4fb09d455376064f9f02",
            "0bd64190681a4e6ab34b14b527d69790",
            "d4b522a10c1741ba973a44fcda643dcc",
            "25dc68291fbd43babf54c2c6ca18dc18",
            "0b0d72ac577a4c849485016399e2c507",
            "059c94cdcf1043dd93540879d98b5dd7",
            "04d66892c9b44798be7a020b4eb72617",
            "ce86bf21bd734af6bde6c756890539bb",
            "5735b4c35a884874817799d2733cf107",
            "d3361886dff64932b9616c97f98f4cd4",
            "e1236af13bee438ea5b247d9c0906558",
            "8300c415118e435ba7ad8e0148f0e19b",
            "fcb4cb1488a6427f9198eaa76149acdf",
            "c016136ec5004817a6b3fc01d558c4ee",
            "3ca6c759b77941f687654f058e802e73",
            "d0fc1ec3795b497cb715281ffe5e5a06",
            "0764a7700efc4c08acc55c69c84ba17f",
            "13b3e12765ea4b94909170c1b21a35ca",
            "c8f08e7112b04478a6e4f6a07a34639f",
            "4a7f30ee55604a1c954a3114a634d0f1",
            "0cadd2f8a1a34947b078f471c900eb02",
            "469eb36ba135442c8f67c09502ada96c",
            "1a1f415c1ca640f0baa79abfd6db25a5",
            "42fa54593a7e406e902b79a90b7348dd",
            "495994f06e3646feaa20d243257d4d0f",
            "23af157bc3c140b39ac404b99bf59b08",
            "909df247fb80457ebeb20d629acd35d6",
            "a058f5e44e2d4065831e7a1d32a33dac",
            "02e6fb78050a4091991b664cb501cdf7",
            "1e0ab156d6354d01a710601af34fd7a0"
          ]
        },
        "id": "_9T1SwW8bQME",
        "outputId": "87ac17c6-bd69-4ca5-fa96-b1ac987d3c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading additional NLTK resources...\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Literary Text to Music Converter\n",
            "================================\n",
            "1. Run demonstration with example text\n",
            "2. Process your own literary text\n",
            "3. Train models\n",
            "4. Exit\n",
            "\n",
            "Enter your choice (1-4): 2\n",
            "Please enter or paste your literary text (type 'END' on a new line when finished):\n",
            "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.  However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families that he is considered as the rightful property of some one or other of their daughters.  \"My dear Mr. Bennet,\" said his lady to him one day, \"have you heard that Netherfield Park is let at last?\"  Mr. Bennet replied that he had not.  \"But it is,\" returned she; \"for Mrs. Long has just been here, and she told me all about it.\"  Mr. Bennet made no answer.  \"Do not you want to know who has taken it?\" cried his wife impatiently.  \"You want to tell me, and I have no objection to hearing it.\"  This was invitation enough.  \"Why, my dear, you must know, Mrs. Long says that Netherfield is taken by a young man of large fortune from the north of England; that he came down on Monday in a chaise and four to see the place, and was so much delighted with it, that he agreed with Mr. Morris immediately; that he is to take possession before Michaelmas, and some of his servants are to be in the house by the end of next week.\"  \"What is his name?\"  \"Bingley.\"  \"Is he married or single?\"  \"Oh! single, my dear, to be sure! A single man of large fortune; four or five thousand a year. What a fine thing for our girls!\"  \"How so? how can it affect them?\"  \"My dear Mr. Bennet,\" replied his wife, \"how can you be so tiresome! You must know that I am thinking of his marrying one of them.\"  \"Is that his design in settling here?\"  \"Design! nonsense, how can you talk so! But it is very likely that he may fall in love with one of them, and therefore you must visit him as soon as he comes.\"  \"I see no occasion for that. You and the girls may go, or you may send them by themselves, which perhaps will be still better, for as you are as handsome as any of them, Mr. Bingley might like you the best of the party.\"  \"My dear, you flatter me. I certainly have had my share of beauty, but I do not pretend to be any thing extraordinary now. When a woman has five grown-up daughters she ought to give over thinking of her own beauty.\"  \"In such cases a woman has not often much beauty to think of.\"  \"But, my dear, you must indeed go and see Mr. Bingley when he comes into the neighbourhood.\"  \"It is more than I engage for, I assure you.\"  \"But consider your daughters. Only think what an establishment it would be for one of them. Sir William and Lady Lucas are determined to go, merely on that account, for in general, you know, they visit no new-comers. Indeed you must go, for it will be impossible for us to visit him if you do not.\"  \"You are over-scrupulous surely. I dare say Mr. Bingley will be very glad to see you; and I will send a few lines by you to assure him of my hearty consent to his marrying whichever he chuses of the girls: though I must throw in a good word for my little Lizzy.\"  \"I desire you will do no such thing. Lizzy is not a bit better than the others; and I am sure she is not half so handsome as Jane, nor half so good-humoured as Lydia. But you are always giving her the preference.\"  \"They have none of them much to recommend them,\" replied he; \"they are all silly and ignorant, like other girls; but Lizzy has something more of quickness than her sisters.\"  \"Mr. Bennet, how can you abuse your own children in such a way! You take delight in vexing me. You have no compassion on my poor nerves.\"  \"You mistake me, my dear. I have a high respect for your nerves. They are my old friends. I have heard you mention them with consideration these twenty years at least.\"  \"Ah! you do not know what I suffer.\"  \"But I hope you will get over it, and live to see many young men of four thousand a year come into the neighbourhood.\"  \"It will be no use to us if twenty such should come, since you will not visit them.\"  \"Depend upon it, my dear, that when there are twenty, I will visit them all.\"  Mr. Bennet was so odd a mixture of quick parts, sarcastic humour, reserve, and caprice, that the experience of three-and-twenty years had been insufficient to make his wife understand his character. Her mind was less difficult to develope. She was a woman of mean understanding, little information, and uncertain temper. When she was discontented she fancied herself nervous. The business of her life was to get her daughters married; its solace was visiting and news.\n",
            "END\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd8de1cfed7148fe9a890204c9ee9735"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a749b020ef6468b9c347473541b6f3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/294 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff9b1506d76a4560a73c16d548ac5592"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "492c48f195d84feab71b6f8602913979"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/329M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f5566aa2e7e494ca87f23758979b292"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "069bb0ad30d641969a96315a9e279303"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5735b4c35a884874817799d2733cf107"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a7f30ee55604a1c954a3114a634d0f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'LiteraryMusicGenerator' object has no attribute 'process_literary_text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-44222b70d936>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;31m# Run the Colab interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m     \u001b[0mcolab_interface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMuLanEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-44222b70d936>\u001b[0m in \u001b[0;36mcolab_interface\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0mprocess_user_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-44222b70d936>\u001b[0m in \u001b[0;36mprocess_user_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;31m# Process the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0mmusic_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLiteraryMusicGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmusic_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_literary_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[0;31m# Display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LiteraryMusicGenerator' object has no attribute 'process_literary_text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import json\n",
        "import os\n",
        "import io\n",
        "import base64\n",
        "import tempfile\n",
        "import random\n",
        "\n",
        "# For MIDI generation\n",
        "from midiutil import MIDIFile\n",
        "import pretty_midi\n",
        "import IPython.display as ipd\n",
        "\n",
        "# Try to import Google Colab files, but handle the case where not running in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "except ImportError:\n",
        "    # Create a dummy files object\n",
        "    class DummyFiles:\n",
        "        def download(self, filename):\n",
        "            print(f\"File download not available outside Colab: {filename}\")\n",
        "    files = DummyFiles()\n",
        "\n",
        "# Download NLTK resources properly\n",
        "# This ensures the data is downloaded correctly\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        # Make sure stopwords are available\n",
        "        try:\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "        except LookupError:\n",
        "            nltk.download('stopwords')\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Make sure wordnet is available\n",
        "        try:\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "        except LookupError:\n",
        "            nltk.download('wordnet')\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Remove punctuation, lowercase, remove stopwords, and lemmatize\"\"\"\n",
        "        # Lowercase the text\n",
        "        text = text.lower()\n",
        "        # Remove punctuation\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "        # Try to tokenize, with fallback method if it fails\n",
        "        try:\n",
        "            # Tokenize into words\n",
        "            words = word_tokenize(text)\n",
        "        except LookupError:\n",
        "            # Fallback: simple space-based tokenization\n",
        "            words = text.split()\n",
        "\n",
        "        # Remove stopwords and lemmatize\n",
        "        cleaned_words = [self.lemmatizer.lemmatize(word) for word in words if word not in self.stop_words]\n",
        "        return ' '.join(cleaned_words)\n",
        "\n",
        "    # Improved TextProcessor with paragraph-aware segmentation\n",
        "    def segment_text(self, text, segment_size=500):\n",
        "        \"\"\"Split text into paragraphs first, then into segments\"\"\"\n",
        "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "        segments = []\n",
        "        for para in paragraphs:\n",
        "            para_sentences = sent_tokenize(para)\n",
        "            current_segment = []\n",
        "            current_length = 0\n",
        "\n",
        "            for sent in para_sentences:\n",
        "                sent_length = len(sent)\n",
        "                if current_length + sent_length > segment_size and current_segment:\n",
        "                    segments.append(' '.join(current_segment))\n",
        "                    current_segment = [sent]\n",
        "                    current_length = sent_length\n",
        "                else:\n",
        "                    current_segment.append(sent)\n",
        "                    current_length += sent_length\n",
        "\n",
        "            if current_segment:\n",
        "                segments.append(' '.join(current_segment))\n",
        "\n",
        "        return segments\n",
        "\n",
        "\n",
        "    def process_text(self, text, segment_size=500):\n",
        "        \"\"\"Process the full text: segment first, then clean each segment\"\"\"\n",
        "        segments = self.segment_text(text, segment_size)\n",
        "        cleaned_segments = [self.clean_text(segment) for segment in segments]\n",
        "        # Also keep the original segments for display purposes\n",
        "        return segments, cleaned_segments\n",
        "\n",
        "class EmotionExtractor:\n",
        "    def __init__(self):\n",
        "        # Load pre-trained emotion classification model\n",
        "        try:\n",
        "            self.emotion_classifier = pipeline(\n",
        "                \"text-classification\",\n",
        "                model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "                top_k=None  # Using top_k=None instead of return_all_scores=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading emotion classifier: {e}\")\n",
        "            # Create a fallback emotion classifier that returns random values\n",
        "            self.emotion_classifier = self._fallback_classifier\n",
        "\n",
        "        # Define our emotion categories\n",
        "        self.emotion_categories = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust', 'neutral']\n",
        "\n",
        "    def _fallback_classifier(self, text):\n",
        "        \"\"\"Fallback classifier that returns random emotion scores\"\"\"\n",
        "        # Generate random scores\n",
        "        scores = np.random.rand(len(self.emotion_categories))\n",
        "        # Normalize to sum to 1\n",
        "        scores = scores / scores.sum()\n",
        "\n",
        "        # Create the expected output format\n",
        "        result = [{\n",
        "            'label': emotion,\n",
        "            'score': float(score)\n",
        "        } for emotion, score in zip(self.emotion_categories, scores)]\n",
        "\n",
        "        return [result]\n",
        "\n",
        "    def extract_emotions(self, text_segments):\n",
        "        \"\"\"Extract emotions from text segments\"\"\"\n",
        "        emotion_maps = []\n",
        "\n",
        "        for segment in text_segments:\n",
        "            # Get emotion scores for the segment\n",
        "            try:\n",
        "                emotion_scores = self.emotion_classifier(segment)[0]\n",
        "            except Exception as e:\n",
        "                print(f\"Error in emotion extraction for segment: {e}\")\n",
        "                # Use fallback with random scores\n",
        "                emotion_scores = self._fallback_classifier(segment)[0]\n",
        "\n",
        "            # Convert to dictionary with emotion as key and score as value\n",
        "            emotion_dict = {item['label']: item['score'] for item in emotion_scores}\n",
        "\n",
        "            # Map the model's emotions to our simplified set\n",
        "            mapped_emotions = {\n",
        "                'joy': emotion_dict.get('joy', 0),\n",
        "                'sadness': emotion_dict.get('sadness', 0),\n",
        "                'anger': emotion_dict.get('anger', 0),\n",
        "                'fear': emotion_dict.get('fear', 0),\n",
        "                'surprise': emotion_dict.get('surprise', 0),\n",
        "                'disgust': emotion_dict.get('disgust', 0),\n",
        "                'neutral': emotion_dict.get('neutral', 0)\n",
        "            }\n",
        "\n",
        "            emotion_maps.append(mapped_emotions)\n",
        "\n",
        "        return emotion_maps\n",
        "\n",
        "    def create_emotional_progression(self, emotion_maps):\n",
        "        \"\"\"Create a time series of emotions for the entire text\"\"\"\n",
        "        progression = {emotion: [] for emotion in self.emotion_categories}\n",
        "\n",
        "        for emotion_map in emotion_maps:\n",
        "            for emotion in self.emotion_categories:\n",
        "                progression[emotion].append(emotion_map[emotion])\n",
        "\n",
        "        return progression\n",
        "\n",
        "    def get_dominant_emotions(self, emotion_maps, top_n=2):\n",
        "        \"\"\"Get the dominant emotions for each segment\"\"\"\n",
        "        dominant_emotions = []\n",
        "\n",
        "        for emotion_map in emotion_maps:\n",
        "            # Sort emotions by score\n",
        "            sorted_emotions = sorted(emotion_map.items(), key=lambda x: x[1], reverse=True)\n",
        "            # Take top n emotions\n",
        "            top_emotions = sorted_emotions[:top_n]\n",
        "            dominant_emotions.append(top_emotions)\n",
        "\n",
        "        return dominant_emotions\n",
        "\n",
        "# Added sequence-aware LSTM architecture\n",
        "class EmotionToMusicMapper(nn.Module):\n",
        "    def __init__(self, input_dim=7, hidden_dim=64, output_dim=10):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        # Take last output in sequence\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        return self.fc(last_output)\n",
        "\n",
        "    def map_to_actual_values(self, normalized_features):\n",
        "        \"\"\"Convert normalized outputs to actual musical values\"\"\"\n",
        "        actual_values = {}\n",
        "\n",
        "        for feature, feature_range in self.musical_features.items():\n",
        "            idx = self.feature_to_idx[feature]\n",
        "            norm_value = normalized_features[idx].item()\n",
        "\n",
        "            # Scale to the actual range\n",
        "            min_val = feature_range['min']\n",
        "            max_val = feature_range['max']\n",
        "            actual_value = min_val + norm_value * (max_val - min_val)\n",
        "\n",
        "            # Round as needed\n",
        "            if feature in ['key', 'instrumentation']:\n",
        "                actual_value = round(actual_value)\n",
        "\n",
        "            actual_values[feature] = actual_value\n",
        "\n",
        "        return actual_values\n",
        "\n",
        "    def get_descriptors_for_emotion_blend(self, emotion_scores):\n",
        "        \"\"\"Get appropriate musical descriptors based on a blend of emotions\"\"\"\n",
        "        # Get top 2 emotions\n",
        "        sorted_emotions = sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_emotions = sorted_emotions[:2]\n",
        "\n",
        "        descriptors = []\n",
        "        instruments = set()\n",
        "\n",
        "        # Weighted selection of descriptors based on emotion intensity\n",
        "        total_weight = sum(score for _, score in top_emotions)\n",
        "\n",
        "        for emotion, score in top_emotions:\n",
        "            # Skip emotions with very low scores\n",
        "            if score < 0.1:\n",
        "                continue\n",
        "\n",
        "            # Weight by the emotion's intensity\n",
        "            weight = score / total_weight\n",
        "\n",
        "            # Get descriptors for this emotion\n",
        "            emotion_descriptors = self.emotion_correlations[emotion]['description']\n",
        "\n",
        "            # Add descriptors proportional to the emotion's weight\n",
        "            num_descriptors = max(1, int(weight * 3))  # At least 1, up to 3 descriptors\n",
        "            selected_descriptors = random.sample(emotion_descriptors, min(num_descriptors, len(emotion_descriptors)))\n",
        "            descriptors.extend(selected_descriptors)\n",
        "\n",
        "            # Add potential instruments\n",
        "            emotion_instruments = self.emotion_correlations[emotion]['instrumentation']\n",
        "            # Choose 1-2 instruments based on weight\n",
        "            num_instruments = max(1, int(weight * 2))\n",
        "            selected_instruments = random.sample(emotion_instruments, min(num_instruments, len(emotion_instruments)))\n",
        "            instruments.update(selected_instruments)\n",
        "\n",
        "        # Return unique descriptors and instruments\n",
        "        return list(set(descriptors)), list(instruments)\n",
        "\n",
        "    def generate_musiclm_prompt(self, musical_features, emotion_scores=None):\n",
        "        \"\"\"Convert musical features to a MusicLM text prompt\"\"\"\n",
        "        # Map key number to name\n",
        "        key_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
        "        key_name = key_names[round(musical_features['key'])]\n",
        "\n",
        "        # Map mode number to name\n",
        "        mode_name = \"major\" if musical_features['mode'] > 0.5 else \"minor\"\n",
        "\n",
        "        # Map instrumentation to instrument types\n",
        "        instrument_types = ['piano', 'strings', 'guitar', 'synth', 'orchestral', 'percussion']\n",
        "        instrument = instrument_types[round(musical_features['instrumentation'])]\n",
        "\n",
        "        # Determine tempo description\n",
        "        tempo = musical_features['tempo']\n",
        "        if tempo < 80:\n",
        "            tempo_desc = \"slow\"\n",
        "        elif tempo < 120:\n",
        "            tempo_desc = \"moderate\"\n",
        "        else:\n",
        "            tempo_desc = \"fast\"\n",
        "\n",
        "        # Determine intensity description\n",
        "        intensity = musical_features['intensity']\n",
        "        if intensity < 0.3:\n",
        "            intensity_desc = \"soft\"\n",
        "        elif intensity < 0.7:\n",
        "            intensity_desc = \"moderate\"\n",
        "        else:\n",
        "            intensity_desc = \"powerful\"\n",
        "\n",
        "        # Get emotional descriptors if emotion scores are provided\n",
        "        if emotion_scores:\n",
        "            emotional_descriptors, suggested_instruments = self.get_descriptors_for_emotion_blend(emotion_scores)\n",
        "\n",
        "            # Use suggested instruments if available, otherwise use the mapped one\n",
        "            if suggested_instruments:\n",
        "                # Prioritize the instrument from musical features but include others\n",
        "                all_instruments = [instrument] + [i for i in suggested_instruments if i != instrument]\n",
        "                # Take up to 2 instruments\n",
        "                instrument_phrase = \" and \".join(all_instruments[:2])\n",
        "            else:\n",
        "                instrument_phrase = instrument\n",
        "\n",
        "            # Build a more emotionally nuanced prompt\n",
        "            prompt = f\"A {' and '.join(emotional_descriptors[:2])} piece in {key_name} {mode_name}, \"\n",
        "            prompt += f\"{tempo_desc} tempo, {intensity_desc} in intensity, \"\n",
        "            prompt += f\"featuring {instrument_phrase}, \"\n",
        "\n",
        "        else:\n",
        "            # Use the original approach as fallback\n",
        "            # Build the prompt\n",
        "            prompt = f\"A {intensity_desc} {tempo_desc} melody in {key_name} {mode_name}, \"\n",
        "\n",
        "            # Add texture\n",
        "            texture = musical_features['texture']\n",
        "            if texture < 0.3:\n",
        "                prompt += \"with a sparse arrangement, \"\n",
        "            elif texture > 0.7:\n",
        "                prompt += \"with a dense, layered arrangement, \"\n",
        "\n",
        "            # Add instrumentation\n",
        "            prompt += f\"featuring {instrument}, \"\n",
        "\n",
        "        # Add complexity\n",
        "        rhythm_complexity = musical_features['rhythm_complexity']\n",
        "        harmonic_complexity = musical_features['harmonic_complexity']\n",
        "\n",
        "        if rhythm_complexity > 0.7 and harmonic_complexity > 0.7:\n",
        "            prompt += \"with complex rhythms and harmonies, \"\n",
        "        elif rhythm_complexity > 0.7:\n",
        "            prompt += \"with complex rhythms, \"\n",
        "        elif harmonic_complexity > 0.7:\n",
        "            prompt += \"with rich harmonies, \"\n",
        "        elif rhythm_complexity < 0.3 and harmonic_complexity < 0.3:\n",
        "            prompt += \"with simple, straightforward patterns, \"\n",
        "\n",
        "        # Add articulation\n",
        "        articulation = musical_features['articulation']\n",
        "        if articulation < 0.3:\n",
        "            prompt += \"played with staccato articulation.\"\n",
        "        elif articulation > 0.7:\n",
        "            prompt += \"played with smooth, legato phrasing.\"\n",
        "        else:\n",
        "            prompt += \"with balanced articulation.\"\n",
        "\n",
        "        # Add emotional context at the end for a more complete prompt\n",
        "        if emotion_scores:\n",
        "            top_emotion = max(emotion_scores.items(), key=lambda x: x[1])\n",
        "            if top_emotion[1] > 0.3:  # Only if the emotion is significant\n",
        "                prompt += f\" The music conveys a sense of {top_emotion[0]}.\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def generate_transition_prompt(self, prev_features, current_features, prev_emotions, current_emotions):\n",
        "        \"\"\"Generate a prompt specifically for transitions between emotional states\"\"\"\n",
        "        # Identify the most significant emotional shift\n",
        "        prev_top_emotion = max(prev_emotions.items(), key=lambda x: x[1])\n",
        "        current_top_emotion = max(current_emotions.items(), key=lambda x: x[1])\n",
        "\n",
        "        # Check if there's a significant emotional shift\n",
        "        if prev_top_emotion[0] != current_top_emotion[0] and prev_top_emotion[1] > 0.3 and current_top_emotion[1] > 0.3:\n",
        "            transition_type = f\"transition from {prev_top_emotion[0]} to {current_top_emotion[0]}\"\n",
        "        else:\n",
        "            # If no major emotional shift, focus on musical parameter changes\n",
        "            changes = []\n",
        "\n",
        "            # Check tempo change\n",
        "            prev_tempo = prev_features['tempo']\n",
        "            current_tempo = current_features['tempo']\n",
        "            if abs(prev_tempo - current_tempo) > 20:  # Significant tempo change\n",
        "                direction = \"accelerating\" if current_tempo > prev_tempo else \"decelerating\"\n",
        "                changes.append(direction)\n",
        "\n",
        "            # Check intensity change\n",
        "            prev_intensity = prev_features['intensity']\n",
        "            current_intensity = current_features['intensity']\n",
        "            if abs(prev_intensity - current_intensity) > 0.3:  # Significant intensity change\n",
        "                direction = \"building in intensity\" if current_intensity > prev_intensity else \"becoming more subdued\"\n",
        "                changes.append(direction)\n",
        "\n",
        "            # Check mode change\n",
        "            prev_mode = \"major\" if prev_features['mode'] > 0.5 else \"minor\"\n",
        "            current_mode = \"major\" if current_features['mode'] > 0.5 else \"minor\"\n",
        "            if prev_mode != current_mode:\n",
        "                changes.append(f\"shifting from {prev_mode} to {current_mode}\")\n",
        "\n",
        "            if changes:\n",
        "                transition_type = \", \".join(changes)\n",
        "            else:\n",
        "                transition_type = \"gradual transition\"\n",
        "\n",
        "        # Generate the transition prompt\n",
        "        prompt = f\"A {transition_type} that maintains musical coherence while \"\n",
        "\n",
        "        # Add emotional descriptors\n",
        "        prev_descriptors, _ = self.get_descriptors_for_emotion_blend(prev_emotions)\n",
        "        current_descriptors, _ = self.get_descriptors_for_emotion_blend(current_emotions)\n",
        "\n",
        "        if prev_descriptors and current_descriptors:\n",
        "            prompt += f\"evolving from {prev_descriptors[0]} to {current_descriptors[0]}. \"\n",
        "\n",
        "        # Add instrumentation continuity\n",
        "        instrument_types = ['piano', 'strings', 'guitar', 'synth', 'orchestral', 'percussion']\n",
        "        prev_instrument = instrument_types[round(prev_features['instrumentation'])]\n",
        "        current_instrument = instrument_types[round(current_features['instrumentation'])]\n",
        "\n",
        "        if prev_instrument == current_instrument:\n",
        "            prompt += f\"Featuring {prev_instrument} throughout. \"\n",
        "        else:\n",
        "            prompt += f\"Transitioning from {prev_instrument} to {current_instrument}. \"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "\n",
        "# Added MusicLM interface component\n",
        "class MusicLMInterface:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/musiclm-medium\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\"google/musiclm-medium\")\n",
        "        self.soundstream = SoundStreamDecoder()\n",
        "\n",
        "    def generate_audio(self, prompt, max_length=512):\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        outputs = self.model.generate(**inputs, max_length=max_length)\n",
        "        audio = self.soundstream.decode(outputs)\n",
        "        return audio\n",
        "\n",
        "# Added sequence modeling wrapper\n",
        "class TemporalCoherenceWrapper(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.temporal_adjuster = nn.LSTM(\n",
        "            input_size=10,\n",
        "            hidden_size=10,\n",
        "            num_layers=2,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "    def forward(self, emotion_sequence):\n",
        "        base_output = self.base_model(emotion_sequence)\n",
        "        adjusted_output, _ = self.temporal_adjuster(base_output)\n",
        "        return adjusted_output.mean(dim=1)\n",
        "\n",
        "class TemporalCoherenceModel(nn.Module):\n",
        "    def __init__(self, input_dim=10, hidden_dim=64, output_dim=10):\n",
        "        super(TemporalCoherenceModel, self).__init__()\n",
        "\n",
        "        # LSTM for sequence modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            dropout=0.3\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        \"\"\"Process a sequence of musical features to ensure temporal coherence\"\"\"\n",
        "        # Expected shape: [batch_size, sequence_length, features]\n",
        "        lstm_out, _ = self.lstm(sequence)\n",
        "\n",
        "        # Apply output layer to each time step\n",
        "        coherent_sequence = self.output_layer(lstm_out)\n",
        "\n",
        "        return coherent_sequence\n",
        "\n",
        "class MusicGenerator:\n",
        "    def __init__(self):\n",
        "        # MIDI settings\n",
        "        self.instruments = {\n",
        "            'piano': 0,       # Acoustic Grand Piano\n",
        "            'strings': 48,    # String Ensemble 1\n",
        "            'guitar': 24,     # Acoustic Guitar (nylon)\n",
        "            'synth': 80,      # Lead 1 (square)\n",
        "            'orchestral': 48, # String Ensemble 1\n",
        "            'percussion': 118 # Synth Drum\n",
        "        }\n",
        "\n",
        "        # Scales (for different keys and modes)\n",
        "        self.major_scale = [0, 2, 4, 5, 7, 9, 11]  # Whole, Whole, Half, Whole, Whole, Whole, Half\n",
        "        self.minor_scale = [0, 2, 3, 5, 7, 8, 10]  # Whole, Half, Whole, Whole, Half, Whole, Whole\n",
        "\n",
        "        # Common chord progressions\n",
        "        self.progressions = {\n",
        "            'major': [\n",
        "                [1, 4, 5, 1],       # I-IV-V-I\n",
        "                [1, 6, 4, 5],       # I-vi-IV-V\n",
        "                [1, 5, 6, 4],       # I-V-vi-IV\n",
        "                [2, 5, 1, 6]        # ii-V-I-vi\n",
        "            ],\n",
        "            'minor': [\n",
        "                [1, 4, 5, 1],       # i-iv-v-i\n",
        "                [1, 6, 3, 7],       # i-VI-III-VII\n",
        "                [1, 7, 6, 5],       # i-VII-VI-v\n",
        "                [1, 4, 7, 3]        # i-iv-VII-III\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def create_midi_from_features(self, musical_features, duration_seconds=15):\n",
        "        \"\"\"Generate MIDI file from musical features\"\"\"\n",
        "        # Extract features\n",
        "        tempo = musical_features['tempo']\n",
        "        key = int(musical_features['key'])\n",
        "        mode = 'major' if musical_features['mode'] > 0.5 else 'minor'\n",
        "        instrument_type = ['piano', 'strings', 'guitar', 'synth', 'orchestral', 'percussion'][int(musical_features['instrumentation'])]\n",
        "        instrument = self.instruments[instrument_type]\n",
        "\n",
        "        # Derived parameters\n",
        "        rhythm_complexity = musical_features['rhythm_complexity']\n",
        "        harmonic_complexity = musical_features['harmonic_complexity']\n",
        "        melodic_range = musical_features['melodic_range']\n",
        "        intensity = musical_features['intensity']\n",
        "        texture = musical_features['texture']\n",
        "        articulation = musical_features['articulation']\n",
        "\n",
        "        # Create MIDI file\n",
        "        midi = MIDIFile(2)  # 2 tracks - one for melody, one for accompaniment\n",
        "        track_melody = 0\n",
        "        track_accomp = 1\n",
        "\n",
        "        # Set tempo\n",
        "        midi.addTempo(track_melody, 0, tempo)\n",
        "        midi.addTempo(track_accomp, 0, tempo)\n",
        "\n",
        "        # Choose scale based on mode\n",
        "        scale = self.major_scale if mode == 'major' else self.minor_scale\n",
        "\n",
        "        # Calculate beats based on tempo and duration\n",
        "        total_beats = int((tempo / 60) * duration_seconds)\n",
        "\n",
        "        # Set program (instrument)\n",
        "        midi.addProgramChange(track_melody, 0, 0, instrument)\n",
        "        midi.addProgramChange(track_accomp, 0, 0, instrument)\n",
        "\n",
        "        # Choose chord progression based on mode and complexity\n",
        "        prog_idx = min(int(harmonic_complexity * len(self.progressions[mode])), len(self.progressions[mode]) - 1)\n",
        "        progression = self.progressions[mode][prog_idx]\n",
        "\n",
        "        # Chord duration in beats\n",
        "        chord_duration = max(4, total_beats // len(progression))\n",
        "        repetitions = max(1, total_beats // (chord_duration * len(progression)))\n",
        "\n",
        "        # Base octave for melody\n",
        "        base_octave = 5 if instrument_type != 'piano' else 4\n",
        "\n",
        "        # Calculate note range based on melodic range\n",
        "        low_note = base_octave * 12  # C in the base octave\n",
        "        high_note = low_note + int(12 + 12 * melodic_range)  # Up to an octave or two higher\n",
        "\n",
        "        # Velocity (volume) based on intensity\n",
        "        melody_velocity = 64 + int(intensity * 63)  # Between 64-127\n",
        "        chord_velocity = int(melody_velocity * 0.8)  # Slightly quieter\n",
        "\n",
        "        # Note duration modifier based on articulation (staccato to legato)\n",
        "        duration_modifier = 0.5 + (articulation * 0.5)  # 0.5 (staccato) to 1.0 (legato)\n",
        "\n",
        "        # Generate melody and chords\n",
        "        current_beat = 0\n",
        "        for rep in range(repetitions):\n",
        "            for chord_idx, chord_root in enumerate(progression):\n",
        "                # Map chord root to actual note in the key\n",
        "                chord_root_idx = chord_root - 1  # Adjust for 0-indexing\n",
        "                root_note = (key + scale[chord_root_idx]) % 12\n",
        "\n",
        "                # Determine chord type based on position in scale\n",
        "                is_major_chord = chord_root_idx in [0, 3, 4] if mode == 'major' else chord_root_idx in [2, 5]\n",
        "\n",
        "                # Create chord notes (root, third, fifth)\n",
        "                third_offset = 4 if is_major_chord else 3\n",
        "                chord_notes = [\n",
        "                    root_note + 60,  # Root note (C4 = 60, middle C)\n",
        "                    root_note + 60 + third_offset,  # Third\n",
        "                    root_note + 60 + 7   # Fifth\n",
        "                ]\n",
        "\n",
        "                # Add chord to accompaniment track with texture variation\n",
        "                if texture < 0.3:\n",
        "                    # Sparse - just root and fifth\n",
        "                    midi.addNote(track_accomp, 0, chord_notes[0], current_beat, chord_duration * 0.9, chord_velocity)\n",
        "                    midi.addNote(track_accomp, 0, chord_notes[2], current_beat, chord_duration * 0.9, chord_velocity)\n",
        "                elif texture < 0.7:\n",
        "                    # Medium - broken chord\n",
        "                    for i, note in enumerate(chord_notes):\n",
        "                        midi.addNote(track_accomp, 0, note, current_beat + i*0.5, chord_duration * 0.9 - i*0.5, chord_velocity)\n",
        "                else:\n",
        "                    # Dense - full chord plus extra notes\n",
        "                    for note in chord_notes:\n",
        "                        midi.addNote(track_accomp, 0, note, current_beat, chord_duration * 0.9, chord_velocity)\n",
        "                    # Add extra notes for texture\n",
        "                    midi.addNote(track_accomp, 0, chord_notes[0] + 12, current_beat + 1, chord_duration * 0.4, chord_velocity - 10)\n",
        "\n",
        "                # Generate melody for this chord\n",
        "                notes_per_beat = 1 + int(rhythm_complexity * 3)  # 1 to 4 notes per beat\n",
        "\n",
        "                for beat_offset in range(chord_duration):\n",
        "                    # Skip some beats randomly for variation\n",
        "                    if random.random() < 0.2:\n",
        "                        continue\n",
        "\n",
        "                    for note_idx in range(notes_per_beat):\n",
        "                        # Calculate precise timing\n",
        "                        note_start = current_beat + beat_offset + (note_idx / notes_per_beat)\n",
        "\n",
        "                        # Choose note from scale\n",
        "                        scale_idx = random.randint(0, len(scale) - 1)\n",
        "                        note = key + scale[scale_idx]\n",
        "\n",
        "                        # Map to the right octave range\n",
        "                        octave = random.randint(base_octave, base_octave + 1)\n",
        "                        note = (note % 12) + (octave * 12)\n",
        "\n",
        "                        # Ensure note is in our range\n",
        "                        note = max(low_note, min(note, high_note))\n",
        "\n",
        "                        # Calculate duration based on articulation and rhythm\n",
        "                        note_duration = (1.0 / notes_per_beat) * duration_modifier\n",
        "\n",
        "                        # Add note to melody track\n",
        "                        if random.random() < 0.8:  # 80% chance to add a note (for rests)\n",
        "                            midi.addNote(track_melody, 0, note, note_start, note_duration, melody_velocity)\n",
        "\n",
        "                current_beat += chord_duration\n",
        "\n",
        "        # Write MIDI file to bytes buffer\n",
        "        buffer = io.BytesIO()\n",
        "        midi.writeFile(buffer)\n",
        "        buffer.seek(0)\n",
        "\n",
        "        return buffer\n",
        "\n",
        "class LiteraryMusicGenerator:\n",
        "    # ... existing code ...\n",
        "\n",
        "    def generate_music_score(self, emotion_maps, musical_features):\n",
        "        \"\"\"Generate a detailed music score with more advanced musical theory\"\"\"\n",
        "        scores = []\n",
        "\n",
        "        for i, (emotion_map, features) in enumerate(zip(emotion_maps, musical_features)):\n",
        "            # Map normalized features to actual values\n",
        "            actual_values = self.emotion_to_music_mapper.map_to_actual_values(features)\n",
        "\n",
        "            # Determine key signature and scale\n",
        "            key_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
        "            key_name = key_names[round(actual_values['key'])]\n",
        "            mode_name = \"major\" if actual_values['mode'] > 0.5 else \"minor\"\n",
        "            scale = self.music_generator.major_scale if mode_name == \"major\" else self.music_generator.minor_scale\n",
        "\n",
        "            # Determine chord progression based on emotion\n",
        "            top_emotion = max(emotion_map.items(), key=lambda x: x[1])[0]\n",
        "            progression_index = min(int(actual_values['harmonic_complexity'] *\n",
        "                                    len(self.music_generator.progressions[mode_name])),\n",
        "                                    len(self.music_generator.progressions[mode_name]) - 1)\n",
        "            progression = self.music_generator.progressions[mode_name][progression_index]\n",
        "\n",
        "            # Create detailed score information\n",
        "            score_info = {\n",
        "                \"segment\": i,\n",
        "                \"key\": key_name,\n",
        "                \"mode\": mode_name,\n",
        "                \"tempo\": actual_values['tempo'],\n",
        "                \"time_signature\": \"4/4\",  # Default, could be variable\n",
        "                \"dominant_emotion\": top_emotion,\n",
        "                \"chord_progression\": [f\"{key_name} {self._chord_notation(p, mode_name)}\" for p in progression],\n",
        "                \"instrumentation\": self._get_instrumentation(actual_values['instrumentation']),\n",
        "                \"dynamics\": self._get_dynamics(actual_values['intensity']),\n",
        "                \"rhythmic_feel\": self._get_rhythmic_feel(actual_values['rhythm_complexity']),\n",
        "                \"melodic_contour\": self._generate_melodic_contour(emotion_map)\n",
        "            }\n",
        "\n",
        "            scores.append(score_info)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def _chord_notation(self, degree, mode):\n",
        "        \"\"\"Convert scale degree to chord notation\"\"\"\n",
        "        if mode == \"major\":\n",
        "            if degree in [1, 4, 5]:\n",
        "                return \"\"  # Major chord\n",
        "            elif degree in [2, 3, 6]:\n",
        "                return \"m\"  # Minor chord\n",
        "            else:\n",
        "                return \"dim\"  # Diminished chord\n",
        "        else:  # minor\n",
        "            if degree in [3, 6, 7]:\n",
        "                return \"\"  # Major chord\n",
        "            elif degree in [1, 4, 5]:\n",
        "                return \"m\"  # Minor chord\n",
        "            else:\n",
        "                return \"dim\"  # Diminished chord\n",
        "\n",
        "    def _get_instrumentation(self, instrumentation_value):\n",
        "        \"\"\"Get detailed instrumentation based on the value\"\"\"\n",
        "        instrument_types = ['piano', 'strings', 'guitar', 'synth', 'orchestral', 'percussion']\n",
        "        primary = instrument_types[round(instrumentation_value)]\n",
        "\n",
        "        # Add secondary instruments based on primary\n",
        "        if primary == \"piano\":\n",
        "            secondary = [\"soft strings\", \"light percussion\"]\n",
        "        elif primary == \"strings\":\n",
        "            secondary = [\"harp\", \"woodwinds\"]\n",
        "        elif primary == \"guitar\":\n",
        "            secondary = [\"bass\", \"light percussion\"]\n",
        "        elif primary == \"synth\":\n",
        "            secondary = [\"bass synth\", \"electronic drums\"]\n",
        "        elif primary == \"orchestral\":\n",
        "            secondary = [\"brass\", \"timpani\"]\n",
        "        else:  # percussion\n",
        "            secondary = [\"bass\", \"synth pads\"]\n",
        "\n",
        "        return {\"primary\": primary, \"secondary\": secondary}\n",
        "\n",
        "    def _get_dynamics(self, intensity):\n",
        "        \"\"\"Convert intensity to musical dynamics notation\"\"\"\n",
        "        if intensity < 0.2:\n",
        "            return \"pp (pianissimo)\"\n",
        "        elif intensity < 0.4:\n",
        "            return \"p (piano)\"\n",
        "        elif intensity < 0.6:\n",
        "            return \"mp (mezzo-piano) to mf (mezzo-forte)\"\n",
        "        elif intensity < 0.8:\n",
        "            return \"f (forte)\"\n",
        "        else:\n",
        "            return \"ff (fortissimo)\"\n",
        "\n",
        "    def _get_rhythmic_feel(self, complexity):\n",
        "        \"\"\"Determine rhythmic feel based on complexity\"\"\"\n",
        "        if complexity < 0.3:\n",
        "            return \"simple, steady rhythm with minimal syncopation\"\n",
        "        elif complexity < 0.7:\n",
        "            return \"moderately complex with some syncopation and rhythmic variety\"\n",
        "        else:\n",
        "            return \"complex polyrhythms with significant syncopation and rhythmic tension\"\n",
        "\n",
        "    def _generate_melodic_contour(self, emotion_map):\n",
        "        \"\"\"Generate description of melodic contour based on emotions\"\"\"\n",
        "        if emotion_map['joy'] > 0.5:\n",
        "            return \"rising, upward melodic contour with occasional leaps\"\n",
        "        elif emotion_map['sadness'] > 0.5:\n",
        "            return \"gradually descending melodic contour with stepwise motion\"\n",
        "        elif emotion_map['anger'] > 0.5:\n",
        "            return \"angular melodic contour with dramatic leaps and falls\"\n",
        "        elif emotion_map['fear'] > 0.5:\n",
        "            return \"unstable melodic contour with unpredictable direction changes\"\n",
        "        elif emotion_map['surprise'] > 0.5:\n",
        "            return \"playful melodic contour with unexpected intervals\"\n",
        "        else:\n",
        "            return \"balanced melodic contour with moderate movement\"\n",
        "\n",
        "    def generate_music_visualization(self, musical_features, emotion_maps):\n",
        "        \"\"\"Generate visual representation of the music for each segment\"\"\"\n",
        "        visualizations = []\n",
        "\n",
        "        for i, (features, emotion_map) in enumerate(zip(musical_features, emotion_maps)):\n",
        "            # Map normalized features to actual values\n",
        "            actual_values = self.emotion_to_music_mapper.map_to_actual_values(features)\n",
        "\n",
        "            # Create visualization data\n",
        "            viz_data = {\n",
        "                \"segment\": i,\n",
        "                \"waveform_parameters\": {\n",
        "                    \"amplitude\": actual_values['intensity'] * 100,\n",
        "                    \"frequency\": 220 + actual_values['key'] * 20,  # Base frequency affected by key\n",
        "                    \"waveform_type\": self._get_waveform_type(emotion_map)\n",
        "                },\n",
        "                \"color_scheme\": self._get_emotion_color(emotion_map),\n",
        "                \"texture_density\": actual_values['texture'] * 100,\n",
        "                \"motion_parameters\": {\n",
        "                    \"speed\": actual_values['tempo'] / 120,  # Normalized to 1.0 at tempo 120\n",
        "                    \"direction\": self._get_motion_direction(emotion_map),\n",
        "                    \"pattern\": self._get_motion_pattern(actual_values['rhythm_complexity'])\n",
        "                }\n",
        "            }\n",
        "\n",
        "            visualizations.append(viz_data)\n",
        "\n",
        "        return visualizations\n",
        "\n",
        "    def _get_waveform_type(self, emotion_map):\n",
        "        \"\"\"Determine appropriate waveform type based on emotion\"\"\"\n",
        "        top_emotion = max(emotion_map.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "        if top_emotion == 'joy':\n",
        "            return \"sine\"\n",
        "        elif top_emotion == 'sadness':\n",
        "            return \"triangle\"\n",
        "        elif top_emotion == 'anger':\n",
        "            return \"sawtooth\"\n",
        "        elif top_emotion == 'fear':\n",
        "            return \"square with noise\"\n",
        "        elif top_emotion == 'surprise':\n",
        "            return \"modulated sine\"\n",
        "        else:\n",
        "            return \"sine with harmonics\"\n",
        "\n",
        "    def _get_emotion_color(self, emotion_map):\n",
        "        \"\"\"Map emotions to colors using common associations\"\"\"\n",
        "        # Calculate weighted color based on emotion intensities\n",
        "        colors = {\n",
        "            'joy': {\"r\": 255, \"g\": 215, \"b\": 0},      # Gold/Yellow\n",
        "            'sadness': {\"r\": 0, \"g\": 0, \"b\": 139},    # Dark Blue\n",
        "            'anger': {\"r\": 220, \"g\": 20, \"b\": 60},    # Crimson/Red\n",
        "            'fear': {\"r\": 75, \"g\": 0, \"b\": 130},      # Indigo/Purple\n",
        "            'surprise': {\"r\": 0, \"g\": 191, \"b\": 255}, # Deep Sky Blue\n",
        "            'disgust': {\"r\": 107, \"g\": 142, \"b\": 35}, # Olive Green\n",
        "            'neutral': {\"r\": 169, \"g\": 169, \"b\": 169}  # Gray\n",
        "        }\n",
        "\n",
        "        # Sum the weighted RGB values\n",
        "        r = g = b = 0\n",
        "        total_weight = 0\n",
        "\n",
        "        for emotion, intensity in emotion_map.items():\n",
        "            if intensity > 0.1:  # Only consider emotions with significant intensity\n",
        "                r += colors[emotion][\"r\"] * intensity\n",
        "                g += colors[emotion][\"g\"] * intensity\n",
        "                b += colors[emotion][\"b\"] * intensity\n",
        "                total_weight += intensity\n",
        "\n",
        "        # Normalize\n",
        "        if total_weight > 0:\n",
        "            r = int(r / total_weight)\n",
        "            g = int(g / total_weight)\n",
        "            b = int(b / total_weight)\n",
        "        else:\n",
        "            r, g, b = 169, 169, 169  # Default gray\n",
        "\n",
        "        return {\"r\": r, \"g\": g, \"b\": b, \"hex\": f\"#{r:02x}{g:02x}{b:02x}\"}\n",
        "\n",
        "    def _get_motion_direction(self, emotion_map):\n",
        "        \"\"\"Determine motion direction based on dominant emotion\"\"\"\n",
        "        if emotion_map['joy'] > 0.5:\n",
        "            return \"upward\"\n",
        "        elif emotion_map['sadness'] > 0.5:\n",
        "            return \"downward\"\n",
        "        elif emotion_map['anger'] > 0.5:\n",
        "            return \"outward\"\n",
        "        elif emotion_map['fear'] > 0.5:\n",
        "            return \"inward\"\n",
        "        else:\n",
        "            return \"horizontal\"\n",
        "\n",
        "    def _get_motion_pattern(self, complexity):\n",
        "        \"\"\"Determine motion pattern based on rhythmic complexity\"\"\"\n",
        "        if complexity < 0.3:\n",
        "            return \"linear\"\n",
        "        elif complexity < 0.7:\n",
        "            return \"undulating\"\n",
        "        else:\n",
        "            return \"complex\"\n",
        "\n",
        "    def create_emotional_narrative_score(self, emotion_maps, text_segments):\n",
        "        \"\"\"Create a narrative score that maps emotions to musical directions\"\"\"\n",
        "        narrative_score = []\n",
        "\n",
        "        for i, (emotion_map, text) in enumerate(zip(emotion_maps, text_segments)):\n",
        "            # Get dominant emotions\n",
        "            emotions_sorted = sorted(emotion_map.items(), key=lambda x: x[1], reverse=True)\n",
        "            primary_emotion = emotions_sorted[0][0]\n",
        "            primary_intensity = emotions_sorted[0][1]\n",
        "\n",
        "            # Get secondary emotion if available and significant\n",
        "            secondary_emotion = None\n",
        "            secondary_intensity = 0\n",
        "            if len(emotions_sorted) > 1 and emotions_sorted[1][1] > 0.25:\n",
        "                secondary_emotion = emotions_sorted[1][0]\n",
        "                secondary_intensity = emotions_sorted[1][1]\n",
        "\n",
        "            # Create musical direction based on emotions\n",
        "            musical_direction = self._emotion_to_musical_direction(primary_emotion, primary_intensity)\n",
        "\n",
        "            # Add modifications from secondary emotion if applicable\n",
        "            if secondary_emotion:\n",
        "                musical_direction += \" \" + self._secondary_emotion_modification(secondary_emotion, secondary_intensity)\n",
        "\n",
        "            # Create entry\n",
        "            narrative_entry = {\n",
        "                \"segment\": i,\n",
        "                \"text_excerpt\": text[:100] + \"...\" if len(text) > 100 else text,\n",
        "                \"primary_emotion\": {\"name\": primary_emotion, \"intensity\": float(primary_intensity)},\n",
        "                \"secondary_emotion\": {\"name\": secondary_emotion, \"intensity\": float(secondary_intensity)} if secondary_emotion else None,\n",
        "                \"musical_direction\": musical_direction,\n",
        "                \"performance_notes\": self._generate_performance_notes(emotion_map)\n",
        "            }\n",
        "\n",
        "            narrative_score.append(narrative_entry)\n",
        "\n",
        "        return narrative_score\n",
        "\n",
        "    def _emotion_to_musical_direction(self, emotion, intensity):\n",
        "        \"\"\"Convert emotion to musical performance direction\"\"\"\n",
        "        intensity_modifier = \"\"\n",
        "        if intensity > 0.8:\n",
        "            intensity_modifier = \"intensely \"\n",
        "        elif intensity > 0.5:\n",
        "            intensity_modifier = \"moderately \"\n",
        "        else:\n",
        "            intensity_modifier = \"slightly \"\n",
        "\n",
        "        if emotion == 'joy':\n",
        "            return f\"{intensity_modifier}vivace (lively and brisk)\"\n",
        "        elif emotion == 'sadness':\n",
        "            return f\"{intensity_modifier}lamentoso (lamenting, sorrowful)\"\n",
        "        elif emotion == 'anger':\n",
        "            return f\"{intensity_modifier}con fuoco (with fire and passion)\"\n",
        "        elif emotion == 'fear':\n",
        "            return f\"{intensity_modifier}misterioso (mysterious, uneasy)\"\n",
        "        elif emotion == 'surprise':\n",
        "            return f\"{intensity_modifier}capriccioso (playful, unpredictable)\"\n",
        "        elif emotion == 'disgust':\n",
        "            return f\"{intensity_modifier}pesante (heavy, ponderous)\"\n",
        "        else:  # neutral\n",
        "            return \"moderato (moderate tempo with balanced expression)\"\n",
        "\n",
        "    def _secondary_emotion_modification(self, emotion, intensity):\n",
        "        \"\"\"Generate modification based on secondary emotion\"\"\"\n",
        "        if emotion == 'joy':\n",
        "            return \"with occasional bright passages\"\n",
        "        elif emotion == 'sadness':\n",
        "            return \"with moments of reflection\"\n",
        "        elif emotion == 'anger':\n",
        "            return \"with underlying tension\"\n",
        "        elif emotion == 'fear':\n",
        "            return \"with hints of uncertainty\"\n",
        "        elif emotion == 'surprise':\n",
        "            return \"with unexpected shifts\"\n",
        "        elif emotion == 'disgust':\n",
        "            return \"with moments of dissonance\"\n",
        "        else:  # neutral\n",
        "            return \"maintaining balanced phrasing\"\n",
        "\n",
        "    def _generate_performance_notes(self, emotion_map):\n",
        "        \"\"\"Generate specific performance notes based on emotional content\"\"\"\n",
        "        notes = []\n",
        "\n",
        "        # Check for specific emotional combinations and generate appropriate notes\n",
        "        if emotion_map['joy'] > 0.3 and emotion_map['surprise'] > 0.3:\n",
        "            notes.append(\"Use rubato freely to emphasize surprising elements while maintaining a joyful character\")\n",
        "\n",
        "        if emotion_map['sadness'] > 0.3 and emotion_map['fear'] > 0.2:\n",
        "            notes.append(\"Allow subtle dissonances to linger, emphasizing the uneasy quality beneath the sadness\")\n",
        "\n",
        "        if emotion_map['anger'] > 0.5:\n",
        "            notes.append(\"Emphasize rhythmic accents and use more percussive articulation\")\n",
        "\n",
        "        if emotion_map['neutral'] > 0.4:\n",
        "            notes.append(\"Maintain tonal clarity and balanced phrasing\")\n",
        "\n",
        "        if emotion_map['surprise'] > 0.5:\n",
        "            notes.append(\"Insert brief pauses before unexpected harmonic shifts\")\n",
        "\n",
        "        # Add default note if none were generated\n",
        "        if not notes:\n",
        "            top_emotion = max(emotion_map.items(), key=lambda x: x[1])[0]\n",
        "            intensity = emotion_map[top_emotion]\n",
        "\n",
        "            if intensity > 0.7:\n",
        "                notes.append(f\"Focus primarily on expressing {top_emotion} with full emotional commitment\")\n",
        "            else:\n",
        "                notes.append(f\"Express {top_emotion} with nuance, allowing for subtle emotional shifts\")\n",
        "\n",
        "        return notes\n",
        "\n",
        "    def generate_adaptive_composition(self, user_parameters, text):\n",
        "        \"\"\"Generate a composition with user-defined parameters that affect emotional mapping\"\"\"\n",
        "        # Process text normally first\n",
        "        result = self.process_literary_text(text)\n",
        "\n",
        "        # Apply user parameters to modify the results\n",
        "        modified_result = self._apply_user_modifications(result, user_parameters)\n",
        "\n",
        "        return modified_result\n",
        "\n",
        "    def _apply_user_modifications(self, original_result, user_parameters):\n",
        "        \"\"\"Apply user customizations to the generated music\"\"\"\n",
        "        modified_result = original_result.copy()\n",
        "\n",
        "        # Modify tempo range if specified\n",
        "        if 'tempo_range' in user_parameters:\n",
        "            min_tempo, max_tempo = user_parameters['tempo_range']\n",
        "            for i, prompt_data in enumerate(modified_result['musiclm_prompts']):\n",
        "                # Adjust the tempo within musical features\n",
        "                original_tempo = prompt_data['musical_features']['tempo']\n",
        "                # Scale the tempo to the new range\n",
        "                normalized_tempo = (original_tempo - 60) / 120  # Normalize to 0-1 range\n",
        "                new_tempo = min_tempo + normalized_tempo * (max_tempo - min_tempo)\n",
        "                modified_result['musiclm_prompts'][i]['musical_features']['tempo'] = new_tempo\n",
        "\n",
        "                # Regenerate MIDI with new tempo\n",
        "                midi_buffer = self.music_generator.create_midi_from_features(\n",
        "                    modified_result['musiclm_prompts'][i]['musical_features'])\n",
        "                modified_result['midi_files'][i] = midi_buffer\n",
        "\n",
        "        # Modify instrument selection if specified\n",
        "        if 'preferred_instruments' in user_parameters:\n",
        "            preferred_instruments = user_parameters['preferred_instruments']\n",
        "            instrument_mapping = {\n",
        "                'piano': 0,\n",
        "                'strings': 1,\n",
        "                'guitar': 2,\n",
        "                'synth': 3,\n",
        "                'orchestral': 4,\n",
        "                'percussion': 5\n",
        "            }\n",
        "\n",
        "            for i, prompt_data in enumerate(modified_result['musiclm_prompts']):\n",
        "                # Get the emotion that would be best represented by each preferred instrument\n",
        "                emotion_instrument_match = self._match_emotions_to_instruments(\n",
        "                    prompt_data['emotion_map'], preferred_instruments)\n",
        "\n",
        "                # Update the instrumentation value\n",
        "                if emotion_instrument_match:\n",
        "                    instrument_value = instrument_mapping.get(emotion_instrument_match, 0)\n",
        "                    modified_result['musiclm_prompts'][i]['musical_features']['instrumentation'] = instrument_value\n",
        "\n",
        "                    # Regenerate MIDI with new instrumentation\n",
        "                    midi_buffer = self.music_generator.create_midi_from_features(\n",
        "                        modified_result['musiclm_prompts'][i]['musical_features'])\n",
        "                    modified_result['midi_files'][i] = midi_buffer\n",
        "\n",
        "        # Modify mode preference if specified\n",
        "        if 'mode_preference' in user_parameters:\n",
        "            mode_pref = user_parameters['mode_preference']  # 'major', 'minor', or 'follow_emotion'\n",
        "\n",
        "            if mode_pref != 'follow_emotion':\n",
        "                mode_value = 1.0 if mode_pref == 'major' else 0.0\n",
        "\n",
        "                for i, prompt_data in enumerate(modified_result['musiclm_prompts']):\n",
        "                    modified_result['musiclm_prompts'][i]['musical_features']['mode'] = mode_value\n",
        "\n",
        "                    # Regenerate MIDI with new mode\n",
        "                    midi_buffer = self.music_generator.create_midi_from_features(\n",
        "                        modified_result['musiclm_prompts'][i]['musical_features'])\n",
        "                    modified_result['midi_files'][i] = midi_buffer\n",
        "\n",
        "        # Add user-specific narrative if requested\n",
        "        if 'personalized_narrative' in user_parameters and user_parameters['personalized_narrative']:\n",
        "            user_name = user_parameters.get('user_name', 'User')\n",
        "            modified_result['personalized_narrative'] = self._generate_personalized_narrative(\n",
        "                original_result['emotion_progression'], user_name)\n",
        "\n",
        "        return modified_result\n",
        "\n",
        "    def _match_emotions_to_instruments(self, emotion_map, preferred_instruments):\n",
        "        \"\"\"Match emotions in the segment to preferred instruments\"\"\"\n",
        "        # Define emotional affinities for different instruments\n",
        "        instrument_emotion_affinity = {\n",
        "            'piano': ['joy', 'sadness', 'neutral'],\n",
        "            'strings': ['sadness', 'fear', 'joy'],\n",
        "            'guitar': ['neutral', 'sadness', 'joy'],\n",
        "            'synth': ['surprise', 'fear', 'anger'],\n",
        "            'orchestral': ['joy', 'fear', 'anger'],\n",
        "            'percussion': ['anger', 'surprise', 'disgust']\n",
        "        }\n",
        "\n",
        "        # Find the dominant emotion\n",
        "        dominant_emotion = max(emotion_map.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "        # Check which preferred instrument has the best affinity for this emotion\n",
        "        best_match = None\n",
        "        best_affinity = -1\n",
        "\n",
        "        for instrument in preferred_instruments:\n",
        "            if instrument in instrument_emotion_affinity:\n",
        "                affinities = instrument_emotion_affinity[instrument]\n",
        "                if dominant_emotion in affinities:\n",
        "                    affinity_score = 3 - affinities.index(dominant_emotion)  # Higher score for earlier in the list\n",
        "                    if affinity_score > best_affinity:\n",
        "                        best_affinity = affinity_score\n",
        "                        best_match = instrument\n",
        "\n",
        "        # If no good match, just use the first preferred instrument\n",
        "        if not best_match and preferred_instruments:\n",
        "            best_match = preferred_instruments[0]\n",
        "\n",
        "        return best_match\n",
        "\n",
        "    def _generate_personalized_narrative(self, emotion_progression, user_name):\n",
        "        \"\"\"Generate a personalized narrative about the emotional journey\"\"\"\n",
        "        # Identify the emotional journey\n",
        "        emotional_journey = []\n",
        "        for emotion, values in emotion_progression.items():\n",
        "            avg_intensity = sum(values) / len(values)\n",
        "            if avg_intensity > 0.2:  # Only consider emotions with significant presence\n",
        "                emotional_journey.append((emotion, avg_intensity))\n",
        "\n",
        "        # Sort by intensity\n",
        "        emotional_journey.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Create personalized narrative\n",
        "        narrative = f\"Dear {user_name},\\n\\n\"\n",
        "        narrative += \"Your literary journey evokes a rich emotional tapestry that I've translated into music. \"\n",
        "\n",
        "        if emotional_journey:\n",
        "            top_emotion, top_intensity = emotional_journey[0]\n",
        "            narrative += f\"The predominant feeling is {top_emotion}, \"\n",
        "\n",
        "            if top_emotion == 'joy':\n",
        "                narrative += \"suggesting a bright, uplifting quality in your writing. \"\n",
        "            elif top_emotion == 'sadness':\n",
        "                narrative += \"revealing depth and poignancy in your narrative. \"\n",
        "            elif top_emotion == 'anger':\n",
        "                narrative += \"showing passionate intensity and powerful expression. \"\n",
        "            elif top_emotion == 'fear':\n",
        "                narrative += \"creating tension and anticipation throughout your text. \"\n",
        "            elif top_emotion == 'surprise':\n",
        "                narrative += \"indicating unexpected turns and creative unpredictability. \"\n",
        "            else:\n",
        "                narrative += \"creating a distinctive mood throughout your text. \"\n",
        "\n",
        "            # Add secondary emotions\n",
        "            if len(emotional_journey) > 1:\n",
        "                narrative += \"This is beautifully complemented by undertones of \"\n",
        "                secondary_emotions = [e[0] for e in emotional_journey[1:3]]  # Up to 2 secondary emotions\n",
        "                narrative += \" and \".join(secondary_emotions) + \". \"\n",
        "\n",
        "        narrative += \"\\n\\nThe music I've composed reflects these emotional qualities, with each segment \"\n",
        "        narrative += \"carefully crafted to enhance the specific mood of your writing. \"\n",
        "        narrative += \"As the narrative progresses, listen for the subtle shifts in melody, harmony, and tempo \"\n",
        "        narrative += \"that mirror the emotional journey of your text.\\n\\n\"\n",
        "        narrative += \"I hope this musical interpretation resonates with your creative vision.\\n\\n\"\n",
        "        narrative += \"Musically yours,\\nLiterary Music Generator\"\n",
        "\n",
        "        return narrative\n",
        "\n",
        "    def export_full_composition(self, result, output_format=\"midi\", output_path=\"literary_composition\"):\n",
        "        \"\"\"Export the full musical composition in the specified format\"\"\"\n",
        "        if output_format == \"midi\":\n",
        "            return self._export_midi_composition(result, output_path)\n",
        "        elif output_format == \"json\":\n",
        "            return self._export_json_composition(result, output_path)\n",
        "        elif output_format == \"musicxml\":\n",
        "            return self._export_musicxml_composition(result, output_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported output format: {output_format}\")\n",
        "\n",
        "    def _export_midi_composition(self, result, output_path):\n",
        "        \"\"\"Export the composition as individual MIDI files and a combined file\"\"\"\n",
        "        # Ensure output directory exists\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        # Save individual segment files\n",
        "        segment_files = []\n",
        "        for i, midi_buffer in enumerate(result['midi_files']):\n",
        "            filename = f\"{output_path}/segment_{i+1}.mid\"\n",
        "            with open(filename, 'wb') as f:\n",
        "                f.write(midi_buffer.getvalue())\n",
        "            segment_files.append(filename)\n",
        "\n",
        "        # Create combined file\n",
        "        combined_filename = f\"{output_path}/combined_composition.mid\"\n",
        "        combined_file = combine_segments(result['midi_files'], combined_filename)\n",
        "\n",
        "        return {\n",
        "            \"segment_files\": segment_files,\n",
        "            \"combined_file\": combined_file\n",
        "        }\n",
        "\n",
        "    def _export_json_composition(self, result, output_path):\n",
        "        \"\"\"Export all composition data to JSON format\"\"\"\n",
        "        # Ensure output directory exists\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        # Create exportable data structure\n",
        "        export_data = {\n",
        "            \"title\": \"Literary Music Composition\",\n",
        "            \"created_date\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"segments\": [],\n",
        "            \"emotion_progression\": {k: [float(v) for v in vals] for k, vals in result['emotion_progression'].items()},\n",
        "            \"emotional_narrative\": result['emotional_narrative']\n",
        "        }\n",
        "\n",
        "        # Add segment data\n",
        "        for i, prompt_data in enumerate(result['musiclm_prompts']):\n",
        "            segment_data = {\n",
        "                \"segment_number\": i + 1,\n",
        "                \"text\": result['segments'][i],\n",
        "                \"dominant_emotions\": prompt_data['dominant_emotions'],\n",
        "                \"musical_features\": {k: float(v) if isinstance(v, (int, float)) else v\n",
        "                                    for k, v in prompt_data['musical_features'].items()},\n",
        "                \"musiclm_prompt\": prompt_data['prompt']\n",
        "            }\n",
        "            export_data[\"segments\"].append(segment_data)\n",
        "\n",
        "        # Add transition data if available\n",
        "        if 'transition_prompts' in result and result['transition_prompts']:\n",
        "            export_data[\"transitions\"] = result['transition_prompts']\n",
        "\n",
        "        # Save to file\n",
        "        output_file = f\"{output_path}/composition_data.json\"\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(export_data, f, indent=2)\n",
        "\n",
        "        return {\"json_file\": output_file}\n",
        "\n",
        "    def _export_musicxml_composition(self, result, output_path):\n",
        "        \"\"\"Export composition in MusicXML format for notation software\"\"\"\n",
        "        # Note: This is a placeholder for the MusicXML export functionality\n",
        "        # In a real implementation, you would use a library like music21\n",
        "        # to create MusicXML files from the MIDI data\n",
        "\n",
        "        # Ensure output directory exists\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "        # Create a mock MusicXML file for demonstration\n",
        "        output_file = f\"{output_path}/composition.musicxml\"\n",
        "\n",
        "        with open(output_file, 'w') as f:\n",
        "            f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
        "            f.write('<!DOCTYPE score-partwise PUBLIC \"-//Recordare//DTD MusicXML 3.1 Partwise//EN\" \"http://www.musicxml.com/dtds/partwise.dtd\">\\n')\n",
        "            f.write('<score-partwise version=\"3.1\">\\n')\n",
        "            f.write('  <part-list>\\n')\n",
        "            f.write('    <score-part id=\"P1\">\\n')\n",
        "            f.write('      <part-name>Literary Music Composition</part-name>\\n')\n",
        "            f.write('    </score-part>\\n')\n",
        "            f.write('  </part-list>\\n')\n",
        "            f.write('  <part id=\"P1\">\\n')\n",
        "            f.write('    <!-- Placeholder for actual music notation -->\\n')\n",
        "            f.write('  </part>\\n')\n",
        "            f.write('</score-partwise>\\n')\n",
        "\n",
        "        return {\"musicxml_file\": output_file}\n",
        "\n",
        "    def analyze_user_feedback(self, user_feedback, result):\n",
        "        \"\"\"Analyze user feedback to improve future compositions\"\"\"\n",
        "        # Extract feedback metrics\n",
        "        emotion_accuracy = user_feedback.get('emotion_accuracy', 0)\n",
        "        musical_quality = user_feedback.get('musical_quality', 0)\n",
        "        narrative_coherence = user_feedback.get('narrative_coherence', 0)\n",
        "        overall_satisfaction = user_feedback.get('overall_satisfaction', 0)\n",
        "\n",
        "        # Calculate weighted score\n",
        "        weighted_score = (emotion_accuracy * 0.4 +\n",
        "                          musical_quality * 0.3 +\n",
        "                          narrative_coherence * 0.2 +\n",
        "                          overall_satisfaction * 0.1)\n",
        "\n",
        "        # Process specific comments\n",
        "        comments = user_feedback.get('comments', '')\n",
        "\n",
        "        # Generate suggestions for improvement\n",
        "        improvement_suggestions = self._generate_improvement_suggestions(\n",
        "            weighted_score, emotion_accuracy, musical_quality, narrative_coherence, comments)\n",
        "\n",
        "        # Create summary report\n",
        "        feedback_analysis = {\n",
        "            \"weighted_score\": weighted_score,\n",
        "            \"strengths\": [],\n",
        "            \"areas_for_improvement\": [],\n",
        "            \"suggested_adjustments\": improvement_suggestions\n",
        "        }\n",
        "\n",
        "        # Identify strengths\n",
        "        if emotion_accuracy >= 4:\n",
        "            feedback_analysis[\"strengths\"].append(\"Emotional mapping accuracy\")\n",
        "        if musical_quality >= 4:\n",
        "            feedback_analysis[\"strengths\"].append(\"Musical composition quality\")\n",
        "        if narrative_coherence >= 4:\n",
        "            feedback_analysis[\"strengths\"].append(\"Narrative coherence\")\n",
        "\n",
        "        # Identify weaknesses\n",
        "        if emotion_accuracy < 3:\n",
        "            feedback_analysis[\"areas_for_improvement\"].append(\"Emotional mapping accuracy\")\n",
        "        if musical_quality < 3:\n",
        "            feedback_analysis[\"areas_for_improvement\"].append(\"Musical composition quality\")\n",
        "        if narrative_coherence < 3:\n",
        "            feedback_analysis[\"areas_for_improvement\"].append(\"Narrative coherence\")\n",
        "\n",
        "        return feedback_analysis\n",
        "\n",
        "    def _generate_improvement_suggestions(self, weighted_score, emotion_accuracy,\n",
        "                                         musical_quality, narrative_coherence, comments):\n",
        "        \"\"\"Generate specific suggestions based on feedback\"\"\"\n",
        "        suggestions = []\n",
        "\n",
        "        # Low emotion accuracy suggestions\n",
        "        if emotion_accuracy < 3:\n",
        "            suggestions.append(\"Refine emotional analysis by adjusting the text segmentation algorithm\")\n",
        "            suggestions.append(\"Update emotion-to-music mapping with more nuanced correlations\")\n",
        "\n",
        "        # Low musical quality suggestions\n",
        "        if musical_quality < 3:\n",
        "            suggestions.append(\"Enhance musical complexity in composition algorithm\")\n",
        "            suggestions.append(\"Improve dynamic range and expressive elements\")\n",
        "\n",
        "        # Low narrative coherence suggestions\n",
        "        if narrative_coherence < 3:\n",
        "            suggestions.append(\"Strengthen transitions between segments\")\n",
        "            suggestions.append(\"Improve temporal coherence model parameters\")\n",
        "\n",
        "        # Check comments for specific keywords\n",
        "        if 'repetitive' in comments.lower():\n",
        "            suggestions.append(\"Increase variation in melodic and harmonic patterns\")\n",
        "\n",
        "        if 'disjointed' in comments.lower() or 'disconnected' in comments.lower():\n",
        "            suggestions.append(\"Apply smoother transition algorithms between emotional segments\")\n",
        "\n",
        "        if 'simple' in comments.lower() or 'basic' in comments.lower():\n",
        "            suggestions.append(\"Increase complexity parameters for harmony and rhythm\")\n",
        "\n",
        "        # Add general suggestions based on overall score\n",
        "        if weighted_score < 2:\n",
        "            suggestions.append(\"Consider fundamental revision of the emotion-to-music mapping algorithm\")\n",
        "        elif weighted_score < 3:\n",
        "            suggestions.append(\"Fine-tune parameters across multiple dimensions for better coherence\")\n",
        "        elif weighted_score < 4:\n",
        "            suggestions.append(\"Make minor adjustments to enhance specific areas of weakness\")\n",
        "\n",
        "        return suggestions"
      ],
      "metadata": {
        "id": "09Sef4lk106m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "ZeXo9kCy5AEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "noCy4-dvAR8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class MuLANTextEncoder(nn.Module):\n",
        "    \"\"\"Text encoder based on BERT with MuLAN-style additions for music understanding\"\"\"\n",
        "    def __init__(self, bert_model=\"bert-base-uncased\", embedding_dim=512):\n",
        "        super(MuLANTextEncoder, self).__init__()\n",
        "        # Load pre-trained BERT model\n",
        "        self.bert = BertModel.from_pretrained(bert_model)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
        "\n",
        "        # Project BERT outputs to our embedding space\n",
        "        self.projection = nn.Linear(self.bert.config.hidden_size, embedding_dim)\n",
        "\n",
        "        # Music-specific token embeddings (words related to music semantics)\n",
        "        self.music_token_embedding = nn.Embedding(1000, embedding_dim)\n",
        "        self.music_vocab = self._create_music_vocab()\n",
        "\n",
        "    def _create_music_vocab(self):\n",
        "        \"\"\"Create vocabulary mapping for music-specific terms\"\"\"\n",
        "        music_terms = [\n",
        "            \"tempo\", \"rhythm\", \"melody\", \"harmony\", \"bass\", \"treble\",\n",
        "            \"major\", \"minor\", \"piano\", \"guitar\", \"drums\", \"strings\",\n",
        "            \"loud\", \"soft\", \"fast\", \"slow\", \"staccato\", \"legato\",\n",
        "            # Emotions\n",
        "            \"happy\", \"sad\", \"angry\", \"fearful\", \"tender\", \"excited\",\n",
        "            # Instruments\n",
        "            \"violin\", \"cello\", \"flute\", \"trumpet\", \"saxophone\", \"harp\",\n",
        "            \"synthesizer\", \"electric guitar\", \"acoustic guitar\", \"bass guitar\",\n",
        "            # Musical styles\n",
        "            \"classical\", \"jazz\", \"rock\", \"electronic\", \"ambient\", \"folk\",\n",
        "            \"pop\", \"hip hop\", \"blues\", \"country\", \"metal\", \"funk\",\n",
        "            # Descriptive terms\n",
        "            \"bright\", \"dark\", \"warm\", \"cold\", \"mellow\", \"harsh\",\n",
        "            \"smooth\", \"rough\", \"ethereal\", \"gritty\", \"lush\", \"sparse\",\n",
        "            \"uplifting\", \"melancholic\", \"energetic\", \"calm\", \"tense\", \"relaxed\"\n",
        "        ]\n",
        "        return {term: i for i, term in enumerate(music_terms)}\n",
        "\n",
        "    def forward(self, text):\n",
        "        \"\"\"Encode text into music-aware embeddings\"\"\"\n",
        "        # Tokenize input\n",
        "        tokens = self.tokenizer(text, return_tensors=\"pt\",\n",
        "                               padding=True, truncation=True, max_length=512)\n",
        "\n",
        "        # Move to the same device as the model\n",
        "        tokens = {k: v.to(self.bert.device) for k, v in tokens.items()}\n",
        "\n",
        "        # Get BERT embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert(**tokens)\n",
        "\n",
        "        # Use the [CLS] token embedding as sequence representation\n",
        "        sequence_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Project to our embedding space\n",
        "        projected_embedding = self.projection(sequence_embedding)\n",
        "\n",
        "        # Extract and enhance music-specific terms\n",
        "        music_embedding = self._enhance_music_terms(text, projected_embedding)\n",
        "\n",
        "        return music_embedding\n",
        "\n",
        "    def _enhance_music_terms(self, text, embedding):\n",
        "        \"\"\"Enhance embeddings with music-specific token information\"\"\"\n",
        "        # Check if text is a list or a single string\n",
        "        if isinstance(text, str):\n",
        "            text = [text]  # Convert to list for consistent processing\n",
        "\n",
        "        batch_enhanced = []\n",
        "\n",
        "        for i, t in enumerate(text):\n",
        "            t_lower = t.lower()\n",
        "\n",
        "            # Initialize music embedding contribution\n",
        "            music_contrib = torch.zeros_like(embedding[i])\n",
        "            count = 0\n",
        "\n",
        "            # Look for music terms - use word boundaries to avoid partial matches\n",
        "            for term, idx in self.music_vocab.items():\n",
        "                # Check for word boundaries to find whole words\n",
        "                import re\n",
        "                matches = re.findall(r'\\b' + re.escape(term) + r'\\b', t_lower)\n",
        "                if matches:\n",
        "                    term_tensor = torch.tensor([idx], device=embedding.device)\n",
        "                    music_contrib += self.music_token_embedding(term_tensor).squeeze(0)\n",
        "                    count += 1\n",
        "\n",
        "            # Add weighted music embedding if any terms found\n",
        "            if count > 0:\n",
        "                enhanced = embedding[i] + (music_contrib / count) * 0.3  # 30% contribution\n",
        "            else:\n",
        "                enhanced = embedding[i]\n",
        "\n",
        "            batch_enhanced.append(enhanced)\n",
        "\n",
        "        return torch.stack(batch_enhanced)\n",
        "\n",
        "\n",
        "class SoundStreamEncoder(nn.Module):\n",
        "    \"\"\"Audio encoder inspired by SoundStream architecture\"\"\"\n",
        "    def __init__(self, input_channels=1, embedding_dim=512):\n",
        "        super(SoundStreamEncoder, self).__init__()\n",
        "\n",
        "        # Convolutional encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            # Initial convolution\n",
        "            nn.Conv1d(input_channels, 32, kernel_size=7, stride=1, padding=3),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Downsampling convolutions\n",
        "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),  # Downsample 2x\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),  # Downsample 2x\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),  # Downsample 2x\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),  # Downsample 2x\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Additional convolutions\n",
        "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Projection to embedding space\n",
        "        self.projection = nn.Linear(512, embedding_dim)\n",
        "\n",
        "        # Quantizer (VQ layer)\n",
        "        self.codebook_size = 1024\n",
        "        self.codebook = nn.Embedding(self.codebook_size, embedding_dim)\n",
        "\n",
        "        # Residual vector quantizer\n",
        "        self.num_quantizers = 8\n",
        "        self.codebook_dim = embedding_dim // self.num_quantizers\n",
        "        self.codebooks = nn.ModuleList([\n",
        "            nn.Embedding(self.codebook_size, self.codebook_dim)\n",
        "            for _ in range(self.num_quantizers)\n",
        "        ])\n",
        "\n",
        "    def vector_quantize(self, x, codebook):\n",
        "        \"\"\"\n",
        "        Vector quantization implementation with improved efficiency and numerical stability\n",
        "        x: (batch_size, codebook_dim)\n",
        "        \"\"\"\n",
        "        # Calculate squared L2 norm - more stable implementation\n",
        "        x_norm = torch.sum(x**2, dim=1, keepdim=True)\n",
        "        codebook_norm = torch.sum(codebook.weight**2, dim=1)\n",
        "\n",
        "        # Calculate dot product\n",
        "        dot_product = torch.matmul(x, codebook.weight.t())\n",
        "\n",
        "        # Calculate distance using the expanded form of Euclidean distance\n",
        "        d = x_norm + codebook_norm - 2 * dot_product\n",
        "\n",
        "        # Prevent numerical instability\n",
        "        d = torch.clamp(d, min=1e-5)\n",
        "\n",
        "        # Find nearest neighbor\n",
        "        indices = torch.argmin(d, dim=1)\n",
        "        quantized = codebook(indices)\n",
        "\n",
        "        # Straight-through estimator\n",
        "        quantized_st = x + (quantized - x).detach()\n",
        "\n",
        "        # Calculate VQ loss components for training\n",
        "        commitment_loss = F.mse_loss(x, quantized.detach())\n",
        "        codebook_loss = F.mse_loss(quantized, x.detach())\n",
        "\n",
        "        return quantized_st, indices, commitment_loss + codebook_loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Encode audio into latent representations\n",
        "        x shape: [batch_size, 1, time]\n",
        "        \"\"\"\n",
        "        # Apply convolutional encoder\n",
        "        encoded = self.encoder(x)  # [batch_size, 512, time/16]\n",
        "\n",
        "        # Global pooling to get fixed-size representation\n",
        "        pooled = F.adaptive_avg_pool1d(encoded, 1).squeeze(-1)  # [batch_size, 512]\n",
        "\n",
        "        # Project to embedding space\n",
        "        embedding = self.projection(pooled)  # [batch_size, embedding_dim]\n",
        "\n",
        "        # Reshape for residual VQ\n",
        "        batch_size = embedding.shape[0]\n",
        "        reshaped = embedding.view(batch_size, self.num_quantizers, self.codebook_dim)\n",
        "\n",
        "        # Apply residual vector quantization\n",
        "        quantized_list = []\n",
        "        indices_list = []\n",
        "\n",
        "        for i in range(self.num_quantizers):\n",
        "            q, idx = self.vector_quantize(reshaped[:, i], self.codebooks[i])\n",
        "            quantized_list.append(q)\n",
        "            indices_list.append(idx)\n",
        "\n",
        "        # Reshape back\n",
        "        quantized = torch.cat(quantized_list, dim=1).view(batch_size, -1)\n",
        "\n",
        "        # Combine with encoded for temporal information\n",
        "        return quantized, encoded\n",
        "\n",
        "\n",
        "class SoundStreamDecoder(nn.Module):\n",
        "    \"\"\"Audio decoder inspired by SoundStream architecture\"\"\"\n",
        "    def __init__(self, embedding_dim=512, output_channels=1):\n",
        "        super(SoundStreamDecoder, self).__init__()\n",
        "\n",
        "        # Project embedding to the right dimension for the decoder\n",
        "        self.pre_decoder = nn.Linear(embedding_dim, 512)\n",
        "\n",
        "        # Convolutional decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            # Initial convolution\n",
        "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Upsampling convolutions using transposed convolutions\n",
        "            nn.ConvTranspose1d(512, 256, kernel_size=4, stride=2, padding=1),  # Upsample 2x\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(256, 128, kernel_size=4, stride=2, padding=1),  # Upsample 2x\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),   # Upsample 2x\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),    # Upsample 2x\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final convolution to get to the right number of channels\n",
        "            nn.Conv1d(32, output_channels, kernel_size=7, stride=1, padding=3),\n",
        "            nn.Tanh()  # Output in [-1, 1] range for audio\n",
        "        )\n",
        "\n",
        "    def forward(self, z, encoded=None, length=16000):\n",
        "        \"\"\"\n",
        "        Decode latent representation to audio\n",
        "        z shape: [batch_size, embedding_dim]\n",
        "        encoded shape (optional): [batch_size, 512, time/16]\n",
        "        \"\"\"\n",
        "        # Project to the right dimension\n",
        "        z_proj = self.pre_decoder(z)  # [batch_size, 512]\n",
        "\n",
        "        if encoded is not None:\n",
        "            # Use the temporal information from the encoder\n",
        "            z_temporal = z_proj.unsqueeze(-1) * F.adaptive_avg_pool1d(encoded, encoded.size(-1))\n",
        "        else:\n",
        "            # Create a temporal dimension\n",
        "            time_steps = length // 16  # Depends on the encoder downsampling\n",
        "            z_temporal = z_proj.unsqueeze(-1).repeat(1, 1, time_steps)\n",
        "\n",
        "        # Apply convolutional decoder\n",
        "        decoded = self.decoder(z_temporal)  # [batch_size, output_channels, time]\n",
        "\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"Self-attention block for the UNet architecture\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.channels = channels\n",
        "\n",
        "        self.norm = nn.GroupNorm(num_groups=8, num_channels=channels)\n",
        "        self.qkv_proj = nn.Conv1d(channels, channels * 3, kernel_size=1)\n",
        "        self.out_proj = nn.Conv1d(channels, channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, channels, length = x.shape\n",
        "        assert channels == self.channels\n",
        "\n",
        "        # Apply normalization\n",
        "        h = self.norm(x)\n",
        "\n",
        "        # Compute query, key, value\n",
        "        qkv = self.qkv_proj(h)\n",
        "        qkv = qkv.reshape(batch, 3, channels, length)\n",
        "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
        "\n",
        "        # Compute attention\n",
        "        scale = 1.0 / (channels ** 0.5)\n",
        "        attention = torch.einsum('bct,bcs->bts', q, k) * scale\n",
        "        attention = F.softmax(attention, dim=2)\n",
        "\n",
        "        # Apply attention\n",
        "        output = torch.einsum('bts,bcs->bct', attention, v)\n",
        "\n",
        "        # Project back to original dimension\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        # Residual connection\n",
        "        return x + output\n",
        "\n",
        "\n",
        "class TextConditionedResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block with text conditioning for UNet architecture\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, text_channels, dropout=0.1):\n",
        "        super(TextConditionedResidualBlock, self).__init__()\n",
        "\n",
        "        self.norm1 = nn.GroupNorm(num_groups=8, num_channels=in_channels)\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        self.norm2 = nn.GroupNorm(num_groups=8, num_channels=out_channels)\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # Text conditioning\n",
        "        self.text_proj = nn.Linear(text_channels, out_channels)\n",
        "\n",
        "        # Shortcut connection\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, text_embed):\n",
        "        \"\"\"\n",
        "        Forward pass with text conditioning\n",
        "        x: [batch_size, in_channels, time]\n",
        "        text_embed: [batch_size, text_channels]\n",
        "        \"\"\"\n",
        "        # First convolution block\n",
        "        h = self.norm1(x)\n",
        "        h = F.silu(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        # Apply text conditioning\n",
        "        text_embed = self.text_proj(text_embed).unsqueeze(-1)\n",
        "        text_embed = text_embed.expand(-1, -1, h.size(-1))\n",
        "        h = h + text_embed\n",
        "\n",
        "        # Second convolution block\n",
        "        h = self.norm2(h)\n",
        "        h = F.silu(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        # Skip connection\n",
        "        return h + self.shortcut(x)\n",
        "\n",
        "\n",
        "class MusicConditionedUNet(nn.Module):\n",
        "    \"\"\"UNet-style model for high-resolution audio generation conditioned on text\"\"\"\n",
        "    def __init__(self, input_channels=1, output_channels=1, base_channels=32, embedding_dim=512):\n",
        "        super(MusicConditionedUNet, self).__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.initial_conv = nn.Conv1d(input_channels, base_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # Encoder (downsampling) blocks\n",
        "        self.down1 = self._make_down_block(base_channels, base_channels*2, embedding_dim)\n",
        "        self.down2 = self._make_down_block(base_channels*2, base_channels*4, embedding_dim)\n",
        "        self.down3 = self._make_down_block(base_channels*4, base_channels*8, embedding_dim)\n",
        "        self.down4 = self._make_down_block(base_channels*8, base_channels*8, embedding_dim)\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mid_resblock1 = TextConditionedResidualBlock(base_channels*8, base_channels*8, embedding_dim)\n",
        "        self.mid_attn = AttentionBlock(base_channels*8)\n",
        "        self.mid_resblock2 = TextConditionedResidualBlock(base_channels*8, base_channels*8, embedding_dim)\n",
        "\n",
        "        # Decoder (upsampling) blocks\n",
        "        self.up4 = self._make_up_block(base_channels*16, base_channels*4, embedding_dim)\n",
        "        self.up3 = self._make_up_block(base_channels*8, base_channels*2, embedding_dim)\n",
        "        self.up2 = self._make_up_block(base_channels*4, base_channels, embedding_dim)\n",
        "        self.up1 = self._make_up_block(base_channels*2, base_channels, embedding_dim)\n",
        "\n",
        "        # Final convolution\n",
        "        self.final_norm = nn.GroupNorm(num_groups=8, num_channels=base_channels)\n",
        "        self.final_conv = nn.Conv1d(base_channels, output_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def _make_down_block(self, in_channels, out_channels, text_channels):\n",
        "        return nn.ModuleList([\n",
        "            TextConditionedResidualBlock(in_channels, in_channels, text_channels),\n",
        "            TextConditionedResidualBlock(in_channels, out_channels, text_channels),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=4, stride=2, padding=1)  # Downsample\n",
        "        ])\n",
        "\n",
        "    def _make_up_block(self, in_channels, out_channels, text_channels):\n",
        "        return nn.ModuleList([\n",
        "            TextConditionedResidualBlock(in_channels, in_channels, text_channels),\n",
        "            TextConditionedResidualBlock(in_channels, out_channels, text_channels),\n",
        "            nn.ConvTranspose1d(out_channels, out_channels, kernel_size=4, stride=2, padding=1)  # Upsample\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, text_embedding):\n",
        "        \"\"\"\n",
        "        Forward pass with text conditioning\n",
        "        x: [batch_size, 1, time] - Initial audio or noise\n",
        "        text_embedding: [batch_size, embedding_dim] - Text embeddings\n",
        "        \"\"\"\n",
        "        # Initial convolution\n",
        "        h = self.initial_conv(x)\n",
        "\n",
        "        # Downsampling path with skip connections\n",
        "        skips = []\n",
        "\n",
        "        # Down 1\n",
        "        h = self.down1[0](h, text_embedding)\n",
        "        h = self.down1[1](h, text_embedding)\n",
        "        skips.append(h)\n",
        "        h = self.down1[2](h)\n",
        "\n",
        "        # Down 2\n",
        "        h = self.down2[0](h, text_embedding)\n",
        "        h = self.down2[1](h, text_embedding)\n",
        "        skips.append(h)\n",
        "        h = self.down2[2](h)\n",
        "\n",
        "        # Down 3\n",
        "        h = self.down3[0](h, text_embedding)\n",
        "        h = self.down3[1](h, text_embedding)\n",
        "        skips.append(h)\n",
        "        h = self.down3[2](h)\n",
        "\n",
        "        # Down 4\n",
        "        h = self.down4[0](h, text_embedding)\n",
        "        h = self.down4[1](h, text_embedding)\n",
        "        skips.append(h)\n",
        "        h = self.down4[2](h)\n",
        "\n",
        "        # Middle\n",
        "        h = self.mid_resblock1(h, text_embedding)\n",
        "        h = self.mid_attn(h)\n",
        "        h = self.mid_resblock2(h, text_embedding)\n",
        "\n",
        "        # Upsampling path with skip connections\n",
        "        # Up 4 (connecting with Down 4 skip)\n",
        "        h = torch.cat([h, skips.pop()], dim=1)\n",
        "        h = self.up4[0](h, text_embedding)\n",
        "        h = self.up4[1](h, text_embedding)\n",
        "        h = self.up4[2](h)\n",
        "\n",
        "        # Up 3 (connecting with Down 3 skip)\n",
        "        h = torch.cat([h, skips.pop()], dim=1)\n",
        "        h = self.up3[0](h, text_embedding)\n",
        "        h = self.up3[1](h, text_embedding)\n",
        "        h = self.up3[2](h)\n",
        "\n",
        "        # Up 2 (connecting with Down 2 skip)\n",
        "        h = torch.cat([h, skips.pop()], dim=1)\n",
        "        h = self.up2[0](h, text_embedding)\n",
        "        h = self.up2[1](h, text_embedding)\n",
        "        h = self.up2[2](h)\n",
        "\n",
        "        # Up 1 (connecting with Down 1 skip)\n",
        "        h = torch.cat([h, skips.pop()], dim=1)\n",
        "        h = self.up1[0](h, text_embedding)\n",
        "        h = self.up1[1](h, text_embedding)\n",
        "        h = self.up1[2](h)\n",
        "\n",
        "        # Final layers\n",
        "        h = self.final_norm(h)\n",
        "        h = F.silu(h)\n",
        "        h = self.final_conv(h)\n",
        "\n",
        "        return h\n",
        "\n",
        "class TextToMusicGenerationModel(nn.Module):\n",
        "    \"\"\"Full text-to-music generation model combining all components\"\"\"\n",
        "    def __init__(self, embedding_dim=512):\n",
        "        super(TextToMusicGenerationModel, self).__init__()\n",
        "\n",
        "        # Text encoder\n",
        "        self.text_encoder = MuLANTextEncoder(embedding_dim=embedding_dim)\n",
        "\n",
        "        # Audio encoder for conditioning\n",
        "        self.audio_encoder = SoundStreamEncoder(embedding_dim=embedding_dim)\n",
        "\n",
        "        # Audio decoder for reconstruction\n",
        "        self.audio_decoder = SoundStreamDecoder(embedding_dim=embedding_dim)\n",
        "\n",
        "        # UNet for high-resolution generation\n",
        "        self.unet = MusicConditionedUNet(embedding_dim=embedding_dim)\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        \"\"\"Encode text description\"\"\"\n",
        "        return self.text_encoder(text)\n",
        "\n",
        "    def encode_audio(self, audio):\n",
        "        \"\"\"Encode audio\"\"\"\n",
        "        return self.audio_encoder(audio)\n",
        "\n",
        "    def decode_audio(self, z, encoded=None, length=16000):\n",
        "        \"\"\"Decode latent representation to audio\"\"\"\n",
        "        return self.audio_decoder(z, encoded, length)\n",
        "\n",
        "    # Add this method to your CustomMusicLM class\n",
        "    def generate_from_text(self, text, noise_level=0.5, steps=50, length=16000):\n",
        "        \"\"\"Generate audio from text using iterative refinement\"\"\"\n",
        "        print(f\"Starting generation for text: '{text}'\")\n",
        "\n",
        "        # Encode text\n",
        "        text_embedding = self.text_encoder(text)\n",
        "        print(f\"Text encoded, embedding shape: {text_embedding.shape}\")\n",
        "\n",
        "        # Initialize with noise\n",
        "        device = next(self.parameters()).device\n",
        "        audio = torch.randn(1, 1, length).to(device) * noise_level\n",
        "        print(f\"Initial noise created with shape: {audio.shape}\")\n",
        "\n",
        "        # Iterative refinement\n",
        "        from tqdm.notebook import tqdm\n",
        "        for i in tqdm(range(steps)):\n",
        "            # Progressively decrease noise influence\n",
        "            t = 1.0 - (i / steps)\n",
        "            noise_scale = noise_level * t\n",
        "\n",
        "            # Generate an update using the UNet\n",
        "            update = self.unet(audio, text_embedding)\n",
        "\n",
        "            # Apply the update with noise scheduling\n",
        "            audio = audio * (1.0 - noise_scale) + update * noise_scale\n",
        "\n",
        "            # Optional: apply constraints to keep audio in range\n",
        "            audio = torch.clamp(audio, -1.0, 1.0)\n",
        "\n",
        "            # Print progress occasionally\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Step {i}/{steps}, audio range: {audio.min().item():.3f} to {audio.max().item():.3f}\")\n",
        "\n",
        "        print(\"Generation complete!\")\n",
        "        return audio\n",
        "\n",
        "    def forward(self, audio_input=None, text_input=None, mode='train'):\n",
        "        \"\"\"\n",
        "        Forward pass through the model\n",
        "        Modes:\n",
        "        - 'train': Train the full model (reconstruction + generation)\n",
        "        - 'encode_text': Only encode text\n",
        "        - 'encode_audio': Only encode audio\n",
        "        - 'generate': Generate audio from text\n",
        "        \"\"\"\n",
        "        if mode == 'encode_text' and text_input is not None:\n",
        "            return self.encode_text(text_input)\n",
        "\n",
        "        elif mode == 'encode_audio' and audio_input is not None:\n",
        "            return self.encode_audio(audio_input)\n",
        "\n",
        "        elif mode == 'generate' and text_input is not None:\n",
        "            # Use default parameters for generation\n",
        "            return self.generate_from_text(text_input)\n",
        "\n",
        "        elif mode == 'train':\n",
        "            if audio_input is None or text_input is None:\n",
        "                raise ValueError(\"Both audio_input and text_input are required for training\")\n",
        "\n",
        "            # Encode text\n",
        "            text_embedding = self.encode_text(text_input)\n",
        "\n",
        "            # Encode audio\n",
        "            quantized, encoded = self.encode_audio(audio_input)\n",
        "\n",
        "            # Decode audio (reconstruction path)\n",
        "            reconstructed = self.decode_audio(quantized, encoded, audio_input.shape[-1])\n",
        "\n",
        "            # UNet for high-resolution details (enhancement path)\n",
        "            enhanced = self.unet(reconstructed, text_embedding)\n",
        "\n",
        "            return {\n",
        "                \"reconstructed\": reconstructed,\n",
        "                \"enhanced\": enhanced,\n",
        "                \"quantized\": quantized,\n",
        "                \"encoded\": encoded,\n",
        "                \"text_embedding\": text_embedding\n",
        "            }\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode: {mode}\")\n",
        "\n",
        "# 5. Define training functions for the model\n",
        "def train_step(model, audio_batch, text_batch, optimizer, device, lambda_rec=1.0, lambda_enhance=0.5):\n",
        "    \"\"\"Single training step for the text-to-music model\"\"\"\n",
        "    # Move data to device\n",
        "    audio_batch = audio_batch.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(audio_batch, text_batch, mode='train')\n",
        "\n",
        "    # Calculate losses\n",
        "    # Reconstruction loss - compare original audio with reconstructed audio\n",
        "    rec_loss = F.mse_loss(outputs[\"reconstructed\"], audio_batch)\n",
        "\n",
        "    # Enhancement loss - compare enhanced audio with original audio\n",
        "    enhance_loss = F.mse_loss(outputs[\"enhanced\"], audio_batch)\n",
        "\n",
        "    # Total loss\n",
        "    loss = lambda_rec * rec_loss + lambda_enhance * enhance_loss\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient clipping for stability\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    return {\n",
        "        \"loss\": loss.item(),\n",
        "        \"rec_loss\": rec_loss.item(),\n",
        "        \"enhance_loss\": enhance_loss.item()\n",
        "    }\n",
        "\n",
        "# 6. Example usage of the model\n",
        "def example_usage():\n",
        "    # Initialize model\n",
        "    model = TextToMusicGenerationModel(embedding_dim=512)\n",
        "    model.to(device)\n",
        "\n",
        "    # Set up optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Example training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for audio_batch, text_batch in dataloader:\n",
        "            metrics = train_step(model, audio_batch, text_batch, optimizer, device)\n",
        "            print(f\"Epoch {epoch}, Loss: {metrics['loss']:.4f}\")\n",
        "\n",
        "    # Example generation\n",
        "    text_prompt = \"A calm classical piano piece with a melancholic melody\"\n",
        "    generated_audio = model.generate_from_text(text_prompt, steps=100)\n",
        "\n",
        "    # Save the generated audio (pseudo-code)\n",
        "    save_audio(generated_audio.cpu().numpy(), \"generated_music.wav\")\n",
        "\n",
        "# Add this at the top of your notebook\n",
        "import IPython.display as ipd\n",
        "from scipy.io import wavfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After generating your audio\n",
        "def display_audio_in_colab(audio_tensor, sample_rate=16000, filename=\"generated_audio.wav\"):\n",
        "    # Convert to numpy and scale appropriately\n",
        "    audio_np = audio_tensor.squeeze().detach().cpu().numpy()\n",
        "    audio_np = np.clip(audio_np, -1.0, 1.0)\n",
        "\n",
        "    # Display waveform\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(audio_np)\n",
        "    plt.title(\"Generated Audio Waveform\")\n",
        "    plt.xlabel(\"Time (samples)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Scale for 16-bit WAV\n",
        "    audio_int16 = (audio_np * 32767).astype(np.int16)\n",
        "\n",
        "    # Save as WAV file\n",
        "    wavfile.write(filename, sample_rate, audio_int16)\n",
        "\n",
        "    # Display playable audio in the notebook\n",
        "    print(\"Generated audio playback:\")\n",
        "    return ipd.Audio(audio_np, rate=sample_rate)\n",
        "\n",
        "# Example usage:\n",
        "# Define necessary variables first\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TextToMusicGenerationModel(embedding_dim=512)\n",
        "model.to(device)\n",
        "\n",
        "# Try generating audio\n",
        "text_prompt = \"A calm piano melody with soft strings\"\n",
        "generated_audio = model.generate_from_text(text_prompt)\n",
        "display_audio_in_colab(generated_audio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440,
          "referenced_widgets": [
            "2c656350e7c84905ba4d841fa3e5648e",
            "d1408ab89f3d4bb289fd8745abb31ec8",
            "875658072f9640c4b689939c9f533ef6",
            "20a01cb93a9440e1aa6637d75068e7b8",
            "d1c3cea2a9fa4a749c59e8f0c200fd4e",
            "87f085e9e07c4b4f845f5c55e231dc9d",
            "9693dd4cce4e4b80b3907453c88b1d6e",
            "4af94ef844c14aefabedca57cfe00758",
            "474174ad24e94e8c8e0a6e202c791b1d",
            "8223c875558649a68ab7d15521cd7006",
            "fba45276b966462da1ef88acf3e79f82"
          ]
        },
        "id": "ah37My0ZASw6",
        "outputId": "fe575f22-c2a0-46dd-f146-20844bb1330c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting generation for text: 'A calm piano melody with soft strings'\n",
            "Text encoded, embedding shape: torch.Size([1, 512])\n",
            "Initial noise created with shape: torch.Size([1, 1, 16000])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c656350e7c84905ba4d841fa3e5648e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1000 but got size 2000 for tensor number 1 in the list.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-808d23771199>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;31m# Try generating audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0mtext_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"A calm piano melody with soft strings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m \u001b[0mgenerated_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0mdisplay_audio_in_colab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_audio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-808d23771199>\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text, noise_level, steps, length)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;31m# Generate an update using the UNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;31m# Apply the update with noise scheduling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-808d23771199>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, text_embedding)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;31m# Upsampling path with skip connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;31m# Up 4 (connecting with Down 4 skip)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1000 but got size 2000 for tensor number 1 in the list."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k4PTLLhXAzlR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}