{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMruUNgrg1kZS6tgytik7QQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f210aeac9f6549f88d6a3b28f8cbc493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea3e288a59c94c54b5d3ca25b8012d45",
              "IPY_MODEL_8e035e331e7f4c278a335c91d47f5ff6",
              "IPY_MODEL_b1217825f4c143c69a4aa52a430327d1"
            ],
            "layout": "IPY_MODEL_f83f18a50be94734b9340dc3405d7920"
          }
        },
        "ea3e288a59c94c54b5d3ca25b8012d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31d325b4185246f3a2b1ee6d8b93a585",
            "placeholder": "​",
            "style": "IPY_MODEL_78f87d717ca44b49b913014b25bbb9ea",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8e035e331e7f4c278a335c91d47f5ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6975867fd1494d88a256c2df8f66da52",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03883f5917dd4b198ad6dc94218f1fcb",
            "value": 25
          }
        },
        "b1217825f4c143c69a4aa52a430327d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22831ca7fb32446e9febf7df17acdbb1",
            "placeholder": "​",
            "style": "IPY_MODEL_857f2ad115f245cdb82bb0f4d0cc43c3",
            "value": " 25.0/25.0 [00:00&lt;00:00, 346B/s]"
          }
        },
        "f83f18a50be94734b9340dc3405d7920": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d325b4185246f3a2b1ee6d8b93a585": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78f87d717ca44b49b913014b25bbb9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6975867fd1494d88a256c2df8f66da52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03883f5917dd4b198ad6dc94218f1fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22831ca7fb32446e9febf7df17acdbb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "857f2ad115f245cdb82bb0f4d0cc43c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "359e7caf220543b69baf5da8a66db05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aabbba3c1cb142f0a2a553f79ff98b90",
              "IPY_MODEL_c8f92721a5154b5ba3b05a265acde526",
              "IPY_MODEL_545ae924017148a6ae71c0cb87cba88d"
            ],
            "layout": "IPY_MODEL_39b81dad0f1e4a2ea10671b0bfee880d"
          }
        },
        "aabbba3c1cb142f0a2a553f79ff98b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf52fca7d4e44ba0b1305ce006b683ed",
            "placeholder": "​",
            "style": "IPY_MODEL_82e3e69418cb49c980001e0890517f5b",
            "value": "vocab.json: 100%"
          }
        },
        "c8f92721a5154b5ba3b05a265acde526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be5e390a73cd46849b2187eb943e2cef",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac0b6944d72f4ea6a3f70d9c86fce0d4",
            "value": 898823
          }
        },
        "545ae924017148a6ae71c0cb87cba88d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfa9f238454c44879c1c3b6fdb0207dd",
            "placeholder": "​",
            "style": "IPY_MODEL_9bc3539382094750b5c5c59bbcb4cd31",
            "value": " 899k/899k [00:00&lt;00:00, 4.17MB/s]"
          }
        },
        "39b81dad0f1e4a2ea10671b0bfee880d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf52fca7d4e44ba0b1305ce006b683ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82e3e69418cb49c980001e0890517f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be5e390a73cd46849b2187eb943e2cef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac0b6944d72f4ea6a3f70d9c86fce0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dfa9f238454c44879c1c3b6fdb0207dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bc3539382094750b5c5c59bbcb4cd31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56b45069c0d0432ab7171f474a89eaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ae9f54519484e4f950e79650e38fb98",
              "IPY_MODEL_a291de3d111c40ecb80c7e285e18b4e0",
              "IPY_MODEL_ca1372524e714a07ab86b198b68eb636"
            ],
            "layout": "IPY_MODEL_ad466016c92f46249c616d6222564e78"
          }
        },
        "2ae9f54519484e4f950e79650e38fb98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ced10c4cf1674469a6a5975df6775774",
            "placeholder": "​",
            "style": "IPY_MODEL_d4deb2a378e54c01a09f1a4bd3a4d8ba",
            "value": "merges.txt: 100%"
          }
        },
        "a291de3d111c40ecb80c7e285e18b4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fa54f053f184447b240e5bd58a0b79e",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d3ceb282a444b249ad266d118fbd513",
            "value": 456318
          }
        },
        "ca1372524e714a07ab86b198b68eb636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9e849bc7c574493bc9ae5b825151686",
            "placeholder": "​",
            "style": "IPY_MODEL_fcc2e212e0c8449cb116622850ee2bd5",
            "value": " 456k/456k [00:00&lt;00:00, 7.49MB/s]"
          }
        },
        "ad466016c92f46249c616d6222564e78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced10c4cf1674469a6a5975df6775774": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4deb2a378e54c01a09f1a4bd3a4d8ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fa54f053f184447b240e5bd58a0b79e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d3ceb282a444b249ad266d118fbd513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9e849bc7c574493bc9ae5b825151686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc2e212e0c8449cb116622850ee2bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a68348758c44c65b74d798935790b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4e3da8c8d9e4846ab906c8b5232b290",
              "IPY_MODEL_5a9228e0aa4a43aaa6d937ac17e4a3ac",
              "IPY_MODEL_3aad45ab18d54bf1b249abd278c150f4"
            ],
            "layout": "IPY_MODEL_41134b9b17cd4734921cf5a230d7ed76"
          }
        },
        "a4e3da8c8d9e4846ab906c8b5232b290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0272f2541de64e6f9d8c2a16c8437807",
            "placeholder": "​",
            "style": "IPY_MODEL_ac855e2baf3640cca9cb1d5a5f1164a6",
            "value": "tokenizer.json: 100%"
          }
        },
        "5a9228e0aa4a43aaa6d937ac17e4a3ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5812c47ce2784515a567cbbb55b64379",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72d851d5c10b44a88b3ac86b231f4618",
            "value": 1355863
          }
        },
        "3aad45ab18d54bf1b249abd278c150f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25554aba16394cf3b17666ccab55731a",
            "placeholder": "​",
            "style": "IPY_MODEL_f179b2c0c4fc4aa3af0ff1dff4eaad3a",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 4.89MB/s]"
          }
        },
        "41134b9b17cd4734921cf5a230d7ed76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0272f2541de64e6f9d8c2a16c8437807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac855e2baf3640cca9cb1d5a5f1164a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5812c47ce2784515a567cbbb55b64379": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d851d5c10b44a88b3ac86b231f4618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25554aba16394cf3b17666ccab55731a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f179b2c0c4fc4aa3af0ff1dff4eaad3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "679349b2a5c6426297f3ddf9e05ee6bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b76eb146a27c4794ad8139ac8e121daa",
              "IPY_MODEL_12fd4b50740047f298fd1a867e1c0a3b",
              "IPY_MODEL_db371d010405414ebf44f3836be62727"
            ],
            "layout": "IPY_MODEL_5e33d2928a944831bd6bf2b0c1c7e346"
          }
        },
        "b76eb146a27c4794ad8139ac8e121daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b0e59134b24579a0d268a4996b4311",
            "placeholder": "​",
            "style": "IPY_MODEL_e702dc0e23064c7e8dbd853ee99d8e71",
            "value": "config.json: 100%"
          }
        },
        "12fd4b50740047f298fd1a867e1c0a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73f5315b39c54c42a69d328ec34d5009",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_980dd7d976494c0ea0a77b6fb2bb7df8",
            "value": 481
          }
        },
        "db371d010405414ebf44f3836be62727": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba4d3177243c49fd9f26e1d951fe3b32",
            "placeholder": "​",
            "style": "IPY_MODEL_7648d12bd24543f5b2d788ffc0c66b71",
            "value": " 481/481 [00:00&lt;00:00, 4.99kB/s]"
          }
        },
        "5e33d2928a944831bd6bf2b0c1c7e346": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49b0e59134b24579a0d268a4996b4311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e702dc0e23064c7e8dbd853ee99d8e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73f5315b39c54c42a69d328ec34d5009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "980dd7d976494c0ea0a77b6fb2bb7df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba4d3177243c49fd9f26e1d951fe3b32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7648d12bd24543f5b2d788ffc0c66b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a957370d9e954bde98b35d7ad8486376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_793b4fd673a24d54940a3eb875a650aa",
              "IPY_MODEL_c94eef18d3db41b9bbefb510eae32880",
              "IPY_MODEL_412b7fa07bc34105b26511555eb0a1df"
            ],
            "layout": "IPY_MODEL_e43387fab39647709e59fa4534b52d6a"
          }
        },
        "793b4fd673a24d54940a3eb875a650aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4b7ba940ed74c86a557a4e23d0c446f",
            "placeholder": "​",
            "style": "IPY_MODEL_6c463fdce3db45e5bf4245bdb5351394",
            "value": "model.safetensors: 100%"
          }
        },
        "c94eef18d3db41b9bbefb510eae32880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7478dbc7993c420e8ade013a243892c6",
            "max": 498818054,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2525f69b4b594521a9d8e7e973998a30",
            "value": 498818054
          }
        },
        "412b7fa07bc34105b26511555eb0a1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fa29fb89ac3457da5854262e2eb3c34",
            "placeholder": "​",
            "style": "IPY_MODEL_d8a10e8ef6024a59bc9d5b9bda6eb605",
            "value": " 499M/499M [00:05&lt;00:00, 165MB/s]"
          }
        },
        "e43387fab39647709e59fa4534b52d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4b7ba940ed74c86a557a4e23d0c446f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c463fdce3db45e5bf4245bdb5351394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7478dbc7993c420e8ade013a243892c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2525f69b4b594521a9d8e7e973998a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fa29fb89ac3457da5854262e2eb3c34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8a10e8ef6024a59bc9d5b9bda6eb605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hjangir080/EmotionAwareMusicGeneration/blob/main/v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 2"
      ],
      "metadata": {
        "id": "uPxz-aFZL0Cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies and Imports\n"
      ],
      "metadata": {
        "id": "SYHAYS6ZG3In"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section installs and imports all the required libraries for the text-to-music pipeline, including PyTorch for deep learning, Hugging Face Transformers for language modeling, and numpy for numerical operations."
      ],
      "metadata": {
        "id": "a6y5xMLRG7nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers librosa"
      ],
      "metadata": {
        "id": "HA0klj4sG7E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "51ZmgOqEHA1j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WUfzypCCHCnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Emotion Extraction Module"
      ],
      "metadata": {
        "id": "Kmr1UEhpHMLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class preprocesses text, segments it if necessary, and uses a RoBERTa-based classifier to extract emotion scores for each segment. It supports both overall emotion extraction and emotional arc analysis across the narrative."
      ],
      "metadata": {
        "id": "gf8CytVXHgj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionExtractionModule(nn.Module):\n",
        "    \"\"\"Module for extracting emotional information from literary text\"\"\"\n",
        "    def __init__(self, model_name=\"roberta-base\", num_emotions=8, device=None):\n",
        "        super(EmotionExtractionModule, self).__init__()\n",
        "\n",
        "        # Load pre-trained language model for emotion analysis\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "        self.model = RobertaModel.from_pretrained(model_name)\n",
        "\n",
        "        # Emotion classifier head\n",
        "        self.emotion_classifier = nn.Sequential(\n",
        "            nn.Linear(self.model.config.hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_emotions),\n",
        "            nn.Sigmoid()  # Multiple emotions can be present simultaneously\n",
        "        )\n",
        "\n",
        "        # Define the emotion categories\n",
        "        self.emotion_categories = [\n",
        "            \"joy\", \"sadness\", \"anger\", \"fear\",\n",
        "            \"tenderness\", \"excitement\", \"calmness\", \"tension\"\n",
        "        ]\n",
        "\n",
        "        # Sliding window parameters for analyzing longer texts\n",
        "        self.window_size = 512  # Max tokens per window\n",
        "        self.stride = 256  # Overlap between windows\n",
        "\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def forward(self, text, return_attention=False):\n",
        "        \"\"\"\n",
        "        Extract emotion scores from text\n",
        "        Returns emotion scores and attention weights for visualization\n",
        "        \"\"\"\n",
        "        # For longer texts, break into overlapping chunks\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "\n",
        "        all_emotion_scores = []\n",
        "        all_attention_weights = []\n",
        "\n",
        "        for t in text:\n",
        "            # Break long text into chunks with sliding window\n",
        "            chunks = self._create_text_chunks(t)\n",
        "            chunk_emotions = []\n",
        "            chunk_attentions = []\n",
        "\n",
        "            for chunk in chunks:\n",
        "                # Tokenize with attention mask\n",
        "                inputs = self.tokenizer(chunk, return_tensors=\"pt\",\n",
        "                                      padding=True, truncation=True, max_length=self.window_size)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}  # Fixed to self.device\n",
        "\n",
        "                # Get model outputs\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(**inputs, output_attentions=return_attention)  # Only get attention if needed\n",
        "\n",
        "                # Use [CLS] token representation for classification\n",
        "                sequence_output = outputs.last_hidden_state[:, 0, :]\n",
        "                emotions = self.emotion_classifier(sequence_output)\n",
        "\n",
        "                # Store results\n",
        "                chunk_emotions.append(emotions)\n",
        "\n",
        "                # Extract attention patterns only if requested\n",
        "                if return_attention:\n",
        "                    attentions = outputs.attentions[-1].mean(dim=1)  # Average over heads in last layer\n",
        "                    chunk_attentions.append(attentions)\n",
        "\n",
        "            # Combine emotions from chunks with temporal weighting\n",
        "            if len(chunk_emotions) > 1:\n",
        "                chunk_emotions = torch.cat(chunk_emotions, dim=0)\n",
        "                # Exponential weighting might work better for many chunks\n",
        "                weights = torch.exp(torch.linspace(0.0, 1.0, len(chunk_emotions))).to(chunk_emotions.device)\n",
        "                weights = weights / weights.sum()  # Normalize\n",
        "                weighted_emotions = chunk_emotions * weights.unsqueeze(1)\n",
        "                final_emotions = weighted_emotions.sum(dim=0, keepdim=True)\n",
        "            else:\n",
        "                final_emotions = chunk_emotions[0]\n",
        "\n",
        "            all_emotion_scores.append(final_emotions)\n",
        "            if return_attention:\n",
        "                all_attention_weights.append(chunk_attentions)\n",
        "\n",
        "        # Combine results from batch\n",
        "        emotion_scores = torch.cat(all_emotion_scores, dim=0)\n",
        "\n",
        "        result = {\n",
        "            \"emotion_scores\": emotion_scores,\n",
        "            \"emotion_categories\": self.emotion_categories\n",
        "        }\n",
        "\n",
        "        if return_attention:\n",
        "            result[\"attention_weights\"] = all_attention_weights\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _create_text_chunks(self, text):\n",
        "        \"\"\"Create overlapping chunks for long text analysis\"\"\"\n",
        "        # Quick tokenize to get token count (without padding/truncation)\n",
        "        tokens = self.tokenizer.encode(text)\n",
        "\n",
        "        if len(tokens) <= self.window_size:\n",
        "            return [text]  # Text fits in one window\n",
        "\n",
        "        # For longer texts, create overlapping chunks\n",
        "        chunks = []\n",
        "        words = text.split()\n",
        "\n",
        "        # Estimate words per window based on tokens\n",
        "        words_per_token = max(1, len(words) / len(tokens))\n",
        "        words_per_window = int(self.window_size * words_per_token * 0.9)  # 90% to be safe\n",
        "        stride_in_words = int(self.stride * words_per_token * 0.9)\n",
        "\n",
        "        # Create overlapping chunks\n",
        "        for i in range(0, len(words), stride_in_words):\n",
        "            chunk = \" \".join(words[i:i + words_per_window])\n",
        "            chunks.append(chunk)\n",
        "\n",
        "            # Stop if we've covered the whole text\n",
        "            if i + words_per_window >= len(words):\n",
        "                break\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def extract_emotional_arc(self, text, num_segments=10):\n",
        "        \"\"\"\n",
        "        Extract emotional arc across text for narrative progression\n",
        "        Returns emotion scores for multiple segments of text\n",
        "        \"\"\"\n",
        "        # Create segments\n",
        "        if isinstance(text, str):\n",
        "            words = text.split()\n",
        "            segment_size = max(1, len(words) // num_segments)\n",
        "            segments = []\n",
        "\n",
        "            for i in range(0, len(words), segment_size):\n",
        "                segment = \" \".join(words[i:i + segment_size])\n",
        "                segments.append(segment)\n",
        "\n",
        "                # Stop if we've covered the whole text\n",
        "                if len(segments) >= num_segments:\n",
        "                    break\n",
        "        else:\n",
        "            segments = text  # Already a list of segments\n",
        "\n",
        "        # Process each segment\n",
        "        segment_emotions = []\n",
        "\n",
        "        for segment in segments:\n",
        "            result = self.forward([segment], return_attention=return_attention)\n",
        "            segment_emotions.append(result[\"emotion_scores\"][0])\n",
        "\n",
        "        # Stack emotions to create emotional arc [num_segments, num_emotions]\n",
        "        emotional_arc = torch.stack(segment_emotions)\n",
        "\n",
        "        return {\n",
        "            \"emotional_arc\": emotional_arc,\n",
        "            \"emotion_categories\": self.emotion_categories,\n",
        "            \"segments\": segments\n",
        "        }"
      ],
      "metadata": {
        "id": "bIKDBkDhHOfq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AEJWMxlcHPFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Emotion to Music"
      ],
      "metadata": {
        "id": "9CaqLqNdHWHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This neural network maps the extracted emotion scores to a set of musical parameters (tempo, key, mode, etc.), blending learned mappings with expert-defined emotion-music rules for more musically meaningful outputs. It also generates detailed prompts for music generation models."
      ],
      "metadata": {
        "id": "aZX_nFpRHmdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionToMusicMapper(nn.Module):\n",
        "    \"\"\"Maps emotional content to musical parameters\"\"\"\n",
        "    def __init__(self, emotion_dim=8, music_param_dim=16, device=None):\n",
        "        super(EmotionToMusicMapper, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.emotion_dim = emotion_dim\n",
        "        self.music_param_dim = music_param_dim\n",
        "\n",
        "        # Neural mapping from emotions to musical parameters\n",
        "        self.mapping_network = nn.Sequential(\n",
        "            nn.Linear(emotion_dim, 64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, music_param_dim)\n",
        "        )\n",
        "        self.mapping_network = self.mapping_network.to(self.device)\n",
        "\n",
        "        # Define musical parameter ranges\n",
        "        self.music_params = {\n",
        "            \"tempo\": {\"min\": 60, \"max\": 180},        # BPM\n",
        "            \"key\": {\"min\": 0, \"max\": 11},            # C=0, C#=1, etc.\n",
        "            \"mode\": {\"min\": 0, \"max\": 1},            # Major=0, Minor=1\n",
        "            \"rhythm_density\": {\"min\": 0.2, \"max\": 0.9},\n",
        "            \"note_duration\": {\"min\": 0.1, \"max\": 0.8},\n",
        "            \"articulation\": {\"min\": 0.2, \"max\": 0.9}, # Staccato to legato\n",
        "            \"dynamics\": {\"min\": 0.1, \"max\": 0.9},     # Soft to loud\n",
        "            \"timbre_brightness\": {\"min\": 0.1, \"max\": 0.9},\n",
        "            \"harmonic_complexity\": {\"min\": 0.1, \"max\": 0.9},\n",
        "            \"dissonance\": {\"min\": 0.0, \"max\": 0.7},\n",
        "            \"reverb\": {\"min\": 0.0, \"max\": 0.8},\n",
        "            \"stereo_width\": {\"min\": 0.3, \"max\": 1.0},\n",
        "            \"instrumentation\": {\"min\": 0, \"max\": 5},  # Different ensemble types\n",
        "            \"melodic_range\": {\"min\": 12, \"max\": 36},  # Range in semitones\n",
        "            \"bass_presence\": {\"min\": 0.1, \"max\": 0.9},\n",
        "            \"density\": {\"min\": 0.1, \"max\": 0.9}       # Sparse to dense\n",
        "        }\n",
        "\n",
        "        # Emotion to music parameter mapping rules (prior knowledge)\n",
        "        # These serve as biases for the neural mapping\n",
        "        self.emotion_music_rules = {\n",
        "            \"joy\": {\n",
        "                \"tempo\": 0.7,          # Faster tempo\n",
        "                \"mode\": 0.2,           # Tends toward major\n",
        "                \"dissonance\": 0.2,     # Low dissonance\n",
        "                \"dynamics\": 0.7        # Louder dynamics\n",
        "            },\n",
        "            \"sadness\": {\n",
        "                \"tempo\": 0.3,          # Slower tempo\n",
        "                \"mode\": 0.8,           # Tends toward minor\n",
        "                \"note_duration\": 0.7,  # Longer notes\n",
        "                \"reverb\": 0.7          # More reverb\n",
        "            },\n",
        "            \"anger\": {\n",
        "                \"tempo\": 0.8,          # Fast tempo\n",
        "                \"dissonance\": 0.7,     # High dissonance\n",
        "                \"dynamics\": 0.9,       # Loud dynamics\n",
        "                \"articulation\": 0.3    # More staccato\n",
        "            },\n",
        "            \"fear\": {\n",
        "                \"dissonance\": 0.6,     # Higher dissonance\n",
        "                \"dynamics\": 0.4,       # Varied dynamics\n",
        "                \"stereo_width\": 0.8    # Wide stereo field\n",
        "            },\n",
        "            \"tenderness\": {\n",
        "                \"tempo\": 0.4,          # Moderate-slow tempo\n",
        "                \"dynamics\": 0.3,       # Soft dynamics\n",
        "                \"articulation\": 0.8,   # Legato\n",
        "                \"harmonic_complexity\": 0.4  # Simple harmonies\n",
        "            },\n",
        "            \"excitement\": {\n",
        "                \"tempo\": 0.8,          # Fast tempo\n",
        "                \"rhythm_density\": 0.8, # Dense rhythms\n",
        "                \"dynamics\": 0.8        # Loud dynamics\n",
        "            },\n",
        "            \"calmness\": {\n",
        "                \"tempo\": 0.3,          # Slow tempo\n",
        "                \"dynamics\": 0.3,       # Soft dynamics\n",
        "                \"reverb\": 0.6,         # More reverb\n",
        "                \"dissonance\": 0.1      # Consonant harmonies\n",
        "            },\n",
        "            \"tension\": {\n",
        "                \"harmonic_complexity\": 0.7,  # Complex harmonies\n",
        "                \"dissonance\": 0.6,     # More dissonance\n",
        "                \"dynamics\": 0.5        # Varied dynamics\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def forward(self, emotion_scores):\n",
        "        \"\"\"\n",
        "        Map emotion scores to musical parameters\n",
        "        emotion_scores: [batch_size, emotion_dim] or [emotion_dim]\n",
        "        \"\"\"\n",
        "        # Ensure emotion_scores has batch dimension\n",
        "        if len(emotion_scores.shape) == 1:\n",
        "            emotion_scores = emotion_scores.unsqueeze(0)\n",
        "\n",
        "        # Make sure it's on the right device\n",
        "        emotion_scores = emotion_scores.to(self.device)\n",
        "\n",
        "        # Apply neural mapping\n",
        "        raw_params = self.mapping_network(emotion_scores)\n",
        "\n",
        "        # Apply prior knowledge as bias\n",
        "        music_params = self._apply_emotion_rules(emotion_scores, raw_params)\n",
        "\n",
        "        # Scale parameters to their defined ranges\n",
        "        scaled_params = self._scale_to_ranges(music_params)\n",
        "\n",
        "        return {\n",
        "            \"music_params\": music_params,\n",
        "            \"scaled_params\": scaled_params\n",
        "        }\n",
        "\n",
        "    def _apply_emotion_rules(self, emotion_scores, raw_params):\n",
        "        \"\"\"Apply emotion-music rules as biases to neural output\"\"\"\n",
        "        batch_size = emotion_scores.shape[0]\n",
        "        device = emotion_scores.device\n",
        "\n",
        "        # Initialize parameter tensor with neural output\n",
        "        music_params = raw_params.clone()\n",
        "\n",
        "        # Get the indices for each parameter in the output tensor\n",
        "        param_indices = {param: i for i, param in enumerate(self.music_params.keys())}\n",
        "\n",
        "        # Map emotion indices to names\n",
        "        emotion_names = [\"joy\", \"sadness\", \"anger\", \"fear\",\n",
        "                         \"tenderness\", \"excitement\", \"calmness\", \"tension\"]\n",
        "\n",
        "        # For each emotion, apply its rules based on strength\n",
        "        for b in range(batch_size):\n",
        "            for i, emotion in enumerate(emotion_names):\n",
        "                if i >= emotion_scores.shape[1]:\n",
        "                    continue  # Skip if index is out of bounds\n",
        "\n",
        "                # Get emotion strength (0 to 1)\n",
        "                emotion_strength = emotion_scores[b, i].item()\n",
        "\n",
        "                # Skip if emotion is not strongly present\n",
        "                if emotion_strength < 0.2:\n",
        "                    continue\n",
        "\n",
        "                # Apply each rule for this emotion\n",
        "                if emotion in self.emotion_music_rules:\n",
        "                    for param, value in self.emotion_music_rules[emotion].items():\n",
        "                        if param in param_indices:\n",
        "                            idx = param_indices[param]\n",
        "                            # Blend neural output with rule-based value based on emotion strength\n",
        "                            blend_factor = emotion_strength * 0.7  # Max 70% influence\n",
        "                            music_params[b, idx] = (1 - blend_factor) * music_params[b, idx] + blend_factor * value\n",
        "\n",
        "        return music_params\n",
        "\n",
        "    def _scale_to_ranges(self, music_params):\n",
        "        \"\"\"Scale normalized parameters to their actual ranges\"\"\"\n",
        "        batch_size = music_params.shape[0]\n",
        "        scaled_params = {}\n",
        "\n",
        "        for i, (param, range_info) in enumerate(self.music_params.items()):\n",
        "            # Get parameter values (clamped to 0-1)\n",
        "            values = torch.clamp(music_params[:, i], 0.0, 1.0)\n",
        "\n",
        "            # Scale to actual range\n",
        "            min_val, max_val = range_info[\"min\"], range_info[\"max\"]\n",
        "            scaled = min_val + values * (max_val - min_val)\n",
        "\n",
        "            # Special handling for discrete parameters\n",
        "            if param in [\"key\", \"instrumentation\"]:\n",
        "                scaled = scaled.round()\n",
        "\n",
        "            scaled_params[param] = scaled\n",
        "\n",
        "        return scaled_params\n",
        "\n",
        "    def generate_musiclm_prompt(self, emotion_scores, emotion_categories):\n",
        "        \"\"\"\n",
        "        Generate a detailed MusicLM-compatible prompt based on emotion analysis\n",
        "        emotion_scores: tensor of emotion scores\n",
        "        emotion_categories: list of emotion category names\n",
        "        \"\"\"\n",
        "        # Ensure emotion_scores has batch dimension and is on CPU for processing\n",
        "        if len(emotion_scores.shape) == 1:\n",
        "            emotion_scores = emotion_scores.unsqueeze(0)\n",
        "\n",
        "        emotion_scores = emotion_scores.detach().cpu()\n",
        "\n",
        "        # Get music parameters for these emotions\n",
        "        music_params = self.forward(emotion_scores)\n",
        "        scaled_params = music_params[\"scaled_params\"]\n",
        "\n",
        "        # Build prompt\n",
        "        prompts = []\n",
        "\n",
        "        for b in range(emotion_scores.shape[0]):\n",
        "            # Get top emotions\n",
        "            if emotion_scores.shape[1] <= len(emotion_categories):\n",
        "                emotions_data = [(emotion_categories[i], emotion_scores[b, i].item())\n",
        "                                for i in range(emotion_scores.shape[1])]\n",
        "            else:\n",
        "                emotions_data = [(f\"Emotion {i}\", emotion_scores[b, i].item())\n",
        "                                for i in range(emotion_scores.shape[1])]\n",
        "\n",
        "            # Sort emotions by strength\n",
        "            emotions_data.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Get top 3 emotions\n",
        "            top_emotions = [e for e, s in emotions_data if s > 0.2][:3]\n",
        "\n",
        "            # Get key musical parameters\n",
        "            tempo = scaled_params[\"tempo\"][b].item()\n",
        "\n",
        "            # Determine mode (major/minor)\n",
        "            mode = \"minor\" if scaled_params[\"mode\"][b].item() > 0.5 else \"major\"\n",
        "\n",
        "            # Determine instrumentation based on emotions\n",
        "            instruments = self._choose_instrumentation(top_emotions)\n",
        "\n",
        "            # Determine musical style based on emotions and parameters\n",
        "            style = self._choose_style(top_emotions, scaled_params, b)\n",
        "\n",
        "            # Build descriptive prompt\n",
        "            emotion_desc = \" and \".join(top_emotions) if top_emotions else \"neutral\"\n",
        "\n",
        "            prompt = f\"A {style} piece in {mode} key at {tempo:.0f} BPM, evoking feelings of {emotion_desc}. \"\n",
        "            prompt += f\"Featuring {instruments}. \"\n",
        "\n",
        "            # Add specifics based on parameters\n",
        "            if scaled_params[\"harmonic_complexity\"][b].item() > 0.7:\n",
        "                prompt += \"Complex harmonies with unexpected chord changes. \"\n",
        "            elif scaled_params[\"harmonic_complexity\"][b].item() < 0.3:\n",
        "                prompt += \"Simple, consonant harmonies. \"\n",
        "\n",
        "            if scaled_params[\"rhythm_density\"][b].item() > 0.7:\n",
        "                prompt += \"Dense, intricate rhythms. \"\n",
        "            elif scaled_params[\"rhythm_density\"][b].item() < 0.3:\n",
        "                prompt += \"Sparse, spacious rhythms. \"\n",
        "\n",
        "            if scaled_params[\"dynamics\"][b].item() > 0.7:\n",
        "                prompt += \"Dramatic dynamic range with powerful crescendos. \"\n",
        "            elif scaled_params[\"dynamics\"][b].item() < 0.3:\n",
        "                prompt += \"Gentle, subtle dynamics. \"\n",
        "\n",
        "            if scaled_params[\"reverb\"][b].item() > 0.6:\n",
        "                prompt += \"Immersive, spacious reverb. \"\n",
        "\n",
        "            prompts.append(prompt)\n",
        "\n",
        "        return prompts\n",
        "\n",
        "    def _choose_instrumentation(self, emotions):\n",
        "        \"\"\"Choose appropriate instrumentation based on emotions\"\"\"\n",
        "        if any(e in [\"sadness\", \"tenderness\", \"calmness\"] for e in emotions):\n",
        "            return \"piano and strings with subtle woodwinds\"\n",
        "        elif any(e in [\"joy\", \"excitement\"] for e in emotions):\n",
        "            return \"full orchestra with prominent brass and percussion\"\n",
        "        elif any(e in [\"anger\", \"tension\"] for e in emotions):\n",
        "            return \"distorted electric guitars, heavy percussion, and synthesizers\"\n",
        "        elif any(e in [\"fear\"] for e in emotions):\n",
        "            return \"dissonant strings, prepared piano, and electronic elements\"\n",
        "        else:\n",
        "            return \"chamber ensemble with piano, strings, and woodwinds\"\n",
        "\n",
        "    def _choose_style(self, emotions, scaled_params, batch_idx):\n",
        "        \"\"\"Choose musical style based on emotions and parameters\"\"\"\n",
        "        tempo = scaled_params[\"tempo\"][batch_idx].item()\n",
        "        harmonic_complexity = scaled_params[\"harmonic_complexity\"][batch_idx].item()\n",
        "\n",
        "        if any(e in [\"sadness\", \"tenderness\"] for e in emotions) and tempo < 100:\n",
        "            return \"melancholic, cinematic\"\n",
        "        elif any(e in [\"joy\", \"excitement\"] for e in emotions) and tempo > 120:\n",
        "            return \"uplifting, energetic\"\n",
        "        elif any(e in [\"fear\", \"tension\"] for e in emotions):\n",
        "            return \"suspenseful, atmospheric\"\n",
        "        elif any(e in [\"calmness\"] for e in emotions):\n",
        "            return \"ambient, peaceful\"\n",
        "        elif any(e in [\"anger\"] for e in emotions):\n",
        "            return \"intense, dramatic\"\n",
        "        elif harmonic_complexity > 0.6:\n",
        "            return \"complex, avant-garde\"\n",
        "        else:\n",
        "            return \"melodic, contemporary\""
      ],
      "metadata": {
        "id": "0hV5oWmDHZvR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_music_with_musiclm(text, emotion_extractor, emotion_mapper, musiclm_model=None):\n",
        "    \"\"\"\n",
        "    Generate music from text using emotion mapping and MusicLM approach\n",
        "    \"\"\"\n",
        "    # Extract emotions from text\n",
        "    emotion_info = emotion_extractor(text)\n",
        "    emotion_scores = emotion_info[\"emotion_scores\"]\n",
        "    emotion_categories = emotion_info[\"emotion_categories\"]\n",
        "\n",
        "    print(f\"Extracted emotions with shape: {emotion_scores.shape}\")\n",
        "\n",
        "    # Get emotional arc for longer texts\n",
        "    if len(text.split()) > 100:\n",
        "        arc_info = emotion_extractor.extract_emotional_arc(text)\n",
        "        emotional_arc = arc_info[\"emotional_arc\"]\n",
        "        segments = arc_info[\"segments\"]\n",
        "        print(f\"Extracted emotional arc with shape: {emotional_arc.shape}\")\n",
        "    else:\n",
        "        emotional_arc = None\n",
        "        segments = [text]\n",
        "\n",
        "    # Generate MusicLM prompts from emotions\n",
        "    if emotional_arc is not None:\n",
        "        # Generate multiple prompts for different segments\n",
        "        prompts = emotion_mapper.generate_musiclm_prompt(emotional_arc, emotion_categories)\n",
        "        print(f\"Generated {len(prompts)} segment prompts from emotional arc\")\n",
        "    else:\n",
        "        # Generate a single prompt\n",
        "        prompts = emotion_mapper.generate_musiclm_prompt(emotion_scores, emotion_categories)\n",
        "        print(f\"Generated single prompt from emotion analysis\")\n",
        "\n",
        "    # Call MusicLM model if provided\n",
        "    if musiclm_model is not None:\n",
        "        # This would be the integration point with your MusicLM model\n",
        "        print(\"Calling MusicLM model with generated prompts...\")\n",
        "        # Example: audio = musiclm_model.generate(prompts[0])\n",
        "        audio = None  # Replace with actual MusicLM output\n",
        "    else:\n",
        "        audio = None\n",
        "\n",
        "    return {\n",
        "        \"prompts\": prompts,\n",
        "        \"emotion_scores\": emotion_scores,\n",
        "        \"emotional_arc\": emotional_arc,\n",
        "        \"segments\": segments,\n",
        "        \"audio\": audio\n",
        "    }"
      ],
      "metadata": {
        "id": "LTuoR8IYHbWi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfP9rbMjHtal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Coherence Model"
      ],
      "metadata": {
        "id": "g2OaneCXHtz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This LSTM-based model smooths the sequence of musical parameters generated for narrative segments, ensuring that the music transitions naturally and reflects the evolving emotional arc of the text."
      ],
      "metadata": {
        "id": "PfpOiscHH4-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temporal Coherence Mechanism\n",
        "\n",
        "| Component           | Function                    | Technical Implementation               |\n",
        "|---------------------|-----------------------------|-----------------------------------------|\n",
        "| LSTM                | Sequence modeling           | 2-layer, 128 hidden dim                 |\n",
        "| Text Conditioning   | Narrative context injection | Projection layer to LSTM state         |\n",
        "| Parameter Smoothing | Transition naturalness      | Exponential moving average (β=0.85)     |\n",
        "\n"
      ],
      "metadata": {
        "id": "Igpaog21K345"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalCoherenceModel(nn.Module):\n",
        "    \"\"\"Model for ensuring temporal coherence in music generation\"\"\"\n",
        "    def __init__(self, music_param_dim=16, hidden_dim=128, num_layers=2):\n",
        "        super(TemporalCoherenceModel, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.music_param_dim = music_param_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # LSTM for modeling parameter sequences\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=music_param_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.1 if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Output layer to predict next parameters\n",
        "        self.output_layer = nn.Linear(hidden_dim, music_param_dim)\n",
        "\n",
        "        # Layer for embedding text condition into the LSTM\n",
        "        self.text_condition_layer = nn.Linear(512, hidden_dim)  # Assuming text_embedding_dim=512\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, param_sequence, text_embedding=None):\n",
        "        \"\"\"\n",
        "        Process a sequence of musical parameters with optional text condition\n",
        "        param_sequence: [batch_size, seq_length, music_param_dim]\n",
        "        text_embedding: [batch_size, text_embedding_dim]\n",
        "        \"\"\"\n",
        "        batch_size, seq_length = param_sequence.shape[0], param_sequence.shape[1]\n",
        "\n",
        "        # Initialize hidden state, optionally with text condition\n",
        "        h0 = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_dim).to(param_sequence.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_dim).to(param_sequence.device)\n",
        "\n",
        "        if text_embedding is not None:\n",
        "            text_hidden = self.text_condition_layer(text_embedding)\n",
        "            h0[0] = text_hidden\n",
        "\n",
        "        if segment_embedding is not None:\n",
        "            # You'd need to add a segment embedding layer in __init__\n",
        "            segment_hidden = self.segment_condition_layer(segment_embedding)\n",
        "            # Combine with text embedding or use in second layer\n",
        "            if text_embedding is not None:\n",
        "                h0[0] = 0.7 * h0[0] + 0.3 * segment_hidden  # Weighted combination\n",
        "            else:\n",
        "                h0[0] = segment_hidden\n",
        "\n",
        "        # Run LSTM\n",
        "        lstm_out, (hn, cn) = self.lstm(param_sequence, (h0, c0))\n",
        "\n",
        "        # Project to output parameter space\n",
        "        output_sequence = self.output_layer(lstm_out)\n",
        "\n",
        "        return output_sequence, (hn, cn)\n",
        "\n",
        "    def generate_sequence(self, initial_params, sequence_length, text_embedding=None):\n",
        "        \"\"\"\n",
        "        Generate a coherent sequence of musical parameters\n",
        "        initial_params: [batch_size, music_param_dim]\n",
        "        \"\"\"\n",
        "        \"\"\"Generate a coherent sequence of musical parameters\"\"\"\n",
        "        # Store current training state and set to eval mode\n",
        "        was_training = self.training\n",
        "        self.eval()\n",
        "\n",
        "        try:\n",
        "            batch_size = initial_params.shape[0]\n",
        "            device = initial_params.device\n",
        "\n",
        "            # Initialize sequence with initial parameters\n",
        "            generated_sequence = [initial_params]\n",
        "\n",
        "            # Initialize hidden state with text embedding if provided\n",
        "            h = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "            c = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "\n",
        "            if text_embedding is not None:\n",
        "                # Project text embedding to hidden dimension\n",
        "                text_hidden = self.text_condition_layer(text_embedding)\n",
        "\n",
        "                # Use text as initial hidden state in first layer\n",
        "                h[0] = text_hidden\n",
        "\n",
        "            # Generate sequence autoregressively\n",
        "            current_params = initial_params\n",
        "\n",
        "            for _ in range(sequence_length - 1):\n",
        "                # Reshape for LSTM (add sequence dimension)\n",
        "                params_input = current_params.unsqueeze(1)\n",
        "\n",
        "                # Get prediction and update hidden state\n",
        "                with torch.no_grad():\n",
        "                    lstm_out, (h, c) = self.lstm(params_input, (h, c))\n",
        "                    next_params = self.output_layer(lstm_out[:, -1, :])\n",
        "\n",
        "                # Add to sequence\n",
        "                generated_sequence.append(next_params)\n",
        "\n",
        "                # Update current parameters\n",
        "                current_params = next_params\n",
        "\n",
        "                if smoothing > 0:\n",
        "                    next_params = (1 - smoothing) * next_params + smoothing * current_params\n",
        "\n",
        "            # Stack into a single tensor [batch_size, sequence_length, music_param_dim]\n",
        "            return torch.stack(generated_sequence, dim=1)\n",
        "        finally:\n",
        "            # Restore previous training state\n",
        "            self.train(was_training)"
      ],
      "metadata": {
        "id": "cV90GvykHxKd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_coherent_music_sequence(text, emotion_extractor, emotion_mapper, coherence_model, musiclm_model=None):\n",
        "    \"\"\"Generate coherent music across narrative segments\"\"\"\n",
        "\n",
        "    # Extract emotional arc\n",
        "    arc_info = emotion_extractor.extract_emotional_arc(text, num_segments=10)\n",
        "    emotional_arc = arc_info[\"emotional_arc\"]\n",
        "    segments = arc_info[\"segments\"]\n",
        "\n",
        "    # Map emotions to initial music parameters\n",
        "    music_params_sequence = []\n",
        "    for i in range(emotional_arc.shape[0]):\n",
        "        segment_emotions = emotional_arc[i]\n",
        "        music_params = emotion_mapper.forward(segment_emotions)[\"music_params\"]\n",
        "        music_params_sequence.append(music_params)\n",
        "\n",
        "    # Stack into sequence tensor\n",
        "    music_params_sequence = torch.stack(music_params_sequence, dim=1)\n",
        "\n",
        "    # Apply temporal coherence model to smooth transitions\n",
        "    coherent_params = coherence_model.generate_sequence(\n",
        "        music_params_sequence[:, 0, :],  # Initial parameters\n",
        "        emotional_arc.shape[0],          # Sequence length\n",
        "        text_embedding=None              # Could add text embedding if available\n",
        "    )\n",
        "\n",
        "    # Generate prompts from coherent parameters\n",
        "    prompts = []\n",
        "    for i in range(coherent_params.shape[1]):\n",
        "        segment_params = coherent_params[:, i, :]\n",
        "        # Convert parameters back to emotion space (you'd need to add this method)\n",
        "        segment_emotions = emotion_mapper.params_to_emotions(segment_params)\n",
        "        prompt = emotion_mapper.generate_musiclm_prompt(segment_emotions, arc_info[\"emotion_categories\"])\n",
        "        prompts.append(prompt[0])  # Assuming single batch\n",
        "\n",
        "    # Now you could feed these prompts to MusicLM with appropriate transitions\n",
        "\n",
        "    return {\n",
        "        \"coherent_params\": coherent_params,\n",
        "        \"prompts\": prompts,\n",
        "        \"segments\": segments\n",
        "    }"
      ],
      "metadata": {
        "id": "r5O1IhmwH4My"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3TsG-TzuIyT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EnhancedTextToMusicGenerationModel\n",
        "\n"
      ],
      "metadata": {
        "id": "jPKg8NdpIytH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The central integration point combining all subsystems:\n",
        "\n",
        "- **Text Processing Pipeline** -\n",
        "Uses MuLANTextEncoder (RoBERTa-based) for semantic understanding and EmotionExtractionModule for emotional arc analysis\n",
        "\n",
        "- **Multimodal Mapping** -\n",
        "EmotionToMusicMapper translates emotional signals to 16 musical parameters (tempo, key, instrumentation)\n",
        "\n",
        "- **Temporal Modeling** -\n",
        "TemporalCoherenceModel (LSTM-based) ensures smooth transitions between musical segments\n",
        "\n",
        "- **Audio Synthesis** -\n",
        "UNet architecture with diffusion process for high-quality audio generation"
      ],
      "metadata": {
        "id": "_GH4FeyTJesC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Emotion-to-Music Translation**\n",
        "- Hybrid approach combining neural networks (75% influence) with expert-defined emotion-music rules (25%)\n",
        "\n",
        "- Dynamic parameter scaling to musical ranges (e.g., tempo: 60-180 BPM)\n",
        "\n",
        "- Context-aware instrumentation selection based on emotion combinations"
      ],
      "metadata": {
        "id": "5CGxidflJ6Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedTextToMusicGenerationModel(nn.Module):\n",
        "    \"\"\"Enhanced text-to-music generation model with emotional analysis and temporal coherence\"\"\"\n",
        "    def __init__(self, embedding_dim=512):\n",
        "        super(EnhancedTextToMusicGenerationModel, self).__init__()\n",
        "\n",
        "        # Text encoders\n",
        "        self.text_encoder = MuLANTextEncoder(embedding_dim=embedding_dim)\n",
        "        self.emotion_extractor = EmotionExtractionModule(num_emotions=8)\n",
        "\n",
        "        # Emotion to music mapping\n",
        "        self.emotion_mapper = EmotionToMusicMapper(emotion_dim=8, music_param_dim=16)\n",
        "\n",
        "        # Temporal coherence model\n",
        "        self.temporal_model = TemporalCoherenceModel(music_param_dim=16, hidden_dim=128)\n",
        "\n",
        "        # Audio encoder for conditioning\n",
        "        self.audio_encoder = SoundStreamEncoder(embedding_dim=embedding_dim)\n",
        "\n",
        "        # Audio decoder for reconstruction\n",
        "        self.audio_decoder = SoundStreamDecoder(embedding_dim=embedding_dim)\n",
        "\n",
        "        # UNet for high-resolution generation\n",
        "        self.unet = MusicConditionedUNet(embedding_dim=embedding_dim)\n",
        "\n",
        "        # Music parameter conditioning layer (to inject into UNet)\n",
        "        self.music_param_conditioning = nn.Linear(16, embedding_dim)\n",
        "\n",
        "    def analyze_text(self, text, extract_arc=False):\n",
        "        \"\"\"Analyze text for emotional content and create music parameters\"\"\"\n",
        "        # Get base text embedding\n",
        "        text_embedding = self.text_encoder(text)\n",
        "\n",
        "        # Extract emotional information\n",
        "        if extract_arc:\n",
        "            # For longer texts, extract emotional arc\n",
        "            emotion_info = self.emotion_extractor.extract_emotional_arc(text)\n",
        "            emotion_scores = emotion_info[\"emotional_arc\"][0]  # Use first segment for initial params\n",
        "            emotional_arc = emotion_info[\"emotional_arc\"]\n",
        "        else:\n",
        "            # For shorter texts, get overall emotion\n",
        "            emotion_info = self.emotion_extractor(text)\n",
        "            emotion_scores = emotion_info[\"emotion_scores\"]\n",
        "            emotional_arc = None\n",
        "\n",
        "        # Map emotions to musical parameters\n",
        "        music_params = self.emotion_mapper(emotion_scores)\n",
        "\n",
        "        # Generate temporal sequence of parameters if we have an emotional arc\n",
        "        if emotional_arc is not None:\n",
        "            # Generate parameter sequence for each segment in the arc\n",
        "            param_sequences = []\n",
        "            for i in range(emotional_arc.size(0)):\n",
        "                segment_params = self.emotion_mapper(emotional_arc[i].unsqueeze(0))\n",
        "                param_sequences.append(segment_params[\"music_params\"])\n",
        "\n",
        "            # Stack into sequence [sequence_length, batch_size, param_dim]\n",
        "            param_sequence = torch.stack(param_sequences)\n",
        "\n",
        "            # Apply temporal coherence\n",
        "            smoothed_sequence = self.temporal_model.generate_sequence(\n",
        "                param_sequences[0],\n",
        "                len(param_sequences),\n",
        "                text_embedding\n",
        "            )\n",
        "\n",
        "            # Also create a conditioning embedding from musical parameters\n",
        "            music_conditioning = self.music_param_conditioning(music_params[\"music_params\"])\n",
        "            combined_embedding = text_embedding + 0.3 * music_conditioning\n",
        "        else:\n",
        "            param_sequence = None\n",
        "            smoothed_sequence = None\n",
        "\n",
        "            # Create conditioning embedding from musical parameters\n",
        "            music_conditioning = self.music_param_conditioning(music_params[\"music_params\"])\n",
        "            combined_embedding = text_embedding + 0.3 * music_conditioning\n",
        "\n",
        "        return {\n",
        "            \"text_embedding\": text_embedding,\n",
        "            \"emotion_scores\": emotion_scores,\n",
        "            \"music_params\": music_params,\n",
        "            \"param_sequence\": smoothed_sequence,\n",
        "            \"combined_embedding\": combined_embedding,\n",
        "            \"emotional_arc\": emotional_arc\n",
        "        }\n",
        "\n",
        "    def generate_from_text(self, text, noise_level=0.9, steps=50, length=16000, extract_arc=True):\n",
        "        \"\"\"\n",
        "        Generate audio conditioned on text with enhanced emotional mapping\n",
        "        \"\"\"\n",
        "        # Determine device\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # Analyze text for emotions and musical parameters\n",
        "        analysis = self.analyze_text(text, extract_arc=extract_arc)\n",
        "        print(f\"Text analyzed. Emotion scores: {analysis['emotion_scores'].tolist()}\")\n",
        "\n",
        "        # Print key musical parameters for transparency\n",
        "        scaled_params = analysis['music_params']['scaled_params']\n",
        "        print(f\"Musical parameters derived from text:\")\n",
        "        for param, value in scaled_params.items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                print(f\"  {param}: {value.item():.2f}\")\n",
        "\n",
        "        # Use combined embedding that incorporates musical parameters\n",
        "        combined_embedding = analysis['combined_embedding'].to(device)\n",
        "        print(f\"Combined embedding shape: {combined_embedding.shape}\")\n",
        "\n",
        "        # Create initial noise\n",
        "        audio = torch.randn(1, 1, length, device=device) * noise_level\n",
        "        print(f\"Initial noise created with shape: {audio.shape}\")\n",
        "\n",
        "        # Diffusion sampling loop with musical parameter conditioning\n",
        "        with torch.no_grad():\n",
        "            for i in range(steps):\n",
        "                # Get model prediction\n",
        "                update = self.unet(audio, combined_embedding)\n",
        "\n",
        "                # Calculate noise scale for this step\n",
        "                noise_scale = noise_level * (1.0 - (i / steps))\n",
        "\n",
        "                # Make sure dimensions match before combining\n",
        "                if audio.shape[2] != update.shape[2]:\n",
        "                    min_len = min(audio.shape[2], update.shape[2])\n",
        "                    audio = audio[:, :, :min_len]\n",
        "                    update = update[:, :, :min_len]\n",
        "\n",
        "                # Apply the update with noise scheduling\n",
        "                audio = audio * (1.0 - noise_scale) + update * noise_scale\n",
        "\n",
        "                # Optional: Print progress\n",
        "                if i % 10 == 0:\n",
        "                    print(f\"Step {i}/{steps}, audio range: {audio.min().item():.3f} to {audio.max().item():.3f}\")\n",
        "\n",
        "                # Free up memory\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        # Normalize final audio to [-1, 1] range\n",
        "        audio = torch.clamp(audio, -1.0, 1.0)\n",
        "\n",
        "        return audio.cpu().numpy(), analysis"
      ],
      "metadata": {
        "id": "tAE34BN9IA4B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicGenerationEvaluator:\n",
        "    \"\"\"Evaluation metrics for the text-to-music generation model\"\"\"\n",
        "    def __init__(self):\n",
        "        # Import libraries for audio analysis\n",
        "        import librosa\n",
        "        self.librosa = librosa\n",
        "\n",
        "    def calculate_metrics(self, audio_array, sample_rate=16000):\n",
        "        \"\"\"Calculate objective metrics for generated audio\"\"\"\n",
        "        # Convert to mono if needed\n",
        "        if len(audio_array.shape) > 1 and audio_array.shape[0] > 1:\n",
        "            audio_mono = audio_array.mean(axis=0)\n",
        "        else:\n",
        "            audio_mono = audio_array.squeeze()\n",
        "\n",
        "        # Spectral features\n",
        "        spectral_centroid = self.librosa.feature.spectral_centroid(y=audio_mono, sr=sample_rate)[0]\n",
        "        spectral_contrast = self.librosa.feature.spectral_contrast(y=audio_mono, sr=sample_rate)[0]\n",
        "\n",
        "        # Rhythm features\n",
        "        tempo, _ = self.librosa.beat.beat_track(y=audio_mono, sr=sample_rate)\n",
        "\n",
        "        # Harmonic-percussive separation\n",
        "        harmonic, percussive = self.librosa.effects.hpss(audio_mono)\n",
        "\n",
        "        # MFCC features\n",
        "        mfccs = self.librosa.feature.mfcc(y=audio_mono, sr=sample_rate, n_mfcc=13)\n",
        "\n",
        "        # Calculate statistics\n",
        "        metrics = {\n",
        "            \"tempo\": tempo,\n",
        "            \"spectral_centroid_mean\": np.mean(spectral_centroid),\n",
        "            \"spectral_centroid_std\": np.std(spectral_centroid),\n",
        "            \"spectral_contrast_mean\": np.mean(spectral_contrast),\n",
        "            \"harmonic_ratio\": np.sum(np.abs(harmonic)) / (np.sum(np.abs(percussive)) + 1e-8),\n",
        "            \"mfcc_means\": np.mean(mfccs, axis=1).tolist(),\n",
        "            \"mfcc_stds\": np.std(mfccs, axis=1).tolist(),\n",
        "            \"rms_energy\": np.sqrt(np.mean(audio_mono**2)),\n",
        "            \"zero_crossing_rate\": np.mean(self.librosa.feature.zero_crossing_rate(audio_mono)[0])\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def compare_with_target(self, generated_metrics, target_metrics):\n",
        "        \"\"\"Compare generated audio with target metrics\"\"\"\n",
        "        comparison = {}\n",
        "\n",
        "        # Compare numerical metrics\n",
        "        for key in generated_metrics:\n",
        "            if key in target_metrics:\n",
        "                if isinstance(generated_metrics[key], list):\n",
        "                    # For lists (like MFCCs), calculate mean absolute difference\n",
        "                    comparison[key + \"_diff\"] = np.mean(np.abs(np.array(generated_metrics[key]) -\n",
        "                                                             np.array(target_metrics[key])))\n",
        "                else:\n",
        "                    # For single values\n",
        "                    comparison[key + \"_diff\"] = abs(generated_metrics[key] - target_metrics[key])\n",
        "\n",
        "        return comparison\n",
        "\n",
        "    def emotion_alignment_score(self, target_emotions, music_params, audio_metrics):\n",
        "        \"\"\"\n",
        "        Calculate how well the generated music aligns with target emotions\n",
        "        based on both specified parameters and extracted audio features\n",
        "        \"\"\"\n",
        "        alignment_scores = {}\n",
        "\n",
        "        # Map emotions to expected audio features\n",
        "        emotion_feature_map = {\n",
        "            \"joy\": {\n",
        "                \"tempo\": 0.7,                # Higher tempo\n",
        "                \"spectral_centroid_mean\": 0.7,  # Brighter sound\n",
        "                \"harmonic_ratio\": 0.7,       # More harmonic content\n",
        "                \"rms_energy\": 0.7            # More energy\n",
        "            },\n",
        "            \"sadness\": {\n",
        "                \"tempo\": 0.3,                # Lower tempo\n",
        "                \"spectral_centroid_mean\": 0.4,  # Darker sound\n",
        "                \"harmonic_ratio\": 0.6,       # More harmonic content\n",
        "                \"rms_energy\": 0.4            # Less energy\n",
        "            },\n",
        "            # Add maps for other emotions...\n",
        "        }\n",
        "\n",
        "        # Normalize audio metrics to 0-1 range for comparison\n",
        "        normalized_metrics = {\n",
        "            \"tempo\": min(1.0, max(0.0, audio_metrics[\"tempo\"] / 180.0)),\n",
        "            \"spectral_centroid_mean\": min(1.0, max(0.0, audio_metrics[\"spectral_centroid_mean\"] / 5000.0)),\n",
        "            \"harmonic_ratio\": min(1.0, max(0.0, audio_metrics[\"harmonic_ratio\"] / 5.0)),\n",
        "            \"rms_energy\": min(1.0, max(0.0, audio_metrics[\"rms_energy\"] / 0.3))\n",
        "        }\n",
        "\n",
        "        # Calculate alignment scores for each emotion\n",
        "        for emotion, strength in target_emotions.items():\n",
        "            if emotion in emotion_feature_map and strength > 0.2:\n",
        "                # Get expected features for this emotion\n",
        "                expected = emotion_feature_map[emotion]\n",
        "\n",
        "                # Calculate distance to expected features\n",
        "                feature_distances = []\n",
        "                for feature, value in expected.items():\n",
        "                    if feature in normalized_metrics:\n",
        "                        distance = 1.0 - abs(normalized_metrics[feature] - value)\n",
        "                        feature_distances.append(distance)\n",
        "\n",
        "                # Average the distances\n",
        "                if feature_distances:\n",
        "                    alignment_scores[emotion] = sum(feature_distances) / len(feature_distances)\n",
        "\n",
        "        # Calculate overall alignment\n",
        "        weighted_score = 0\n",
        "        total_weight = 0\n",
        "\n",
        "        for emotion, strength in target_emotions.items():\n",
        "            if emotion in alignment_scores:\n",
        "                weighted_score += alignment_scores[emotion] * strength\n",
        "                total_weight += strength\n",
        "\n",
        "        if total_weight > 0:\n",
        "            overall_score = weighted_score / total_weight\n",
        "        else:\n",
        "            overall_score = 0\n",
        "\n",
        "        return {\n",
        "            \"emotion_scores\": alignment_scores,\n",
        "            \"overall_alignment\": overall_score\n",
        "        }"
      ],
      "metadata": {
        "id": "1w0lMWdHIQqO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MuLANTextEncoder(nn.Module):\n",
        "    \"\"\"Text encoder for music generation based on MuLAN architecture\"\"\"\n",
        "    def __init__(self, embedding_dim=512, model_name=\"roberta-base\"):\n",
        "        super(MuLANTextEncoder, self).__init__()\n",
        "\n",
        "        # Use RoBERTa as base model\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "        self.text_model = RobertaModel.from_pretrained(model_name)\n",
        "\n",
        "        # Project to specified embedding dimension\n",
        "        self.projection = nn.Linear(self.text_model.config.hidden_size, embedding_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        \"\"\"\n",
        "        Encode text into a fixed-size embedding\n",
        "        text: string or list of strings\n",
        "        \"\"\"\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "\n",
        "        # Tokenize text\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(self.text_model.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get text embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = self.text_model(**inputs)\n",
        "\n",
        "        # Use [CLS] token embedding as text representation\n",
        "        text_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Project to target dimension\n",
        "        projected_embedding = self.projection(text_embedding)\n",
        "\n",
        "        return projected_embedding"
      ],
      "metadata": {
        "id": "lk8ld4qxIWgP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoundStreamEncoder(nn.Module):\n",
        "    \"\"\"Audio encoder based on SoundStream architecture\"\"\"\n",
        "    def __init__(self, embedding_dim=512):\n",
        "        super(SoundStreamEncoder, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=7, stride=1, padding=3),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Conv1d(32, 64, kernel_size=7, stride=2, padding=3),  # Downsampling\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=7, stride=2, padding=3),  # Downsampling\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Conv1d(128, 256, kernel_size=7, stride=2, padding=3),  # Downsampling\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Conv1d(256, embedding_dim, kernel_size=7, stride=2, padding=3),  # Downsampling\n",
        "            nn.BatchNorm1d(embedding_dim),\n",
        "            nn.LeakyReLU(0.1),\n",
        "        )\n",
        "\n",
        "        # Global average pooling for fixed-length embedding\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def forward(self, audio):\n",
        "        \"\"\"\n",
        "        Encode audio into a fixed-size embedding\n",
        "        audio: [batch_size, channels, samples]\n",
        "        \"\"\"\n",
        "        # Apply conv layers\n",
        "        x = self.conv_layers(audio)\n",
        "\n",
        "        # Global pooling\n",
        "        embedding = self.global_pool(x).squeeze(-1)\n",
        "\n",
        "        return embedding\n",
        "\n",
        "\n",
        "class SoundStreamDecoder(nn.Module):\n",
        "    \"\"\"Audio decoder based on SoundStream architecture\"\"\"\n",
        "    def __init__(self, embedding_dim=512):\n",
        "        super(SoundStreamDecoder, self).__init__()\n",
        "\n",
        "        # Initial projection\n",
        "        self.initial_proj = nn.Linear(embedding_dim, embedding_dim * 4)\n",
        "\n",
        "        # Reshape to [batch, channels, time]\n",
        "        self.reshape = lambda x: x.view(x.size(0), embedding_dim, 4)\n",
        "\n",
        "        # Transposed convolutional layers for upsampling\n",
        "        self.deconv_layers = nn.Sequential(\n",
        "            nn.ConvTranspose1d(embedding_dim, 256, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.ConvTranspose1d(256, 128, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.ConvTranspose1d(128, 64, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.ConvTranspose1d(64, 32, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.ConvTranspose1d(32, 1, kernel_size=7, stride=1, padding=3),\n",
        "            nn.Tanh()  # Output in range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, embedding, length=16000):\n",
        "        \"\"\"\n",
        "        Decode embedding to audio waveform\n",
        "        embedding: [batch_size, embedding_dim]\n",
        "        length: target audio length\n",
        "        \"\"\"\n",
        "        # Initial projection and reshape\n",
        "        x = self.initial_proj(embedding)\n",
        "        x = x.view(x.size(0), -1, 4)  # [batch, channels, time]\n",
        "        x = x.to(device)\n",
        "\n",
        "        # Apply transposed convolutions\n",
        "        audio = self.deconv_layers(x)\n",
        "\n",
        "        # Ensure output has the right length\n",
        "        if audio.shape[2] < length:\n",
        "            # Pad if too short\n",
        "            padding = torch.zeros(audio.shape[0], audio.shape[1], length - audio.shape[2],\n",
        "                                device=audio.device)\n",
        "            audio = torch.cat([audio, padding], dim=2)\n",
        "        elif audio.shape[2] > length:\n",
        "            # Truncate if too long\n",
        "            audio = audio[:, :, :length]\n",
        "\n",
        "        return audio\n",
        "\n"
      ],
      "metadata": {
        "id": "m0TF3x90IbWV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNet Architecture**\n",
        "\n",
        "- 5-level encoder/decoder with skip connections\n",
        "\n",
        "- Conditioning via projected text embeddings\n",
        "\n",
        "- Output: 16kHz audio in [-1,1] range"
      ],
      "metadata": {
        "id": "fvkPEODLKr2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicConditionedUNet(nn.Module):\n",
        "    \"\"\"UNet-based architecture for music generation with conditioning\"\"\"\n",
        "    def __init__(self, embedding_dim=512, channels=[32, 64, 128, 256, 512]):\n",
        "        super(MusicConditionedUNet, self).__init__()\n",
        "\n",
        "        # Encoder blocks (downsampling path)\n",
        "        self.encoder_blocks = nn.ModuleList()\n",
        "        in_channels = 1  # Audio input channels\n",
        "\n",
        "        for c in channels:\n",
        "            self.encoder_blocks.append(self._make_encoder_block(in_channels, c))\n",
        "            in_channels = c\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv1d(channels[-1], channels[-1] * 2, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(channels[-1] * 2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(channels[-1] * 2, channels[-1], kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(channels[-1]),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "        # Conditioning projection\n",
        "        self.cond_projection = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, channels[-1]),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "        # Decoder blocks (upsampling path)\n",
        "        self.decoder_blocks = nn.ModuleList()\n",
        "        in_channels = channels[-1]\n",
        "\n",
        "        for c in reversed(channels[:-1]):\n",
        "            self.decoder_blocks.append(self._make_decoder_block(in_channels, c))\n",
        "            in_channels = c\n",
        "\n",
        "        # Final output layer\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.Conv1d(channels[0], 1, kernel_size=7, padding=3),\n",
        "            nn.Tanh()  # Output in range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def _make_encoder_block(self, in_channels, out_channels):\n",
        "        \"\"\"Create a single encoder block\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def _make_decoder_block(self, in_channels, out_channels):\n",
        "        \"\"\"Create a single decoder block\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, audio, conditioning):\n",
        "        \"\"\"\n",
        "        Forward pass through UNet with conditioning\n",
        "        audio: [batch_size, channels, samples]\n",
        "        conditioning: [batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        # Store skip connections\n",
        "        skip_connections = []\n",
        "\n",
        "        # Encoder path\n",
        "        x = audio\n",
        "        for encoder in self.encoder_blocks:\n",
        "            x = encoder(x)\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Apply conditioning\n",
        "        cond = self.cond_projection(conditioning)  # [batch, channels[-1]]\n",
        "        cond = cond.unsqueeze(-1)  # [batch, channels[-1], 1]\n",
        "        cond = cond.expand(-1, -1, x.size(-1))  # [batch, channels[-1], time]\n",
        "\n",
        "        # Add conditioning (residual connection)\n",
        "        x = x + 0.3 * cond\n",
        "\n",
        "        # Decoder path with skip connections\n",
        "        for i, decoder in enumerate(self.decoder_blocks):\n",
        "            skip = skip_connections[-(i+1)]\n",
        "\n",
        "            # Ensure dimensions match before concatenating\n",
        "            if x.shape[2] != skip.shape[2]:\n",
        "                x = torch.nn.functional.interpolate(x, size=skip.shape[2], mode='linear')\n",
        "\n",
        "            x = decoder(x + 0.1 * skip)  # Residual connection with skip\n",
        "\n",
        "        # Final layer\n",
        "        output = self.final_layer(x)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "khN6pzMPIflP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uirTuJ0oKeKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo"
      ],
      "metadata": {
        "id": "VpN9XLc8Kehi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_musiclm_integration():\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize emotion components only (not the full music generation model)\n",
        "    emotion_extractor = EmotionExtractionModule(num_emotions=8)\n",
        "    emotion_mapper = EmotionToMusicMapper(emotion_dim=8, music_param_dim=16)\n",
        "\n",
        "    # Move to device\n",
        "    emotion_extractor = emotion_extractor.to(device)\n",
        "    emotion_mapper = emotion_mapper.to(device)\n",
        "\n",
        "    # Example literary text\n",
        "    literary_text = \"\"\"\n",
        "    The old mansion stood silent against the stormy sky. Inside, memories of\n",
        "    laughter and dance echoed through empty halls. A lone figure stood at the\n",
        "    window, watching raindrops trace patterns like tears upon the glass.\n",
        "    For years this place had been home, but tomorrow it would belong to strangers.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Analyzing text and generating MusicLM prompts...\")\n",
        "    print(f\"Input text: {literary_text[:100]}...\")\n",
        "\n",
        "    # Generate prompts for MusicLM\n",
        "    result = generate_music_with_musiclm(literary_text, emotion_extractor, emotion_mapper)\n",
        "\n",
        "    # Print the generated prompts\n",
        "    print(\"\\nGenerated MusicLM Prompts:\")\n",
        "    for i, prompt in enumerate(result[\"prompts\"]):\n",
        "        print(f\"\\nPrompt {i+1}:\")\n",
        "        print(prompt)\n",
        "\n",
        "    print(\"\\nThese prompts can now be used with a MusicLM model to generate the actual music.\")\n",
        "\n",
        "    # Print emotional analysis\n",
        "    print(\"\\nEmotional Analysis:\")\n",
        "    for i, score in enumerate(result[\"emotion_scores\"][0]):\n",
        "        print(f\"  {emotion_extractor.emotion_categories[i]}: {score.item():.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_musiclm_integration()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819,
          "referenced_widgets": [
            "f210aeac9f6549f88d6a3b28f8cbc493",
            "ea3e288a59c94c54b5d3ca25b8012d45",
            "8e035e331e7f4c278a335c91d47f5ff6",
            "b1217825f4c143c69a4aa52a430327d1",
            "f83f18a50be94734b9340dc3405d7920",
            "31d325b4185246f3a2b1ee6d8b93a585",
            "78f87d717ca44b49b913014b25bbb9ea",
            "6975867fd1494d88a256c2df8f66da52",
            "03883f5917dd4b198ad6dc94218f1fcb",
            "22831ca7fb32446e9febf7df17acdbb1",
            "857f2ad115f245cdb82bb0f4d0cc43c3",
            "359e7caf220543b69baf5da8a66db05a",
            "aabbba3c1cb142f0a2a553f79ff98b90",
            "c8f92721a5154b5ba3b05a265acde526",
            "545ae924017148a6ae71c0cb87cba88d",
            "39b81dad0f1e4a2ea10671b0bfee880d",
            "bf52fca7d4e44ba0b1305ce006b683ed",
            "82e3e69418cb49c980001e0890517f5b",
            "be5e390a73cd46849b2187eb943e2cef",
            "ac0b6944d72f4ea6a3f70d9c86fce0d4",
            "dfa9f238454c44879c1c3b6fdb0207dd",
            "9bc3539382094750b5c5c59bbcb4cd31",
            "56b45069c0d0432ab7171f474a89eaa0",
            "2ae9f54519484e4f950e79650e38fb98",
            "a291de3d111c40ecb80c7e285e18b4e0",
            "ca1372524e714a07ab86b198b68eb636",
            "ad466016c92f46249c616d6222564e78",
            "ced10c4cf1674469a6a5975df6775774",
            "d4deb2a378e54c01a09f1a4bd3a4d8ba",
            "8fa54f053f184447b240e5bd58a0b79e",
            "0d3ceb282a444b249ad266d118fbd513",
            "d9e849bc7c574493bc9ae5b825151686",
            "fcc2e212e0c8449cb116622850ee2bd5",
            "9a68348758c44c65b74d798935790b73",
            "a4e3da8c8d9e4846ab906c8b5232b290",
            "5a9228e0aa4a43aaa6d937ac17e4a3ac",
            "3aad45ab18d54bf1b249abd278c150f4",
            "41134b9b17cd4734921cf5a230d7ed76",
            "0272f2541de64e6f9d8c2a16c8437807",
            "ac855e2baf3640cca9cb1d5a5f1164a6",
            "5812c47ce2784515a567cbbb55b64379",
            "72d851d5c10b44a88b3ac86b231f4618",
            "25554aba16394cf3b17666ccab55731a",
            "f179b2c0c4fc4aa3af0ff1dff4eaad3a",
            "679349b2a5c6426297f3ddf9e05ee6bb",
            "b76eb146a27c4794ad8139ac8e121daa",
            "12fd4b50740047f298fd1a867e1c0a3b",
            "db371d010405414ebf44f3836be62727",
            "5e33d2928a944831bd6bf2b0c1c7e346",
            "49b0e59134b24579a0d268a4996b4311",
            "e702dc0e23064c7e8dbd853ee99d8e71",
            "73f5315b39c54c42a69d328ec34d5009",
            "980dd7d976494c0ea0a77b6fb2bb7df8",
            "ba4d3177243c49fd9f26e1d951fe3b32",
            "7648d12bd24543f5b2d788ffc0c66b71",
            "a957370d9e954bde98b35d7ad8486376",
            "793b4fd673a24d54940a3eb875a650aa",
            "c94eef18d3db41b9bbefb510eae32880",
            "412b7fa07bc34105b26511555eb0a1df",
            "e43387fab39647709e59fa4534b52d6a",
            "e4b7ba940ed74c86a557a4e23d0c446f",
            "6c463fdce3db45e5bf4245bdb5351394",
            "7478dbc7993c420e8ade013a243892c6",
            "2525f69b4b594521a9d8e7e973998a30",
            "3fa29fb89ac3457da5854262e2eb3c34",
            "d8a10e8ef6024a59bc9d5b9bda6eb605"
          ]
        },
        "id": "mpa9A5t3Ikwb",
        "outputId": "dcd3b6ab-cebb-427a-f73f-43e0bb2068f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f210aeac9f6549f88d6a3b28f8cbc493"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "359e7caf220543b69baf5da8a66db05a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56b45069c0d0432ab7171f474a89eaa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a68348758c44c65b74d798935790b73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "679349b2a5c6426297f3ddf9e05ee6bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a957370d9e954bde98b35d7ad8486376"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing text and generating MusicLM prompts...\n",
            "Input text: \n",
            "    The old mansion stood silent against the stormy sky. Inside, memories of\n",
            "    laughter and dance...\n",
            "Extracted emotions with shape: torch.Size([1, 8])\n",
            "Generated single prompt from emotion analysis\n",
            "\n",
            "Generated MusicLM Prompts:\n",
            "\n",
            "Prompt 1:\n",
            "A suspenseful, atmospheric piece in major key at 116 BPM, evoking feelings of joy and excitement and fear. Featuring full orchestra with prominent brass and percussion. \n",
            "\n",
            "These prompts can now be used with a MusicLM model to generate the actual music.\n",
            "\n",
            "Emotional Analysis:\n",
            "  joy: 0.515\n",
            "  sadness: 0.494\n",
            "  anger: 0.458\n",
            "  fear: 0.499\n",
            "  tenderness: 0.446\n",
            "  excitement: 0.501\n",
            "  calmness: 0.452\n",
            "  tension: 0.484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QdF46Lo3In04"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}