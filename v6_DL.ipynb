{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqHWbQrKhEaZ",
        "outputId": "9fd398b9-7f6c-4642-d502-9476793c5317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from IPython) (0.19.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch numpy librosa soundfile matplotlib IPython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp\n",
        "!apt-get install ffmpeg\n",
        "!pip install kagglehub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkAGd9NIJHVG",
        "outputId": "bc52c615-b5e1-4c33-d575-6eaf483ba537"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.3.31-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/172.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.3.31-py3-none-any.whl (3.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.3.31\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"googleai/musiccaps\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "NrEFZkOxJcnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import io\n",
        "\n",
        "class EmotionExtractionModule(nn.Module):\n",
        "    \"\"\"Module for extracting emotional information from literary text\"\"\"\n",
        "    def __init__(self, model_name=\"roberta-base\", num_emotions=8, device=None):\n",
        "        super(EmotionExtractionModule, self).__init__()\n",
        "\n",
        "        # Load pre-trained language model for emotion analysis\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "        self.model = RobertaModel.from_pretrained(model_name)\n",
        "\n",
        "        # Emotion classifier head\n",
        "        self.emotion_classifier = nn.Sequential(\n",
        "            nn.Linear(self.model.config.hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_emotions),\n",
        "            nn.Sigmoid()  # Multiple emotions can be present simultaneously\n",
        "        )\n",
        "\n",
        "        # Define the emotion categories\n",
        "        self.emotion_categories = [\n",
        "            \"joy\", \"sadness\", \"anger\", \"fear\",\n",
        "            \"tenderness\", \"excitement\", \"calmness\", \"tension\"\n",
        "        ]\n",
        "\n",
        "        # Sliding window parameters for analyzing longer texts\n",
        "        self.window_size = 512  # Max tokens per window\n",
        "        self.stride = 256  # Overlap between windows\n",
        "\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.emotion_classifier = self.emotion_classifier.to(self.device)\n",
        "\n",
        "    def forward(self, text, return_attention=False):\n",
        "        \"\"\"\n",
        "        Extract emotion scores from text\n",
        "        Returns emotion scores and attention weights for visualization\n",
        "        \"\"\"\n",
        "        # For longer texts, break into overlapping chunks\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "\n",
        "        all_emotion_scores = []\n",
        "        all_attention_weights = []\n",
        "\n",
        "        for t in text:\n",
        "            # Break long text into chunks with sliding window\n",
        "            chunks = self._create_text_chunks(t)\n",
        "            chunk_emotions = []\n",
        "            chunk_attentions = []\n",
        "\n",
        "            for chunk in chunks:\n",
        "                # Tokenize with attention mask\n",
        "                inputs = self.tokenizer(chunk, return_tensors=\"pt\",\n",
        "                                      padding=True, truncation=True, max_length=self.window_size)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "                # Get model outputs\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(**inputs, output_attentions=return_attention)\n",
        "\n",
        "                # Use [CLS] token representation for classification\n",
        "                sequence_output = outputs.last_hidden_state[:, 0, :]\n",
        "                emotions = self.emotion_classifier(sequence_output)\n",
        "\n",
        "                # Store results\n",
        "                chunk_emotions.append(emotions)\n",
        "\n",
        "                # Extract attention patterns only if requested\n",
        "                if return_attention:\n",
        "                    attentions = outputs.attentions[-1].mean(dim=1)  # Average over heads in last layer\n",
        "                    chunk_attentions.append(attentions)\n",
        "\n",
        "            # Combine emotions from chunks with temporal weighting\n",
        "            if len(chunk_emotions) > 1:\n",
        "                chunk_emotions = torch.cat(chunk_emotions, dim=0)\n",
        "                # Exponential weighting might work better for many chunks\n",
        "                weights = torch.exp(torch.linspace(0.0, 1.0, len(chunk_emotions))).to(chunk_emotions.device)\n",
        "                weights = weights / weights.sum()  # Normalize\n",
        "                weighted_emotions = chunk_emotions * weights.unsqueeze(1)\n",
        "                final_emotions = weighted_emotions.sum(dim=0, keepdim=True)\n",
        "            else:\n",
        "                final_emotions = chunk_emotions[0]\n",
        "\n",
        "            all_emotion_scores.append(final_emotions)\n",
        "            if return_attention:\n",
        "                all_attention_weights.append(chunk_attentions)\n",
        "\n",
        "        # Combine results from batch\n",
        "        emotion_scores = torch.cat(all_emotion_scores, dim=0)\n",
        "\n",
        "        result = {\n",
        "            \"emotion_scores\": emotion_scores,\n",
        "            \"emotion_categories\": self.emotion_categories\n",
        "        }\n",
        "\n",
        "        if return_attention:\n",
        "            result[\"attention_weights\"] = all_attention_weights\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _create_text_chunks(self, text):\n",
        "        \"\"\"Create overlapping chunks for long text analysis\"\"\"\n",
        "        # Quick tokenize to get token count (without padding/truncation)\n",
        "        tokens = self.tokenizer.encode(text)\n",
        "\n",
        "        if len(tokens) <= self.window_size:\n",
        "            return [text]  # Text fits in one window\n",
        "\n",
        "        # For longer texts, create overlapping chunks\n",
        "        chunks = []\n",
        "        words = text.split()\n",
        "\n",
        "        # Estimate words per window based on tokens\n",
        "        words_per_token = max(1, len(words) / len(tokens))\n",
        "        words_per_window = int(self.window_size * words_per_token * 0.9)  # 90% to be safe\n",
        "        stride_in_words = int(self.stride * words_per_token * 0.9)\n",
        "\n",
        "        # Create overlapping chunks\n",
        "        for i in range(0, len(words), stride_in_words):\n",
        "            chunk = \" \".join(words[i:i + words_per_window])\n",
        "            chunks.append(chunk)\n",
        "            # Stop if we've covered the whole text\n",
        "            if i + words_per_window >= len(words):\n",
        "                break\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def extract_emotional_arc(self, text, num_segments=10, return_attention=False):\n",
        "        \"\"\"\n",
        "        Extract emotional arc across text for narrative progression\n",
        "        Returns emotion scores for multiple segments of text\n",
        "        \"\"\"\n",
        "        # Create segments\n",
        "        if isinstance(text, str):\n",
        "            words = text.split()\n",
        "            segment_size = max(1, len(words) // num_segments)\n",
        "            segments = []\n",
        "\n",
        "            for i in range(0, len(words), segment_size):\n",
        "                segment = \" \".join(words[i:i + segment_size])\n",
        "                segments.append(segment)\n",
        "\n",
        "                # Stop if we've covered the whole text\n",
        "                if len(segments) >= num_segments:\n",
        "                    break\n",
        "        else:\n",
        "            segments = text  # Already a list of segments\n",
        "\n",
        "        # Process each segment\n",
        "        segment_emotions = []\n",
        "\n",
        "        for segment in segments:\n",
        "            result = self.forward([segment], return_attention=return_attention)\n",
        "            segment_emotions.append(result[\"emotion_scores\"][0])\n",
        "\n",
        "        # Stack emotions to create emotional arc [num_segments, num_emotions]\n",
        "        emotional_arc = torch.stack(segment_emotions)\n",
        "\n",
        "        return {\n",
        "            \"emotional_arc\": emotional_arc,\n",
        "            \"emotion_categories\": self.emotion_categories,\n",
        "            \"segments\": segments\n",
        "        }"
      ],
      "metadata": {
        "id": "GtkfCCNahKi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionToMusicMapper(nn.Module):\n",
        "    \"\"\"Maps emotional content to musical parameters\"\"\"\n",
        "    def __init__(self, emotion_dim=8, music_param_dim=16, device=None):\n",
        "        super(EmotionToMusicMapper, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.emotion_dim = emotion_dim\n",
        "        self.music_param_dim = music_param_dim\n",
        "\n",
        "        # Neural mapping from emotions to musical parameters\n",
        "        self.mapping_network = nn.Sequential(\n",
        "            nn.Linear(emotion_dim, 64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, music_param_dim)\n",
        "        )\n",
        "        self.mapping_network = self.mapping_network.to(self.device)\n",
        "\n",
        "        # Define musical parameter ranges\n",
        "        self.music_params = {\n",
        "            \"tempo\": {\"min\": 60, \"max\": 180},        # BPM\n",
        "            \"key\": {\"min\": 0, \"max\": 11},            # C=0, C#=1, etc.\n",
        "            \"mode\": {\"min\": 0, \"max\": 1},            # Major=0, Minor=1\n",
        "            \"rhythm_density\": {\"min\": 0.2, \"max\": 0.9},\n",
        "            \"note_duration\": {\"min\": 0.1, \"max\": 0.8},\n",
        "            \"articulation\": {\"min\": 0.2, \"max\": 0.9}, # Staccato to legato\n",
        "            \"dynamics\": {\"min\": 0.1, \"max\": 0.9},     # Soft to loud\n",
        "            \"timbre_brightness\": {\"min\": 0.1, \"max\": 0.9},\n",
        "            \"harmonic_complexity\": {\"min\": 0.1, \"max\": 0.9},\n",
        "            \"dissonance\": {\"min\": 0.0, \"max\": 0.7},\n",
        "            \"reverb\": {\"min\": 0.0, \"max\": 0.8},\n",
        "            \"stereo_width\": {\"min\": 0.3, \"max\": 1.0},\n",
        "            \"instrumentation\": {\"min\": 0, \"max\": 5},  # Different ensemble types\n",
        "            \"melodic_range\": {\"min\": 12, \"max\": 36},  # Range in semitones\n",
        "            \"bass_presence\": {\"min\": 0.1, \"max\": 0.9},\n",
        "            \"density\": {\"min\": 0.1, \"max\": 0.9}       # Sparse to dense\n",
        "        }\n",
        "\n",
        "        # Emotion to music parameter mapping rules (prior knowledge)\n",
        "        # These serve as biases for the neural mapping\n",
        "        self.emotion_music_rules = {\n",
        "            \"joy\": {\n",
        "                \"tempo\": 0.7,          # Faster tempo\n",
        "                \"mode\": 0.2,           # Tends toward major\n",
        "                \"dissonance\": 0.2,     # Low dissonance\n",
        "                \"dynamics\": 0.7        # Louder dynamics\n",
        "            },\n",
        "            \"sadness\": {\n",
        "                \"tempo\": 0.3,          # Slower tempo\n",
        "                \"mode\": 0.8,           # Tends toward minor\n",
        "                \"note_duration\": 0.7,  # Longer notes\n",
        "                \"reverb\": 0.7          # More reverb\n",
        "            },\n",
        "            \"anger\": {\n",
        "                \"tempo\": 0.8,          # Fast tempo\n",
        "                \"dissonance\": 0.7,     # High dissonance\n",
        "                \"dynamics\": 0.9,       # Loud dynamics\n",
        "                \"articulation\": 0.3    # More staccato\n",
        "            },\n",
        "            \"fear\": {\n",
        "                \"dissonance\": 0.6,     # Higher dissonance\n",
        "                \"dynamics\": 0.4,       # Varied dynamics\n",
        "                \"stereo_width\": 0.8    # Wide stereo field\n",
        "            },\n",
        "            \"tenderness\": {\n",
        "                \"tempo\": 0.4,          # Moderate-slow tempo\n",
        "                \"dynamics\": 0.3,       # Soft dynamics\n",
        "                \"articulation\": 0.8,   # Legato\n",
        "                \"harmonic_complexity\": 0.4  # Simple harmonies\n",
        "            },\n",
        "            \"excitement\": {\n",
        "                \"tempo\": 0.8,          # Fast tempo\n",
        "                \"rhythm_density\": 0.8, # Dense rhythms\n",
        "                \"dynamics\": 0.8        # Loud dynamics\n",
        "            },\n",
        "            \"calmness\": {\n",
        "                \"tempo\": 0.3,          # Slow tempo\n",
        "                \"dynamics\": 0.3,       # Soft dynamics\n",
        "                \"reverb\": 0.6,         # More reverb\n",
        "                \"dissonance\": 0.1      # Consonant harmonies\n",
        "            },\n",
        "            \"tension\": {\n",
        "                \"harmonic_complexity\": 0.7,  # Complex harmonies\n",
        "                \"dissonance\": 0.6,     # More dissonance\n",
        "                \"dynamics\": 0.5        # Varied dynamics\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def forward(self, emotion_scores):\n",
        "        \"\"\"\n",
        "        Map emotion scores to musical parameters\n",
        "        emotion_scores: [batch_size, emotion_dim] or [emotion_dim]\n",
        "        \"\"\"\n",
        "        # Ensure emotion_scores has batch dimension\n",
        "        if len(emotion_scores.shape) == 1:\n",
        "            emotion_scores = emotion_scores.unsqueeze(0)\n",
        "\n",
        "        # Make sure it's on the right device\n",
        "        emotion_scores = emotion_scores.to(self.device)\n",
        "\n",
        "        # Apply neural mapping\n",
        "        raw_params = self.mapping_network(emotion_scores)\n",
        "\n",
        "        # Apply prior knowledge as bias\n",
        "        music_params = self._apply_emotion_rules(emotion_scores, raw_params)\n",
        "\n",
        "        # Scale parameters to their defined ranges\n",
        "        scaled_params = self._scale_to_ranges(music_params)\n",
        "\n",
        "        return {\n",
        "            \"music_params\": music_params,\n",
        "            \"scaled_params\": scaled_params\n",
        "        }\n",
        "\n",
        "    def _apply_emotion_rules(self, emotion_scores, raw_params):\n",
        "        \"\"\"Apply emotion-music rules as biases to neural output\"\"\"\n",
        "        batch_size = emotion_scores.shape[0]\n",
        "        device = emotion_scores.device\n",
        "\n",
        "        # Initialize parameter tensor with neural output\n",
        "        music_params = raw_params.clone()\n",
        "\n",
        "        # Get the indices for each parameter in the output tensor\n",
        "        param_indices = {param: i for i, param in enumerate(self.music_params.keys())}\n",
        "\n",
        "        # Map emotion indices to names\n",
        "        emotion_names = [\"joy\", \"sadness\", \"anger\", \"fear\",\n",
        "                         \"tenderness\", \"excitement\", \"calmness\", \"tension\"]\n",
        "\n",
        "        # For each emotion, apply its rules based on strength\n",
        "        for b in range(batch_size):\n",
        "            for i, emotion in enumerate(emotion_names):\n",
        "                if i >= emotion_scores.shape[1]:\n",
        "                    continue  # Skip if index is out of bounds\n",
        "\n",
        "                # Get emotion strength (0 to 1)\n",
        "                emotion_strength = emotion_scores[b, i].item()\n",
        "\n",
        "                # Skip if emotion is not strongly present\n",
        "                if emotion_strength < 0.2:\n",
        "                    continue\n",
        "\n",
        "                # Apply each rule for this emotion\n",
        "                if emotion in self.emotion_music_rules:\n",
        "                    for param, value in self.emotion_music_rules[emotion].items():\n",
        "                        if param in param_indices:\n",
        "                            idx = param_indices[param]\n",
        "                            # Blend neural output with rule-based value based on emotion strength\n",
        "                            blend_factor = emotion_strength * 0.7  # Max 70% influence\n",
        "                            music_params[b, idx] = (1 - blend_factor) * music_params[b, idx] + blend_factor * value\n",
        "\n",
        "        return music_params\n",
        "\n",
        "    def _scale_to_ranges(self, music_params):\n",
        "        \"\"\"Scale normalized parameters to their actual ranges\"\"\"\n",
        "        batch_size = music_params.shape[0]\n",
        "        scaled_params = {}\n",
        "\n",
        "        for i, (param, range_info) in enumerate(self.music_params.items()):\n",
        "            # Get parameter values (clamped to 0-1)\n",
        "            values = torch.clamp(music_params[:, i], 0.0, 1.0)\n",
        "\n",
        "            # Scale to actual range\n",
        "            min_val, max_val = range_info[\"min\"], range_info[\"max\"]\n",
        "            scaled = min_val + values * (max_val - min_val)\n",
        "\n",
        "            # Special handling for discrete parameters\n",
        "            if param in [\"key\", \"instrumentation\"]:\n",
        "                scaled = scaled.round()\n",
        "\n",
        "            scaled_params[param] = scaled\n",
        "\n",
        "        return scaled_params\n",
        "\n",
        "    def params_to_emotions(self, music_params):\n",
        "        \"\"\"Convert music parameters back to emotion space (approximate inverse mapping)\"\"\"\n",
        "        # Simple linear projection back to emotion space\n",
        "        inverse_mapping = nn.Linear(self.music_param_dim, self.emotion_dim).to(self.device)\n",
        "        emotions = torch.sigmoid(inverse_mapping(music_params))\n",
        "        return emotions\n",
        "\n",
        "    def generate_musiclm_prompt(self, emotion_scores, emotion_categories):\n",
        "        \"\"\"\n",
        "        Generate a detailed MusicLM-compatible prompt based on emotion analysis\n",
        "        emotion_scores: tensor of emotion scores\n",
        "        emotion_categories: list of emotion category names\n",
        "        \"\"\"\n",
        "        # Ensure emotion_scores has batch dimension and is on CPU for processing\n",
        "        if len(emotion_scores.shape) == 1:\n",
        "            emotion_scores = emotion_scores.unsqueeze(0)\n",
        "\n",
        "        emotion_scores = emotion_scores.detach().cpu()\n",
        "\n",
        "        # Get music parameters for these emotions\n",
        "        music_params = self.forward(emotion_scores)\n",
        "        scaled_params = music_params[\"scaled_params\"]\n",
        "\n",
        "        # Build prompt\n",
        "        prompts = []\n",
        "\n",
        "        for b in range(emotion_scores.shape[0]):\n",
        "            # Get top emotions\n",
        "            if emotion_scores.shape[1] <= len(emotion_categories):\n",
        "                emotions_data = [(emotion_categories[i], emotion_scores[b, i].item())\n",
        "                                for i in range(emotion_scores.shape[1])]\n",
        "            else:\n",
        "                emotions_data = [(f\"Emotion {i}\", emotion_scores[b, i].item())\n",
        "                                for i in range(emotion_scores.shape[1])]\n",
        "\n",
        "            # Sort emotions by strength\n",
        "            emotions_data.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Get top 3 emotions\n",
        "            top_emotions = [e for e, s in emotions_data if s > 0.2][:3]\n",
        "\n",
        "            # Get key musical parameters\n",
        "            tempo = scaled_params[\"tempo\"][b].item()\n",
        "\n",
        "            # Determine mode (major/minor)\n",
        "            mode = \"minor\" if scaled_params[\"mode\"][b].item() > 0.5 else \"major\"\n",
        "\n",
        "            # Determine instrumentation based on emotions\n",
        "            instruments = self._choose_instrumentation(top_emotions)\n",
        "\n",
        "            # Determine musical style based on emotions and parameters\n",
        "            style = self._choose_style(top_emotions, scaled_params, b)\n",
        "\n",
        "            # Build descriptive prompt\n",
        "            emotion_desc = \" and \".join(top_emotions) if top_emotions else \"neutral\"\n",
        "\n",
        "            prompt = f\"A {style} piece in {mode} key at {tempo:.0f} BPM, evoking feelings of {emotion_desc}. \"\n",
        "            prompt += f\"Featuring {instruments}. \"\n",
        "\n",
        "            # Add specifics based on parameters\n",
        "            if scaled_params[\"harmonic_complexity\"][b].item() > 0.7:\n",
        "                prompt += \"Complex harmonies with unexpected chord changes. \"\n",
        "            elif scaled_params[\"harmonic_complexity\"][b].item() < 0.3:\n",
        "                prompt += \"Simple, consonant harmonies. \"\n",
        "\n",
        "            if scaled_params[\"rhythm_density\"][b].item() > 0.7:\n",
        "                prompt += \"Dense, intricate rhythms. \"\n",
        "            elif scaled_params[\"rhythm_density\"][b].item() < 0.3:\n",
        "                prompt += \"Sparse, spacious rhythms. \"\n",
        "\n",
        "            if scaled_params[\"dynamics\"][b].item() > 0.7:\n",
        "                prompt += \"Dramatic dynamic range with powerful crescendos. \"\n",
        "            elif scaled_params[\"dynamics\"][b].item() < 0.3:\n",
        "                prompt += \"Gentle, subtle dynamics. \"\n",
        "\n",
        "            if scaled_params[\"reverb\"][b].item() > 0.6:\n",
        "                prompt += \"Immersive, spacious reverb. \"\n",
        "\n",
        "            prompts.append(prompt)\n",
        "\n",
        "        return prompts\n",
        "\n",
        "    def _choose_instrumentation(self, emotions):\n",
        "        \"\"\"Choose appropriate instrumentation based on emotions\"\"\"\n",
        "        if any(e in [\"sadness\", \"tenderness\", \"calmness\"] for e in emotions):\n",
        "            return \"piano and strings with subtle woodwinds\"\n",
        "        elif any(e in [\"joy\", \"excitement\"] for e in emotions):\n",
        "            return \"full orchestra with prominent brass and percussion\"\n",
        "        elif any(e in [\"anger\", \"tension\"] for e in emotions):\n",
        "            return \"distorted electric guitars, heavy percussion, and synthesizers\"\n",
        "        elif any(e in [\"fear\"] for e in emotions):\n",
        "            return \"dissonant strings, prepared piano, and electronic elements\"\n",
        "        else:\n",
        "            return \"chamber ensemble with piano, strings, and woodwinds\"\n",
        "\n",
        "    def _choose_style(self, emotions, scaled_params, batch_idx):\n",
        "        \"\"\"Choose musical style based on emotions and parameters\"\"\"\n",
        "        tempo = scaled_params[\"tempo\"][batch_idx].item()\n",
        "        harmonic_complexity = scaled_params[\"harmonic_complexity\"][batch_idx].item()\n",
        "\n",
        "        if any(e in [\"sadness\", \"tenderness\"] for e in emotions) and tempo < 100:\n",
        "            return \"melancholic, cinematic\"\n",
        "        elif any(e in [\"joy\", \"excitement\"] for e in emotions) and tempo > 120:\n",
        "            return \"uplifting, energetic\"\n",
        "        elif any(e in [\"fear\", \"tension\"] for e in emotions):\n",
        "            return \"suspenseful, atmospheric\"\n",
        "        elif any(e in [\"calmness\"] for e in emotions):\n",
        "            return \"ambient, peaceful\"\n",
        "        elif any(e in [\"anger\"] for e in emotions):\n",
        "            return \"intense, dramatic\"\n",
        "        elif harmonic_complexity > 0.6:\n",
        "            return \"complex, avant-garde\"\n",
        "        else:\n",
        "            return \"melodic, contemporary\"\n"
      ],
      "metadata": {
        "id": "nhWKyBTwiTU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalCoherenceModel(nn.Module):\n",
        "    \"\"\"Model for ensuring temporal coherence in music generation\"\"\"\n",
        "    def __init__(self, music_param_dim=16, hidden_dim=128, num_layers=2, device=None):\n",
        "        super(TemporalCoherenceModel, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.music_param_dim = music_param_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # LSTM for modeling parameter sequences\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=music_param_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.1 if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Output layer to predict next parameters\n",
        "        self.output_layer = nn.Linear(hidden_dim, music_param_dim)\n",
        "\n",
        "        # Layer for embedding text condition into the LSTM\n",
        "        self.text_condition_layer = nn.Linear(512, hidden_dim)  # Assuming text_embedding_dim=512\n",
        "\n",
        "        # Add segment embedding layer\n",
        "        self.segment_condition_layer = nn.Linear(512, hidden_dim)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, param_sequence, text_embedding=None, segment_embedding=None):\n",
        "        \"\"\"\n",
        "        Process a sequence of musical parameters with optional text condition\n",
        "        param_sequence: [batch_size, seq_length, music_param_dim]\n",
        "        text_embedding: [batch_size, text_embedding_dim]\n",
        "        \"\"\"\n",
        "        batch_size, seq_length = param_sequence.shape[0], param_sequence.shape[1]\n",
        "\n",
        "        # Initialize hidden state, optionally with text condition\n",
        "        h0 = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_dim).to(param_sequence.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_dim).to(param_sequence.device)\n",
        "\n",
        "        if text_embedding is not None:\n",
        "            text_hidden = self.text_condition_layer(text_embedding)\n",
        "            h0[0] = text_hidden\n",
        "\n",
        "        if segment_embedding is not None:\n",
        "            segment_hidden = self.segment_condition_layer(segment_embedding)\n",
        "            # Combine with text embedding or use in second layer\n",
        "            if text_embedding is not None:\n",
        "                h0[0] = 0.7 * h0[0] + 0.3 * segment_hidden  # Weighted combination\n",
        "            else:\n",
        "                h0[0] = segment_hidden\n",
        "\n",
        "        # Run LSTM\n",
        "        lstm_out, (hn, cn) = self.lstm(param_sequence, (h0, c0))\n",
        "\n",
        "        # Project to output parameter space\n",
        "        output_sequence = self.output_layer(lstm_out)\n",
        "\n",
        "        return output_sequence, (hn, cn)\n",
        "\n",
        "    def generate_sequence(self, initial_params, sequence_length, text_embedding=None, smoothing=0.1):\n",
        "        \"\"\"\n",
        "        Generate a coherent sequence of musical parameters\n",
        "        initial_params: [batch_size, music_param_dim]\n",
        "        sequence_length: number of steps to generate\n",
        "        text_embedding: optional conditioning embedding\n",
        "        smoothing: parameter to control smoothness between steps\n",
        "        \"\"\"\n",
        "        # Store current training state and set to eval mode\n",
        "        was_training = self.training\n",
        "        self.eval()\n",
        "\n",
        "        try:\n",
        "            batch_size = initial_params.shape[0]\n",
        "            device = initial_params.device\n",
        "\n",
        "            # Initialize sequence with initial parameters\n",
        "            generated_sequence = [initial_params]\n",
        "\n",
        "            # Initialize hidden state with text embedding if provided\n",
        "            h = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "            c = torch.zeros(self.lstm.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "\n",
        "            if text_embedding is not None:\n",
        "                # Project text embedding to hidden dimension\n",
        "                text_hidden = self.text_condition_layer(text_embedding)\n",
        "\n",
        "                # Use text as initial hidden state in first layer\n",
        "                h[0] = text_hidden\n",
        "\n",
        "            # Generate sequence autoregressively\n",
        "            current_params = initial_params\n",
        "\n",
        "            for _ in range(sequence_length - 1):\n",
        "                # Reshape for LSTM (add sequence dimension)\n",
        "                params_input = current_params.unsqueeze(1)\n",
        "\n",
        "                # Get prediction and update hidden state\n",
        "                with torch.no_grad():\n",
        "                    lstm_out, (h, c) = self.lstm(params_input, (h, c))\n",
        "                    next_params = self.output_layer(lstm_out[:, -1, :])\n",
        "\n",
        "                # Apply smoothing if requested\n",
        "                if smoothing > 0:\n",
        "                    next_params = (1 - smoothing) * next_params + smoothing * current_params\n",
        "\n",
        "                # Add to sequence\n",
        "                generated_sequence.append(next_params)\n",
        "\n",
        "                # Update current parameters\n",
        "                current_params = next_params\n",
        "\n",
        "            # Stack into a single tensor [batch_size, sequence_length, music_param_dim]\n",
        "            return torch.stack(generated_sequence, dim=1)\n",
        "        finally:\n",
        "            # Restore previous training state\n",
        "            self.train(was_training)\n"
      ],
      "metadata": {
        "id": "hzjOXaLdiYgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MuLANTextEncoder(nn.Module):\n",
        "    \"\"\"Text encoder for music generation based on MuLAN architecture\"\"\"\n",
        "    def __init__(self, embedding_dim=512, model_name=\"roberta-base\", device=None):\n",
        "        super(MuLANTextEncoder, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Use RoBERTa as base model\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "        self.text_model = RobertaModel.from_pretrained(model_name)\n",
        "        self.text_model = self.text_model.to(self.device)\n",
        "\n",
        "        # Project to specified embedding dimension\n",
        "        self.projection = nn.Linear(self.text_model.config.hidden_size, embedding_dim)\n",
        "        self.projection = self.projection.to(self.device)\n",
        "\n",
        "    def forward(self, text):\n",
        "        \"\"\"\n",
        "        Encode text into a fixed-size embedding\n",
        "        text: string or list of strings\n",
        "        \"\"\"\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "\n",
        "        # Tokenize text\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Get text embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = self.text_model(**inputs)\n",
        "\n",
        "        # Use [CLS] token embedding as text representation\n",
        "        text_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Project to target dimension\n",
        "        projected_embedding = self.projection(text_embedding)\n",
        "\n",
        "        return projected_embedding\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    \"\"\"Vector Quantization layer for SoundStream\"\"\"\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_weight=0.25):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.commitment_weight = commitment_weight\n",
        "\n",
        "        # Codebook\n",
        "        self.codebook = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.codebook.weight.data.uniform_(-1.0 / num_embeddings, 1.0 / num_embeddings)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Flatten input\n",
        "        flat_z = z.reshape(-1, self.embedding_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = torch.sum(flat_z ** 2, dim=1, keepdim=True) + \\\n",
        "                    torch.sum(self.codebook.weight ** 2, dim=1) - \\\n",
        "                    2 * torch.matmul(flat_z, self.codebook.weight.t())\n",
        "\n",
        "        # Find nearest codebook entries\n",
        "        encoding_indices = torch.argmin(distances, dim=1)\n",
        "\n",
        "        # Get quantized latent vectors\n",
        "        z_q = self.codebook(encoding_indices).reshape(z.shape)\n",
        "\n",
        "        # Compute loss\n",
        "        commitment_loss = torch.nn.functional.mse_loss(z_q.detach(), z)\n",
        "        codebook_loss = torch.nn.functional.mse_loss(z_q, z.detach())\n",
        "        loss = codebook_loss + self.commitment_weight * commitment_loss\n",
        "\n",
        "        # Straight-through estimator\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # Calculate perplexity (used to monitor utilization of codebook)\n",
        "        avg_probs = torch.zeros(self.num_embeddings).to(z.device)\n",
        "        avg_probs.scatter_add_(0, encoding_indices, torch.ones_like(encoding_indices, dtype=torch.float))\n",
        "        avg_probs = avg_probs / encoding_indices.size(0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        return z_q, loss, perplexity, encoding_indices\n",
        "\n",
        "class SoundStreamEncoder(nn.Module):\n",
        "    \"\"\"Enhanced audio encoder based on SoundStream architecture\"\"\"\n",
        "    def __init__(self, embedding_dim=512, device=None):\n",
        "        super(SoundStreamEncoder, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Convolutional layers with residual connections\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv1d(1, 32, kernel_size=7, stride=1, padding=3),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            self._make_res_block(32, 32),\n",
        "            nn.Conv1d(32, 64, kernel_size=7, stride=2, padding=3),  # Downsampling\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            self._make_res_block(64, 64),\n",
        "            nn.Conv1d(64, 128, kernel_size=7, stride=2, padding=3),  # Downsampling\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            self._make_res_block(128, 128),\n",
        "            nn.Conv1d(128, 256, kernel_size=7, stride=2, padding=3),  # Downsampling\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            self._make_res_block(256, 256),\n",
        "            nn.Conv1d(256, embedding_dim, kernel_size=7, stride=2, padding=3),  # Downsampling\n",
        "            nn.BatchNorm1d(embedding_dim),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            self._make_res_block(embedding_dim, embedding_dim)\n",
        "        ).to(self.device)\n",
        "\n",
        "    def _make_res_block(self, channels, out_channels):\n",
        "        \"\"\"Create a residual block\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            # Skip connection\n",
        "            nn.Conv1d(channels, out_channels, kernel_size=1) if channels != out_channels else nn.Identity()\n",
        "        )\n",
        "\n",
        "    def forward(self, audio):\n",
        "        \"\"\"\n",
        "        Encode audio into a latent representation\n",
        "        audio: [batch_size, channels, samples]\n",
        "        \"\"\"\n",
        "        return self.conv_layers(audio)\n",
        "\n",
        "class SoundStreamDecoder(nn.Module):\n",
        "    \"\"\"Enhanced audio decoder based on SoundStream architecture\"\"\"\n",
        "    def __init__(self, embedding_dim=512, device=None):\n",
        "        super(SoundStreamDecoder, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Transposed convolutional layers with residual connections\n",
        "        self.deconv_layers = nn.Sequential(\n",
        "            self._make_res_block(embedding_dim, embedding_dim),\n",
        "\n",
        "            nn.ConvTranspose1d(embedding_dim, 256, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            self._make_res_block(256, 256),\n",
        "\n",
        "            nn.ConvTranspose1d(256, 128, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            self._make_res_block(128, 128),\n",
        "\n",
        "            nn.ConvTranspose1d(128, 64, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            self._make_res_block(64, 64),\n",
        "\n",
        "            nn.ConvTranspose1d(64, 32, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            self._make_res_block(32, 32),\n",
        "\n",
        "            nn.ConvTranspose1d(32, 1, kernel_size=7, stride=1, padding=3),\n",
        "            nn.Tanh()  # Output in range [-1, 1]\n",
        "        ).to(self.device)\n",
        "\n",
        "    def _make_res_block(self, channels, out_channels):\n",
        "        \"\"\"Create a residual block\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            # Skip connection\n",
        "            nn.Conv1d(channels, out_channels, kernel_size=1) if channels != out_channels else nn.Identity()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, length=16000):\n",
        "        \"\"\"\n",
        "        Decode latent representation to audio waveform\n",
        "        z: [batch_size, embedding_dim, latent_length]\n",
        "        length: target audio length\n",
        "        \"\"\"\n",
        "        # Apply transposed convolutions\n",
        "        audio = self.deconv_layers(z)\n",
        "\n",
        "        # Ensure output has the right length\n",
        "        if audio.shape[2] < length:\n",
        "            # Pad if too short\n",
        "            padding = torch.zeros(audio.shape[0], audio.shape[1], length - audio.shape[2],\n",
        "                                 device=audio.device)\n",
        "            audio = torch.cat([audio, padding], dim=2)\n",
        "        elif audio.shape[2] > length:\n",
        "            # Truncate if too long\n",
        "            audio = audio[:, :, :length]\n",
        "\n",
        "        return audio\n",
        "\n",
        "class SoundStream(nn.Module):\n",
        "    \"\"\"Complete SoundStream implementation with encoder and decoder\"\"\"\n",
        "    def __init__(self, embedding_dim=512, codebook_size=1024, commitment_weight=0.25, device=None):\n",
        "        super(SoundStream, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Encoder and decoder\n",
        "        self.encoder = SoundStreamEncoder(embedding_dim=embedding_dim, device=self.device)\n",
        "        self.decoder = SoundStreamDecoder(embedding_dim=embedding_dim, device=self.device)\n",
        "\n",
        "        # Vector quantization layer\n",
        "        self.quantizer = VectorQuantizer(\n",
        "            num_embeddings=codebook_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            commitment_weight=commitment_weight\n",
        "        ).to(self.device)\n",
        "\n",
        "    def forward(self, audio, conditioning=None):\n",
        "        \"\"\"\n",
        "        Forward pass through SoundStream\n",
        "        audio: input audio waveform\n",
        "        conditioning: optional conditioning from MuLAN and w2v-BERT\n",
        "        \"\"\"\n",
        "        # Encode audio to latent space\n",
        "        z = self.encoder(audio)\n",
        "\n",
        "        # Apply conditioning if provided\n",
        "        if conditioning is not None:\n",
        "            # Simple residual connection for conditioning\n",
        "            conditioning = conditioning.unsqueeze(2).expand(-1, -1, z.size(2))\n",
        "            z = z + 0.1 * conditioning\n",
        "\n",
        "        # Vector quantization\n",
        "        quantized, commitment_loss, perplexity, indices = self.quantizer(z)\n",
        "\n",
        "        # Decode back to audio\n",
        "        reconstructed = self.decoder(quantized)\n",
        "\n",
        "        return {\n",
        "            'reconstructed': reconstructed,\n",
        "            'quantized': quantized,\n",
        "            'commitment_loss': commitment_loss,\n",
        "            'perplexity': perplexity,\n",
        "            'indices': indices\n",
        "        }\n",
        "\n",
        "    def encode(self, audio):\n",
        "        \"\"\"Encode audio to discrete tokens\"\"\"\n",
        "        z = self.encoder(audio)\n",
        "        quantized, _, _, indices = self.quantizer(z)\n",
        "        return quantized, indices\n",
        "\n",
        "    def decode(self, quantized):\n",
        "        \"\"\"Decode quantized representation to audio\"\"\"\n",
        "        return self.decoder(quantized)\n"
      ],
      "metadata": {
        "id": "bjb4y7bRi2E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicLM(nn.Module):\n",
        "    \"\"\"Complete MusicLM architecture integrating MuLAN, w2v-BERT, and SoundStream\"\"\"\n",
        "    def __init__(self, device=None):\n",
        "        super(MusicLM, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # MuLAN for text-audio joint embedding\n",
        "        self.mulan = MuLAN(embedding_dim=512, device=self.device)\n",
        "\n",
        "        # w2v-BERT for semantic token generation\n",
        "        self.w2v_bert = W2vBERT(input_dim=512, hidden_dim=768, device=self.device)\n",
        "\n",
        "        # SoundStream for audio encoding/decoding\n",
        "        self.soundstream = SoundStream(embedding_dim=512, device=self.device)\n",
        "\n",
        "        # Transformer for generating semantic tokens conditioned on MuLAN embeddings\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=512,\n",
        "            nhead=8,\n",
        "            dim_feedforward=2048,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.semantic_transformer = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=6\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Projection for initial sequence\n",
        "        self.initial_projection = nn.Linear(512, 512).to(self.device)\n",
        "\n",
        "    def forward(self, text=None, audio=None, training=True):\n",
        "        \"\"\"\n",
        "        Forward pass through MusicLM\n",
        "        text: input text prompt\n",
        "        audio: input audio for training\n",
        "        training: whether in training mode\n",
        "        \"\"\"\n",
        "        # Get MuLAN embeddings\n",
        "        if text is not None:\n",
        "            mulan_embedding = self.mulan.get_text_latents(text)\n",
        "        elif audio is not None:\n",
        "            mulan_embedding = self.mulan.get_audio_latents(audio)\n",
        "        else:\n",
        "            raise ValueError(\"Either text or audio must be provided\")\n",
        "\n",
        "        if training and audio is not None:\n",
        "            # During training, encode the target audio\n",
        "            z, indices = self.soundstream.encode(audio)\n",
        "\n",
        "            # Create sequence for w2v-BERT\n",
        "            seq_length = z.size(2)\n",
        "            sequence = torch.zeros(z.size(0), seq_length, 512).to(self.device)\n",
        "\n",
        "            # Generate semantic tokens with w2v-BERT\n",
        "            semantic_tokens = self.w2v_bert(sequence, conditioning=mulan_embedding)\n",
        "\n",
        "            # Compute reconstruction loss\n",
        "            output = self.soundstream(audio, conditioning=semantic_tokens.mean(dim=1))\n",
        "            reconstruction_loss = torch.nn.functional.mse_loss(output['reconstructed'], audio)\n",
        "\n",
        "            return {\n",
        "                'semantic_tokens': semantic_tokens,\n",
        "                'reconstructed_audio': output['reconstructed'],\n",
        "                'reconstruction_loss': reconstruction_loss,\n",
        "                'commitment_loss': output['commitment_loss']\n",
        "            }\n",
        "        else:\n",
        "            # During inference, generate audio from text\n",
        "            # Project MuLAN embedding to initial sequence\n",
        "            initial_seq = self.initial_projection(mulan_embedding).unsqueeze(1)\n",
        "\n",
        "            # Expand to desired length (e.g., 10 seconds at 25 tokens/sec = 250 tokens)\n",
        "            seq_length = 250\n",
        "            expanded_seq = initial_seq.expand(-1, seq_length, -1)\n",
        "\n",
        "            # Generate semantic tokens with transformer\n",
        "            semantic_tokens = self.semantic_transformer(expanded_seq)\n",
        "\n",
        "            # Generate audio with SoundStream\n",
        "            audio_tokens = semantic_tokens.mean(dim=1, keepdim=True).expand(-1, 512, -1).transpose(1, 2)\n",
        "            generated_audio = self.soundstream.decode(audio_tokens)\n",
        "\n",
        "            return {\n",
        "                'generated_audio': generated_audio,\n",
        "                'semantic_tokens': semantic_tokens\n",
        "            }\n",
        "\n",
        "    def generate(self, prompt, duration=10.0, sample_rate=22050):\n",
        "        \"\"\"Generate music from text prompt\"\"\"\n",
        "        # Process text through MuLAN\n",
        "        mulan_embedding = self.mulan.get_text_latents([prompt])\n",
        "\n",
        "        # Project to initial sequence\n",
        "        initial_seq = self.initial_projection(mulan_embedding).unsqueeze(1)\n",
        "\n",
        "        # Expand to desired length\n",
        "        seq_length = int(duration * 25)  # 25 tokens per second\n",
        "        expanded_seq = initial_seq.expand(-1, seq_length, -1)\n",
        "\n",
        "        # Generate semantic tokens\n",
        "        with torch.no_grad():\n",
        "            semantic_tokens = self.semantic_transformer(expanded_seq)\n",
        "\n",
        "            # Generate audio with SoundStream\n",
        "            audio_tokens = semantic_tokens.mean(dim=1, keepdim=True).expand(-1, 512, -1).transpose(1, 2)\n",
        "            generated_audio = self.soundstream.decode(audio_tokens)\n",
        "\n",
        "        # Convert to numpy array\n",
        "        audio_np = generated_audio.squeeze().cpu().numpy()\n",
        "\n",
        "        return audio_np, sample_rate\n"
      ],
      "metadata": {
        "id": "0CoZAs9-IYWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def load_musiccaps_dataset():\n",
        "    \"\"\"Download and load the MusicCaps dataset using kagglehub\"\"\"\n",
        "    # Download latest version\n",
        "    path = kagglehub.dataset_download(\"googleai/musiccaps\")\n",
        "\n",
        "    print(\"Path to dataset files:\", path)\n",
        "\n",
        "    # The dataset contains a CSV file with metadata\n",
        "    csv_path = os.path.join(path, \"musiccaps-public.csv\")\n",
        "\n",
        "    # Load the CSV into a pandas DataFrame\n",
        "    musiccaps_df = pd.read_csv(csv_path)\n",
        "\n",
        "    print(f\"Loaded MusicCaps dataset with {len(musiccaps_df)} entries\")\n",
        "\n",
        "    return musiccaps_df, path\n"
      ],
      "metadata": {
        "id": "el3ZYPBCKVqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_musiccaps_dataset(audio_dir=None):\n",
        "    \"\"\"Create a dataset from MusicCaps using kagglehub\"\"\"\n",
        "    # Load the dataset using kagglehub\n",
        "    musiccaps_df, dataset_path = load_musiccaps_dataset()\n",
        "\n",
        "    class MusicCapsDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, df, audio_dir=None):\n",
        "            self.df = df\n",
        "            self.audio_dir = audio_dir\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.df)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            row = self.df.iloc[idx]\n",
        "\n",
        "            # Get text caption\n",
        "            caption = row['caption']\n",
        "\n",
        "            # Get audio - in a real implementation, you would load from YouTube\n",
        "            # using the YTID, start_sec, and end_sec fields\n",
        "            if self.audio_dir:\n",
        "                audio_path = os.path.join(self.audio_dir, f\"{row['ytid']}_{row['start_s']}_{row['end_s']}.wav\")\n",
        "                if os.path.exists(audio_path):\n",
        "                    audio, sr = librosa.load(audio_path, sr=22050, mono=True)\n",
        "                    # Ensure 10 seconds of audio\n",
        "                    if len(audio) < 10 * sr:\n",
        "                        audio = np.pad(audio, (0, 10 * sr - len(audio)))\n",
        "                    else:\n",
        "                        audio = audio[:10 * sr]\n",
        "                else:\n",
        "                    # If file doesn't exist, generate dummy audio\n",
        "                    audio = np.zeros(10 * 22050)\n",
        "                    sr = 22050\n",
        "            else:\n",
        "                # If no audio directory, generate dummy audio\n",
        "                audio = np.zeros(10 * 22050)\n",
        "                sr = 22050\n",
        "\n",
        "            return {\n",
        "                'caption': caption,\n",
        "                'audio': torch.FloatTensor(audio).unsqueeze(0),  # Add channel dimension\n",
        "                'ytid': row['ytid'],\n",
        "                'start_sec': row['start_s'],\n",
        "                'end_sec': row['end_s']\n",
        "            }\n",
        "\n",
        "    return MusicCapsDataset(musiccaps_df, audio_dir)\n",
        "\n",
        "\n",
        "def train_musiclm(model, epochs=10, batch_size=16, learning_rate=1e-4, checkpoint_dir='checkpoints', audio_dir=None):\n",
        "    \"\"\"Train MusicLM model using MusicCaps dataset\"\"\"\n",
        "    import os\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Create dataset using kagglehub\n",
        "    dataset = create_musiccaps_dataset(audio_dir)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            # Get data\n",
        "            captions = batch['caption']\n",
        "            audio = batch['audio'].to(model.device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            output = model(text=captions, audio=audio, training=True)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = output['reconstruction_loss'] + output['commitment_loss']\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print epoch statistics\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save(model.state_dict(), os.path.join(checkpoint_dir, f\"musiclm_checkpoint_epoch_{epoch+1}.pt\"))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "FXwwVV2fIhZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_musiclm_with_kagglehub(epochs=10, batch_size=16, learning_rate=1e-4, download_audio=False):\n",
        "    \"\"\"Complete training pipeline using kagglehub for dataset access\"\"\"\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Initialize MusicLM model\n",
        "    model = initialize_musiclm_model(device)\n",
        "\n",
        "    # Download audio if requested\n",
        "    audio_dir = None\n",
        "    if download_audio:\n",
        "        print(\"Downloading audio clips from YouTube...\")\n",
        "        audio_dir = download_musiccaps_audio(\"musiccaps_audio\")\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"Training MusicLM model on {device}...\")\n",
        "    trained_model = train_musiclm(\n",
        "        model,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        audio_dir=audio_dir\n",
        "    )\n",
        "\n",
        "    return trained_model\n"
      ],
      "metadata": {
        "id": "-ItFDWTqK1pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_musiccaps_audio(output_dir):\n",
        "    \"\"\"Download audio clips from MusicCaps dataset\"\"\"\n",
        "    # Load the dataset using kagglehub\n",
        "    musiccaps_df, _ = load_musiccaps_dataset()\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Download each clip\n",
        "    for idx, row in tqdm(musiccaps_df.iterrows(), total=len(musiccaps_df), desc=\"Downloading audio\"):\n",
        "        ytid = row['ytid']\n",
        "        start_sec = row['start_s']\n",
        "        end_sec = row['end_s']\n",
        "\n",
        "        # Skip if already downloaded\n",
        "        output_path = os.path.join(output_dir, f\"{ytid}_{start_sec}_{end_sec}.wav\")\n",
        "        if os.path.exists(output_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Download full video audio\n",
        "            temp_file = os.path.join(output_dir, f\"{ytid}_temp.wav\")\n",
        "            youtube_url = f\"https://www.youtube.com/watch?v={ytid}\"\n",
        "\n",
        "            # Use yt-dlp (more reliable than youtube-dl)\n",
        "            cmd = [\n",
        "                \"yt-dlp\",\n",
        "                \"-x\",\n",
        "                \"--audio-format\", \"wav\",\n",
        "                \"--audio-quality\", \"0\",\n",
        "                \"-o\", temp_file,\n",
        "                youtube_url\n",
        "            ]\n",
        "\n",
        "            subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "            # Extract the specific segment using ffmpeg\n",
        "            duration = end_sec - start_sec\n",
        "            extract_cmd = [\n",
        "                \"ffmpeg\",\n",
        "                \"-i\", temp_file,\n",
        "                \"-ss\", str(start_sec),\n",
        "                \"-t\", str(duration),\n",
        "                \"-c:a\", \"pcm_s16le\",\n",
        "                \"-ar\", \"22050\",\n",
        "                \"-ac\", \"1\",\n",
        "                output_path,\n",
        "                \"-y\"\n",
        "            ]\n",
        "\n",
        "            subprocess.run(extract_cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "            # Remove temp file\n",
        "            if os.path.exists(temp_file):\n",
        "                os.remove(temp_file)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {ytid}: {e}\")\n",
        "\n",
        "    return output_dir\n"
      ],
      "metadata": {
        "id": "X0QdQn41InZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class MuLAN(nn.Module):\n",
        "    \"\"\"Complete MuLAN architecture with dual towers for text and audio\"\"\"\n",
        "    def __init__(self, embedding_dim=512, device=None):\n",
        "        super(MuLAN, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Text tower - using RoBERTa\n",
        "        self.text_encoder = MuLANTextEncoder(embedding_dim=embedding_dim, device=self.device)\n",
        "\n",
        "        # Audio tower - using a simplified ResNet-like architecture\n",
        "        self.audio_encoder = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1),\n",
        "\n",
        "            # ResNet-like blocks\n",
        "            self._make_res_block(64, 64),\n",
        "            self._make_res_block(64, 128, stride=2),\n",
        "            self._make_res_block(128, 256, stride=2),\n",
        "            self._make_res_block(256, embedding_dim, stride=2),\n",
        "\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Projection layers to align embeddings\n",
        "        self.text_projection = nn.Linear(embedding_dim, embedding_dim).to(self.device)\n",
        "        self.audio_projection = nn.Linear(embedding_dim, embedding_dim).to(self.device)\n",
        "\n",
        "    def _make_res_block(self, in_channels, out_channels, stride=1):\n",
        "        \"\"\"Create a ResNet-like block\"\"\"\n",
        "        layers = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(out_channels)\n",
        "        )\n",
        "\n",
        "        # Skip connection\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            shortcut = nn.Identity()\n",
        "\n",
        "        return nn.Sequential(\n",
        "            layers,\n",
        "            shortcut,\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, audio=None, text=None):\n",
        "        \"\"\"Contrastive learning between audio and text embeddings\"\"\"\n",
        "        if audio is None and text is None:\n",
        "            raise ValueError(\"Either audio or text must be provided\")\n",
        "\n",
        "        audio_embeds = None\n",
        "        text_embeds = None\n",
        "\n",
        "        if audio is not None:\n",
        "            audio_embeds = self.get_audio_latents(audio)\n",
        "\n",
        "        if text is not None:\n",
        "            text_embeds = self.get_text_latents(text)\n",
        "\n",
        "        # If both provided, compute contrastive loss\n",
        "        if audio_embeds is not None and text_embeds is not None:\n",
        "            # Normalize embeddings\n",
        "            audio_embeds = torch.nn.functional.normalize(audio_embeds, dim=-1)\n",
        "            text_embeds = torch.nn.functional.normalize(text_embeds, dim=-1)\n",
        "\n",
        "            # Compute similarity matrix\n",
        "            similarity = torch.matmul(audio_embeds, text_embeds.transpose(0, 1))\n",
        "\n",
        "            # Temperature parameter for softmax\n",
        "            temperature = 0.07\n",
        "\n",
        "            # Contrastive loss\n",
        "            labels = torch.arange(similarity.size(0)).to(self.device)\n",
        "            loss_audio = torch.nn.functional.cross_entropy(similarity / temperature, labels)\n",
        "            loss_text = torch.nn.functional.cross_entropy(similarity.T / temperature, labels)\n",
        "\n",
        "            return (loss_audio + loss_text) / 2\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_audio_latents(self, audio):\n",
        "        \"\"\"Get audio embeddings\"\"\"\n",
        "        x = self.audio_encoder(audio)\n",
        "        x = x.squeeze(-1)\n",
        "        return self.audio_projection(x)\n",
        "\n",
        "    def get_text_latents(self, text):\n",
        "        \"\"\"Get text embeddings\"\"\"\n",
        "        embeds = self.text_encoder(text)\n",
        "        return self.text_projection(embeds)\n"
      ],
      "metadata": {
        "id": "wEnhei0IHcpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding for transformer models\"\"\"\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(1)].unsqueeze(0)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class W2vBERT(nn.Module):\n",
        "    \"\"\"w2v-BERT implementation for semantic token generation\"\"\"\n",
        "    def __init__(self, input_dim=512, hidden_dim=768, num_layers=12, num_heads=12, device=None):\n",
        "        super(W2vBERT, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Encoder layers\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim*4,\n",
        "            dropout=0.1,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers).to(self.device)\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(input_dim, hidden_dim).to(self.device)\n",
        "\n",
        "        # Token prediction head\n",
        "        self.token_head = nn.Linear(hidden_dim, 1024).to(self.device)  # 1024 semantic token vocabulary\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(hidden_dim, dropout=0.1).to(self.device)\n",
        "\n",
        "    def forward(self, x, conditioning=None):\n",
        "        \"\"\"\n",
        "        Forward pass through w2v-BERT\n",
        "        x: input features\n",
        "        conditioning: optional MuLAN embedding for conditioning\n",
        "        \"\"\"\n",
        "        # Project input to hidden dimension\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.pos_encoder(x)\n",
        "\n",
        "        # Add conditioning if provided\n",
        "        if conditioning is not None:\n",
        "            # Project conditioning to match sequence length\n",
        "            cond_expanded = conditioning.unsqueeze(1).expand(-1, x.size(1), -1)\n",
        "            # Add to input as a residual connection\n",
        "            x = x + 0.1 * cond_expanded\n",
        "\n",
        "        # Pass through transformer encoder\n",
        "        encoded = self.encoder(x)\n",
        "\n",
        "        # Generate semantic tokens\n",
        "        tokens = self.token_head(encoded)\n",
        "\n",
        "        return tokens\n"
      ],
      "metadata": {
        "id": "xqaemRCrHhmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicConditionedUNet(nn.Module):\n",
        "    \"\"\"UNet-based architecture for music generation with conditioning\"\"\"\n",
        "    def __init__(self, embedding_dim=512, channels=[32, 64, 128, 256, 512], device=None):\n",
        "        super(MusicConditionedUNet, self).__init__()\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Encoder blocks (downsampling path)\n",
        "        self.encoder_blocks = nn.ModuleList()\n",
        "        in_channels = 1  # Audio input channels\n",
        "\n",
        "        for c in channels:\n",
        "            self.encoder_blocks.append(self._make_encoder_block(in_channels, c))\n",
        "            in_channels = c\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv1d(channels[-1], channels[-1] * 2, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(channels[-1] * 2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(channels[-1] * 2, channels[-1], kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(channels[-1]),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "        # Conditioning projection\n",
        "        self.cond_projection = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, channels[-1]),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "        # Decoder blocks (upsampling path)\n",
        "        self.decoder_blocks = nn.ModuleList()\n",
        "        in_channels = channels[-1]\n",
        "\n",
        "        for c in reversed(channels[:-1]):\n",
        "            self.decoder_blocks.append(self._make_decoder_block(in_channels, c))\n",
        "            in_channels = c\n",
        "\n",
        "        # Final output layer\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.Conv1d(channels[0], 1, kernel_size=7, padding=3),\n",
        "            nn.Tanh()  # Output in range [-1, 1]\n",
        "        )\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def _make_encoder_block(self, in_channels, out_channels):\n",
        "        \"\"\"Create a single encoder block\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def _make_decoder_block(self, in_channels, out_channels):\n",
        "        \"\"\"Create a single decoder block\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose1d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(out_channels, out_channels, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "    def forward(self, audio, conditioning):\n",
        "        \"\"\"\n",
        "        Forward pass through UNet with conditioning\n",
        "        audio: [batch_size, channels, samples]\n",
        "        conditioning: [batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        # Store skip connections\n",
        "        skip_connections = []\n",
        "\n",
        "        # Encoder path\n",
        "        x = audio\n",
        "        for encoder in self.encoder_blocks:\n",
        "            x = encoder(x)\n",
        "            skip_connections.append(x)\n",
        "\n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Apply conditioning\n",
        "        cond = self.cond_projection(conditioning)  # [batch, channels[-1]]\n",
        "        cond = cond.unsqueeze(-1)  # [batch, channels[-1], 1]\n",
        "        cond = cond.expand(-1, -1, x.size(-1))  # [batch, channels[-1], time]\n",
        "\n",
        "        # Add conditioning (residual connection)\n",
        "        x = x + 0.3 * cond\n",
        "\n",
        "        # Decoder path with skip connections\n",
        "        for i, decoder in enumerate(self.decoder_blocks):\n",
        "            skip = skip_connections[-(i+1)]\n",
        "\n",
        "            # Ensure dimensions match before concatenating\n",
        "            if x.shape[2] != skip.shape[2]:\n",
        "                x = torch.nn.functional.interpolate(x, size=skip.shape[2], mode='linear')\n",
        "\n",
        "            x = decoder(x + 0.1 * skip)  # Residual connection with skip\n",
        "\n",
        "        # Final layer\n",
        "        output = self.final_layer(x)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "CBOgbmD0i5cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_coherent_music_sequence(text, emotion_extractor, emotion_mapper, coherence_model, musiclm_model=None):\n",
        "    \"\"\"Generate coherent music across narrative segments\"\"\"\n",
        "    # Set device\n",
        "    device = next(emotion_extractor.parameters()).device\n",
        "\n",
        "    # Extract emotional arc\n",
        "    arc_info = emotion_extractor.extract_emotional_arc(text, num_segments=10, return_attention=False)\n",
        "    emotional_arc = arc_info[\"emotional_arc\"]\n",
        "    segments = arc_info[\"segments\"]\n",
        "\n",
        "    print(f\"Extracted emotional arc with shape: {emotional_arc.shape}\")\n",
        "\n",
        "    # Map emotions to initial music parameters\n",
        "    music_params_sequence = []\n",
        "    for i in range(emotional_arc.shape[0]):\n",
        "        segment_emotions = emotional_arc[i]\n",
        "        music_params = emotion_mapper.forward(segment_emotions)[\"music_params\"]\n",
        "        music_params_sequence.append(music_params)\n",
        "\n",
        "    # Stack into sequence tensor\n",
        "    initial_params = music_params_sequence[0]\n",
        "\n",
        "    # Apply temporal coherence model to smooth transitions\n",
        "    coherent_params = coherence_model.generate_sequence(\n",
        "        initial_params,  # Initial parameters\n",
        "        emotional_arc.shape[0],          # Sequence length\n",
        "        text_embedding=None,             # Could add text embedding if available\n",
        "        smoothing=0.2                    # Smoothing factor for transitions\n",
        "    )\n",
        "\n",
        "    # Generate prompts from coherent parameters\n",
        "    prompts = []\n",
        "    for i in range(coherent_params.shape[1]):\n",
        "        segment_params = coherent_params[:, i, :]\n",
        "        # Convert parameters back to emotion space\n",
        "        segment_emotions = emotion_mapper.params_to_emotions(segment_params)\n",
        "        prompt = emotion_mapper.generate_musiclm_prompt(segment_emotions, arc_info[\"emotion_categories\"])\n",
        "        prompts.append(prompt[0])  # Assuming single batch\n",
        "\n",
        "    return {\n",
        "        \"coherent_params\": coherent_params,\n",
        "        \"prompts\": prompts,\n",
        "        \"segments\": segments\n",
        "    }\n",
        "\n",
        "def generate_music_with_musiclm(prompt, model, duration=10.0, sample_rate=22050):\n",
        "    \"\"\"\n",
        "    Generate music using MusicLM model\n",
        "\n",
        "    prompt: text prompt describing the desired music\n",
        "    model: trained MusicLM model\n",
        "    duration: duration of generated audio in seconds\n",
        "    sample_rate: sample rate of generated audio\n",
        "    \"\"\"\n",
        "    # Generate audio\n",
        "    audio, sr = model.generate(prompt, duration, sample_rate)\n",
        "\n",
        "    return audio, sr\n",
        "\n",
        "def initialize_musiclm_model(device=None):\n",
        "    \"\"\"Initialize a MusicLM model\"\"\"\n",
        "    device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Initializing MusicLM model on {device}\")\n",
        "\n",
        "    # Create model\n",
        "    model = MusicLM(device=device)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def generate_full_music_from_text(text, emotion_extractor, emotion_mapper, coherence_model, sample_rate=22050):\n",
        "    \"\"\"Generate complete music from text with emotional analysis\"\"\"\n",
        "    # Generate coherent music sequence\n",
        "    result = generate_coherent_music_sequence(\n",
        "        text,\n",
        "        emotion_extractor,\n",
        "        emotion_mapper,\n",
        "        coherence_model\n",
        "    )\n",
        "\n",
        "    prompts = result[\"prompts\"]\n",
        "    segments = result[\"segments\"]\n",
        "\n",
        "    print(f\"Generated {len(prompts)} music prompts for {len(segments)} text segments\")\n",
        "\n",
        "    # Generate audio for each prompt\n",
        "    audio_segments = []\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        print(f\"\\nGenerating music for segment {i+1}:\")\n",
        "        print(f\"Text: {segments[i][:50]}...\")\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "\n",
        "        # Generate audio for this prompt\n",
        "        audio, _ = generate_music_with_musiclm_api(prompt)\n",
        "        audio_segments.append(audio)\n",
        "\n",
        "    # Concatenate audio segments with crossfading\n",
        "    full_audio = concatenate_audio_with_crossfade(audio_segments, crossfade_duration=0.5, sample_rate=sample_rate)\n",
        "\n",
        "    return {\n",
        "        \"audio\": full_audio,\n",
        "        \"sample_rate\": sample_rate,\n",
        "        \"prompts\": prompts,\n",
        "        \"segments\": segments\n",
        "    }\n",
        "\n",
        "def concatenate_audio_with_crossfade(audio_segments, crossfade_duration=0.5, sample_rate=22050):\n",
        "    \"\"\"Concatenate audio segments with crossfading between them\"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    if not audio_segments:\n",
        "        return np.array([])\n",
        "\n",
        "    if len(audio_segments) == 1:\n",
        "        return audio_segments[0]\n",
        "\n",
        "    # Calculate crossfade length in samples\n",
        "    crossfade_length = int(crossfade_duration * sample_rate)\n",
        "\n",
        "    # Initialize output array\n",
        "    total_length = sum(len(segment) for segment in audio_segments) - crossfade_length * (len(audio_segments) - 1)\n",
        "    output = np.zeros(total_length)\n",
        "\n",
        "    # Add first segment\n",
        "    output[:len(audio_segments[0])] = audio_segments[0]\n",
        "    current_position = len(audio_segments[0]) - crossfade_length\n",
        "\n",
        "    # Add remaining segments with crossfading\n",
        "    for i in range(1, len(audio_segments)):\n",
        "        segment = audio_segments[i]\n",
        "\n",
        "        # Create crossfade window\n",
        "        fade_out = np.linspace(1.0, 0.0, crossfade_length)\n",
        "        fade_in = np.linspace(0.0, 1.0, crossfade_length)\n",
        "\n",
        "        # Apply crossfade\n",
        "        output[current_position:current_position + crossfade_length] *= fade_out\n",
        "        output[current_position:current_position + crossfade_length] += fade_in * segment[:crossfade_length]\n",
        "\n",
        "        # Add the rest of the segment\n",
        "        end_position = current_position + len(segment)\n",
        "        output[current_position + crossfade_length:end_position] = segment[crossfade_length:]\n",
        "\n",
        "        # Update position\n",
        "        current_position = end_position - crossfade_length\n",
        "\n",
        "    return output\n",
        "\n",
        "def visualize_emotional_arc(emotional_arc, emotion_categories):\n",
        "    \"\"\"Visualize the emotional arc of the text\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot each emotion over time\n",
        "    for i, emotion in enumerate(emotion_categories):\n",
        "        # Use detach() before calling numpy()\n",
        "        plt.plot(emotional_arc[:, i].detach().cpu().numpy(), label=emotion)\n",
        "\n",
        "    plt.xlabel('Text Segment')\n",
        "    plt.ylabel('Emotion Intensity')\n",
        "    plt.title('Emotional Arc Throughout Text')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ujUg68roi79Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_music_with_musiclm_api(prompt, api_key=None):\n",
        "    \"\"\"\n",
        "    Generate music using MusicLM API (simulated for this implementation)\n",
        "    In a real implementation, this would call an actual music generation API\n",
        "    \"\"\"\n",
        "    # This is a placeholder for the actual API call\n",
        "    # In a real implementation, you would call a music generation API here\n",
        "\n",
        "    # For demonstration, we'll generate a simple sine wave with parameters based on the prompt\n",
        "    import numpy as np\n",
        "\n",
        "    # Extract tempo from prompt if available\n",
        "    import re\n",
        "    tempo_match = re.search(r'at (\\d+) BPM', prompt)\n",
        "    tempo = 120  # Default tempo\n",
        "    if tempo_match:\n",
        "        tempo = int(tempo_match.group(1))\n",
        "\n",
        "    # Determine if major or minor\n",
        "    is_minor = 'minor key' in prompt.lower()\n",
        "\n",
        "    # Generate a simple melody based on extracted parameters\n",
        "    sample_rate = 22050\n",
        "    duration = 5  # seconds\n",
        "\n",
        "    # Base frequency (C4 for major, A3 for minor)\n",
        "    base_freq = 220.0 if is_minor else 261.63\n",
        "\n",
        "    # Generate time array\n",
        "    t = np.linspace(0, duration, int(sample_rate * duration), False)\n",
        "\n",
        "    # Generate a simple melody with harmonics\n",
        "    signal = 0.5 * np.sin(2 * np.pi * base_freq * t)  # Base frequency\n",
        "    signal += 0.25 * np.sin(2 * np.pi * base_freq * 2 * t)  # First harmonic\n",
        "    signal += 0.125 * np.sin(2 * np.pi * base_freq * 3 * t)  # Second harmonic\n",
        "\n",
        "    # Add a simple beat based on tempo\n",
        "    beat_interval = 60.0 / tempo  # seconds per beat\n",
        "    for beat in range(int(duration / beat_interval)):\n",
        "        beat_start = int(beat * beat_interval * sample_rate)\n",
        "        beat_end = min(beat_start + 1000, len(signal))\n",
        "        signal[beat_start:beat_end] += 0.3 * np.exp(-np.linspace(0, 4, beat_end - beat_start))\n",
        "\n",
        "    # Normalize\n",
        "    signal = signal / np.max(np.abs(signal))\n",
        "\n",
        "    return signal, sample_rate\n",
        "\n",
        "def generate_audio_from_prompt(prompt, duration=5, sample_rate=22050):\n",
        "    \"\"\"Generate audio based on a MusicLM prompt\"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # Extract musical parameters from prompt\n",
        "    import re\n",
        "\n",
        "    # Extract tempo\n",
        "    tempo_match = re.search(r'at (\\d+) BPM', prompt)\n",
        "    tempo = 120  # Default tempo\n",
        "    if tempo_match:\n",
        "        tempo = int(tempo_match.group(1))\n",
        "\n",
        "    # Determine key characteristics\n",
        "    is_minor = 'minor key' in prompt.lower()\n",
        "    is_ambient = 'ambient' in prompt.lower()\n",
        "    is_energetic = any(word in prompt.lower() for word in ['energetic', 'uplifting'])\n",
        "    is_melancholic = any(word in prompt.lower() for word in ['melancholic', 'sad'])\n",
        "    is_tense = any(word in prompt.lower() for word in ['tense', 'suspenseful'])\n",
        "\n",
        "    # Determine instrumentation\n",
        "    has_piano = 'piano' in prompt.lower()\n",
        "    has_strings = 'strings' in prompt.lower()\n",
        "    has_percussion = any(word in prompt.lower() for word in ['percussion', 'drums'])\n",
        "    has_synth = any(word in prompt.lower() for word in ['electronic', 'synthesizer'])\n",
        "\n",
        "    # Generate base frequencies based on musical key\n",
        "    base_freq = 220.0 if is_minor else 261.63  # A3 for minor, C4 for major\n",
        "\n",
        "    # Create time array\n",
        "    t = np.linspace(0, duration, int(sample_rate * duration), False)\n",
        "\n",
        "    # Initialize signal\n",
        "    signal = np.zeros_like(t)\n",
        "\n",
        "    # Add base melody\n",
        "    if has_piano:\n",
        "        # Piano-like sound with decay\n",
        "        for note_idx, note_time in enumerate(np.arange(0, duration, 60/tempo)):\n",
        "            if note_idx % 4 == 0:  # Emphasize first beat\n",
        "                freq = base_freq\n",
        "            elif is_minor and note_idx % 4 == 2:  # Minor third on third beat\n",
        "                freq = base_freq * 1.2\n",
        "            else:\n",
        "                # Choose from scale degrees\n",
        "                scale = [1, 1.122, 1.2, 1.335, 1.498, 1.682, 1.888] if is_minor else [1, 1.122, 1.26, 1.335, 1.498, 1.682, 1.888]\n",
        "                freq = base_freq * scale[note_idx % len(scale)]\n",
        "\n",
        "            # Note start and end in samples\n",
        "            note_start = int(note_time * sample_rate)\n",
        "            note_duration = int((60/tempo) * sample_rate * 0.9)  # 90% of beat duration\n",
        "            note_end = min(note_start + note_duration, len(t))\n",
        "\n",
        "            if note_start < len(t):\n",
        "                # Create note envelope\n",
        "                envelope = np.exp(-np.linspace(0, 5, note_end - note_start))\n",
        "                # Add note to signal\n",
        "                note_signal = 0.5 * np.sin(2 * np.pi * freq * t[note_start:note_end])\n",
        "                signal[note_start:note_end] += note_signal * envelope\n",
        "\n",
        "    # Add string pad for sustained sounds\n",
        "    if has_strings:\n",
        "        # Slower evolving pad sound\n",
        "        pad_freq = base_freq * 0.5  # Lower octave\n",
        "        # Add slight vibrato\n",
        "        vibrato = 0.03 * np.sin(2 * np.pi * 5 * t)  # 5 Hz vibrato\n",
        "        pad_signal = 0.3 * np.sin(2 * np.pi * pad_freq * (t + vibrato))\n",
        "\n",
        "        # Add harmonics\n",
        "        pad_signal += 0.15 * np.sin(2 * np.pi * pad_freq * 2 * (t + vibrato))\n",
        "        pad_signal += 0.07 * np.sin(2 * np.pi * pad_freq * 3 * (t + vibrato))\n",
        "\n",
        "        # Apply slow attack and release\n",
        "        envelope = np.ones_like(t)\n",
        "        attack_samples = int(0.5 * sample_rate)  # 0.5 second attack\n",
        "        release_samples = int(1.0 * sample_rate)  # 1.0 second release\n",
        "\n",
        "        envelope[:attack_samples] = np.linspace(0, 1, attack_samples)\n",
        "        envelope[-release_samples:] = np.linspace(1, 0, release_samples)\n",
        "\n",
        "        signal += pad_signal * envelope\n",
        "\n",
        "    # Add percussion\n",
        "    if has_percussion:\n",
        "        # Add kick drum on beats 1 and 3\n",
        "        for beat in range(int(duration / (60/tempo))):\n",
        "            if beat % 4 == 0 or beat % 4 == 2:  # Beats 1 and 3\n",
        "                beat_start = int(beat * (60/tempo) * sample_rate)\n",
        "                beat_end = min(beat_start + 3000, len(signal))\n",
        "\n",
        "                # Synthesize kick drum\n",
        "                kick_env = np.exp(-np.linspace(0, 12, beat_end - beat_start))\n",
        "                kick_freq = np.linspace(150, 50, beat_end - beat_start)\n",
        "                kick = 0.7 * np.sin(2 * np.pi * kick_freq * np.linspace(0, 1, beat_end - beat_start))\n",
        "\n",
        "                signal[beat_start:beat_end] += kick * kick_env\n",
        "\n",
        "        # Add hi-hat on eighth notes\n",
        "        for eighth in range(int(duration / (60/tempo/2))):\n",
        "            eighth_start = int(eighth * (60/tempo/2) * sample_rate)\n",
        "            eighth_end = min(eighth_start + 1000, len(signal))\n",
        "\n",
        "            # Synthesize hi-hat (filtered noise)\n",
        "            hat_env = np.exp(-np.linspace(0, 15, eighth_end - eighth_start))\n",
        "            hat = 0.2 * np.random.normal(0, 1, eighth_end - eighth_start)\n",
        "\n",
        "            signal[eighth_start:eighth_end] += hat * hat_env\n",
        "\n",
        "    # Add synthesizer elements\n",
        "    if has_synth:\n",
        "        # Add a sweeping synth pad\n",
        "        sweep_freq = base_freq * np.linspace(0.8, 1.2, len(t))  # Frequency sweep\n",
        "        synth_signal = 0.2 * np.sin(2 * np.pi * sweep_freq * t)\n",
        "\n",
        "        # Add some noise modulation for texture\n",
        "        noise_mod = 0.05 * np.random.normal(0, 1, len(t))\n",
        "        synth_signal += noise_mod\n",
        "\n",
        "        # Apply envelope\n",
        "        synth_env = np.ones_like(t)\n",
        "        attack = int(0.2 * sample_rate)\n",
        "        release = int(0.5 * sample_rate)\n",
        "        synth_env[:attack] = np.linspace(0, 1, attack)\n",
        "        synth_env[-release:] = np.linspace(1, 0, release)\n",
        "\n",
        "        signal += synth_signal * synth_env\n",
        "\n",
        "    # Adjust overall characteristics based on mood\n",
        "    if is_melancholic:\n",
        "        # Add reverb effect (simple convolution with exponential decay)\n",
        "        reverb_length = int(1.5 * sample_rate)\n",
        "        reverb_impulse = np.exp(-np.linspace(0, 8, reverb_length))\n",
        "        signal = np.convolve(signal, reverb_impulse, mode='same')\n",
        "\n",
        "    if is_tense:\n",
        "        # Add dissonant elements\n",
        "        dissonance = 0.1 * np.sin(2 * np.pi * (base_freq * 1.1) * t)\n",
        "        signal += dissonance\n",
        "\n",
        "    if is_ambient:\n",
        "        # More reverb and slower attack\n",
        "        reverb_length = int(2.0 * sample_rate)\n",
        "        reverb_impulse = np.exp(-np.linspace(0, 6, reverb_length))\n",
        "        signal = np.convolve(signal, reverb_impulse, mode='same')\n",
        "\n",
        "    if is_energetic:\n",
        "        # Boost higher frequencies\n",
        "        high_freq_boost = 0.2 * np.sin(2 * np.pi * base_freq * 2 * t)\n",
        "        signal += high_freq_boost\n",
        "\n",
        "    # Normalize final signal\n",
        "    signal = signal / (np.max(np.abs(signal)) + 1e-6)\n",
        "\n",
        "    return signal, sample_rate\n"
      ],
      "metadata": {
        "id": "9io2PItWjkhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_and_generate_music_with_musiclm(text, num_segments=10, musiclm_model=None):\n",
        "    \"\"\"Process text and generate music based on emotional content using MusicLM\"\"\"\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize models\n",
        "    print(\"Initializing models...\")\n",
        "    emotion_extractor = EmotionExtractionModule(device=device)\n",
        "    emotion_mapper = EmotionToMusicMapper(device=device)\n",
        "    coherence_model = TemporalCoherenceModel(device=device)\n",
        "\n",
        "    # Initialize MusicLM model if not provided\n",
        "    if musiclm_model is None:\n",
        "        musiclm_model = initialize_musiclm_model(device)\n",
        "\n",
        "    # Extract emotional arc\n",
        "    print(\"Extracting emotional arc from text...\")\n",
        "    arc_info = emotion_extractor.extract_emotional_arc(text, num_segments=num_segments, return_attention=False)\n",
        "    emotional_arc = arc_info[\"emotional_arc\"]\n",
        "    segments = arc_info[\"segments\"]\n",
        "\n",
        "    print(f\"Extracted emotional arc with shape: {emotional_arc.shape}\")\n",
        "\n",
        "    # Visualize emotional arc\n",
        "    visualize_emotional_arc(emotional_arc, arc_info[\"emotion_categories\"])\n",
        "\n",
        "    # Map emotions to initial music parameters\n",
        "    print(\"Mapping emotions to music parameters...\")\n",
        "    music_params_sequence = []\n",
        "    for i in range(emotional_arc.shape[0]):\n",
        "        segment_emotions = emotional_arc[i]\n",
        "        music_params = emotion_mapper.forward(segment_emotions)[\"music_params\"]\n",
        "        music_params_sequence.append(music_params)\n",
        "\n",
        "    # Get initial parameters\n",
        "    initial_params = music_params_sequence[0]\n",
        "\n",
        "    # Apply temporal coherence model\n",
        "    print(\"Applying temporal coherence model...\")\n",
        "    coherent_params = coherence_model.generate_sequence(\n",
        "        initial_params,\n",
        "        emotional_arc.shape[0],\n",
        "        text_embedding=None,\n",
        "        smoothing=0.2\n",
        "    )\n",
        "\n",
        "    # Generate prompts from coherent parameters\n",
        "    print(\"Generating music prompts...\")\n",
        "    prompts = []\n",
        "    for i in range(coherent_params.shape[1]):\n",
        "        segment_params = coherent_params[:, i, :]\n",
        "        segment_emotions = emotion_mapper.params_to_emotions(segment_params)\n",
        "        prompt = emotion_mapper.generate_musiclm_prompt(segment_emotions, arc_info[\"emotion_categories\"])\n",
        "        prompts.append(prompt[0])\n",
        "\n",
        "    # Generate audio for each prompt using MusicLM\n",
        "    print(\"Generating audio for each segment with MusicLM...\")\n",
        "    audio_segments = []\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        print(f\"\\nGenerating music for segment {i+1}/{len(prompts)}:\")\n",
        "        print(f\"Text: {segments[i][:50]}...\")\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "\n",
        "        # Generate audio for this prompt using MusicLM\n",
        "        audio, sr = generate_music_with_musiclm(prompt, musiclm_model, duration=5.0)\n",
        "        audio_segments.append(audio)\n",
        "\n",
        "    # Concatenate audio segments with crossfading\n",
        "    print(\"Concatenating audio segments with crossfading...\")\n",
        "    full_audio = concatenate_audio_with_crossfade(audio_segments, crossfade_duration=0.5, sample_rate=sr)\n",
        "\n",
        "    return {\n",
        "        \"audio\": full_audio,\n",
        "        \"sample_rate\": sr,\n",
        "        \"prompts\": prompts,\n",
        "        \"segments\": segments,\n",
        "        \"emotional_arc\": emotional_arc\n",
        "    }\n",
        "\n",
        "\n",
        "def play_generated_music(audio, sample_rate=22050):\n",
        "    \"\"\"Play the generated music\"\"\"\n",
        "    from IPython.display import Audio, display\n",
        "\n",
        "    print(\"Playing generated music...\")\n",
        "    display(Audio(audio, rate=sample_rate))\n"
      ],
      "metadata": {
        "id": "YyG85tVajn7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to demonstrate the emotion-aware music generation system\"\"\"\n",
        "    # Example literary text\n",
        "    literary_text = \"\"\"\n",
        "    The old mansion stood silent against the stormy sky. Inside, memories of\n",
        "    laughter and dance echoed through empty halls. A lone figure stood at the\n",
        "    window, watching raindrops trace patterns like tears upon the glass.\n",
        "    For years this place had been home, but tomorrow it would belong to strangers.\n",
        "    As night fell, the wind picked up, rattling the windows like impatient ghosts.\n",
        "    The figure lit a candle, its warm glow a defiant stand against the gathering darkness.\n",
        "    Morning came with unexpected brightness, sunlight streaming through windows,\n",
        "    illuminating dust motes dancing in the air. Perhaps, thought the figure with a smile,\n",
        "    endings could also be beginnings.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Starting emotion-aware music generation process...\")\n",
        "    print(f\"Input text: {literary_text[:100]}...\")\n",
        "\n",
        "    # Process text and generate music\n",
        "    result = process_text_and_generate_music(literary_text, num_segments=8)\n",
        "\n",
        "    # Play the generated music\n",
        "    play_generated_music(result[\"audio\"], result[\"sample_rate\"])\n",
        "\n",
        "    # Print the generated prompts\n",
        "    print(\"\\nGenerated Music Prompts:\")\n",
        "    for i, prompt in enumerate(result[\"prompts\"]):\n",
        "        print(f\"\\nPrompt {i+1}:\")\n",
        "        print(prompt)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Run the main function if this script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-45bBLDxjqih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_emotion_aware_music_generation_with_musiclm():\n",
        "    \"\"\"Demonstrate the complete emotion-aware music generation with MusicLM\"\"\"\n",
        "    # Example literary text\n",
        "    literary_text = \"\"\"\n",
        "    The old mansion stood silent against the stormy sky. Inside, memories of\n",
        "    laughter and dance echoed through empty halls. A lone figure stood at the\n",
        "    window, watching raindrops trace patterns like tears upon the glass.\n",
        "    For years this place had been home, but tomorrow it would belong to strangers.\n",
        "    As night fell, the wind picked up, rattling the windows like impatient ghosts.\n",
        "    The figure lit a candle, its warm glow a defiant stand against the gathering darkness.\n",
        "    Morning came with unexpected brightness, sunlight streaming through windows,\n",
        "    illuminating dust motes dancing in the air. Perhaps, thought the figure with a smile,\n",
        "    endings could also be beginnings.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Starting emotion-aware music generation with MusicLM...\")\n",
        "    print(f\"Input text: {literary_text[:100]}...\")\n",
        "\n",
        "    # Initialize MusicLM model\n",
        "    musiclm_model = initialize_musiclm_model()\n",
        "\n",
        "    # Process text and generate music\n",
        "    result = process_text_and_generate_music_with_musiclm(literary_text, num_segments=8, musiclm_model=musiclm_model)\n",
        "\n",
        "    # Play the generated music\n",
        "    play_generated_music(result[\"audio\"], result[\"sample_rate\"])\n",
        "\n",
        "    # Print the generated prompts\n",
        "    print(\"\\nGenerated Music Prompts:\")\n",
        "    for i, prompt in enumerate(result[\"prompts\"]):\n",
        "        print(f\"\\nPrompt {i+1}:\")\n",
        "        print(prompt)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Uncomment to run the demo\n",
        "# demo_emotion_aware_music_generation_with_musiclm()\n"
      ],
      "metadata": {
        "id": "AUjmDzg2JBtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_emotion_aware_music_generation_with_trained_model(trained_model=None):\n",
        "    \"\"\"Demonstrate the complete emotion-aware music generation with a trained MusicLM model\"\"\"\n",
        "    # Example literary text\n",
        "    literary_text = \"\"\"\n",
        "    The old mansion stood silent against the stormy sky. Inside, memories of\n",
        "    laughter and dance echoed through empty halls. A lone figure stood at the\n",
        "    window, watching raindrops trace patterns like tears upon the glass.\n",
        "    For years this place had been home, but tomorrow it would belong to strangers.\n",
        "    As night fell, the wind picked up, rattling the windows like impatient ghosts.\n",
        "    The figure lit a candle, its warm glow a defiant stand against the gathering darkness.\n",
        "    Morning came with unexpected brightness, sunlight streaming through windows,\n",
        "    illuminating dust motes dancing in the air. Perhaps, thought the figure with a smile,\n",
        "    endings could also be beginnings.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Starting emotion-aware music generation with trained MusicLM...\")\n",
        "    print(f\"Input text: {literary_text[:100]}...\")\n",
        "\n",
        "    # Initialize MusicLM model if not provided\n",
        "    if trained_model is None:\n",
        "        # Try to load a trained model if available\n",
        "        model_path = \"musiclm_checkpoint_epoch_10.pt\"\n",
        "        if os.path.exists(model_path):\n",
        "            print(f\"Loading trained model from {model_path}\")\n",
        "            musiclm_model = initialize_musiclm_model()\n",
        "            musiclm_model.load_state_dict(torch.load(model_path, map_location=musiclm_model.device))\n",
        "        else:\n",
        "            print(\"No trained model found, initializing with random weights\")\n",
        "            musiclm_model = initialize_musiclm_model()\n",
        "    else:\n",
        "        musiclm_model = trained_model\n",
        "\n",
        "    # Process text and generate music\n",
        "    result = process_text_and_generate_music_with_musiclm(literary_text, num_segments=8, musiclm_model=musiclm_model)\n",
        "\n",
        "    # Play the generated music\n",
        "    play_generated_music(result[\"audio\"], result[\"sample_rate\"])\n",
        "\n",
        "    # Print the generated prompts\n",
        "    print(\"\\nGenerated Music Prompts:\")\n",
        "    for i, prompt in enumerate(result[\"prompts\"]):\n",
        "        print(f\"\\nPrompt {i+1}:\")\n",
        "        print(prompt)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "n-BS_kOSK7DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# 1. Download the dataset\n",
        "musiccaps_df, dataset_path = load_musiccaps_dataset()\n",
        "\n",
        "# 2. Train the model (this will take a long time)\n",
        "# trained_model = train_musiclm_with_kagglehub(epochs=10, batch_size=16, download_audio=True)\n",
        "\n",
        "# 3. Generate music using your emotion extraction pipeline\n",
        "# demo_emotion_aware_music_generation_with_trained_model(trained_model)\n",
        "\n",
        "# Or just use the existing process_text_and_generate_music function\n",
        "# which uses the simulated audio generation\n",
        "result = process_text_and_generate_music(\n",
        "    \"\"\"The sun set over the calm ocean, painting the sky in brilliant hues of orange and pink.\n",
        "    A gentle breeze carried the scent of salt and seaweed across the empty beach.\"\"\",\n",
        "    num_segments=5\n",
        ")\n",
        "play_generated_music(result[\"audio\"], result[\"sample_rate\"])\n"
      ],
      "metadata": {
        "id": "2Mx92sJ6LAFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_audio_to_file(audio, sample_rate, filename=\"generated_music.wav\"):\n",
        "    \"\"\"Save the generated audio to a file\"\"\"\n",
        "    import soundfile as sf\n",
        "    import os\n",
        "\n",
        "    print(f\"Saving audio to {filename}...\")\n",
        "    sf.write(filename, audio, sample_rate)\n",
        "    print(f\"Audio saved to {os.path.abspath(filename)}\")\n",
        "\n",
        "def analyze_audio_features(audio, sample_rate):\n",
        "    \"\"\"Analyze audio features using librosa\"\"\"\n",
        "    import librosa\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    print(\"Analyzing audio features...\")\n",
        "\n",
        "    # Compute spectrogram\n",
        "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
        "\n",
        "    # Compute MFCCs\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
        "\n",
        "    # Compute chroma features\n",
        "    chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
        "\n",
        "    # Compute spectral contrast\n",
        "    contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate)\n",
        "\n",
        "    # Plot features\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    plt.subplot(4, 1, 1)\n",
        "    librosa.display.specshow(D, x_axis='time', y_axis='log')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Spectrogram')\n",
        "\n",
        "    plt.subplot(4, 1, 2)\n",
        "    librosa.display.specshow(mfccs, x_axis='time')\n",
        "    plt.colorbar()\n",
        "    plt.title('MFCCs')\n",
        "\n",
        "    plt.subplot(4, 1, 3)\n",
        "    librosa.display.specshow(chroma, x_axis='time', y_axis='chroma')\n",
        "    plt.colorbar()\n",
        "    plt.title('Chromagram')\n",
        "\n",
        "    plt.subplot(4, 1, 4)\n",
        "    librosa.display.specshow(contrast, x_axis='time')\n",
        "    plt.colorbar()\n",
        "    plt.title('Spectral Contrast')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Return computed features\n",
        "    return {\n",
        "        \"spectrogram\": D,\n",
        "        \"mfccs\": mfccs,\n",
        "        \"chroma\": chroma,\n",
        "        \"contrast\": contrast\n",
        "    }\n"
      ],
      "metadata": {
        "id": "E0xi-HLQjuDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_music_from_user_input():\n",
        "    \"\"\"Generate music from user-provided text\"\"\"\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "    # Get text input from user\n",
        "    print(\"Enter your literary text (press Enter twice to finish):\")\n",
        "    lines = []\n",
        "    while True:\n",
        "        line = input()\n",
        "        if not line and lines:  # Empty line and we already have content\n",
        "            break\n",
        "        lines.append(line)\n",
        "\n",
        "    text = \"\\n\".join(lines)\n",
        "\n",
        "    # Clear output for cleaner display\n",
        "    clear_output()\n",
        "\n",
        "    print(f\"Generating music for text: {text[:100]}...\")\n",
        "\n",
        "    # Process text and generate music\n",
        "    result = process_text_and_generate_music(text)\n",
        "\n",
        "    # Play the generated music\n",
        "    play_generated_music(result[\"audio\"], result[\"sample_rate\"])\n",
        "\n",
        "    # Ask if user wants to save the audio\n",
        "    save_option = input(\"Do you want to save this music? (y/n): \")\n",
        "    if save_option.lower() == 'y':\n",
        "        filename = input(\"Enter filename (default: generated_music.wav): \") or \"generated_music.wav\"\n",
        "        save_audio_to_file(result[\"audio\"], result[\"sample_rate\"], filename)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "generate_music_from_user_input()\n",
        "\n"
      ],
      "metadata": {
        "id": "mcp0Ef7bjxMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "46heyzm_j2q4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}